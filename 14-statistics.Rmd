# Statistical modeling {#seminar11}
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(glue)
library(tidyverse)
```
I suspect that this is a seminar that you were waiting for the most as it finally tells you how to call statistical functions in R. However, from my perspective, it is the least useful seminar in the entire course because if you know statistics and you know which specific tool you need, figuring out how to use it in R is fairly trivial. Conversely, if your knowledge of statistics is approximate, knowing how to call functions will do you little good. The catch about statistical models is that they are very easy to run (even if you implement them by hand from scratch) but they are easy to misuse and very hard to interpret^[In the Statistical Rethinking seminar we spend three seminars learning how to understand and interpret a simple linear multiple regression with just two predictors. And the conclusion is that even in this simple case, you are not guaranteed to fully understand it. And if you think that you can easily interpret an interaction term even for two continuous predictors...].

To make things worse, computers and algorithms do not care. In absolute majority of cases, statistical models will happily accept any input you provide, even if it is completely unsuitable, and spit out numbers. Unfortunately, it is on you, not on the computer, to know what you are doing and whether results even make sense. The only solution to this problem: do not spare any effort to learn statistics. Having a solid understanding of a basic regression analysis will help you in figuring out which statistical tools are applicable and, even more importantly, which will definitely misguide you. This is why I will give an general overview with some examples simulations but I will not explain here when and why you should use a particular tool or how to interpret the outputs. Want to know more? Attend my _Bayesian Statistics_ seminar or read an excellent [Statistical Rethinking](https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919) by Richard McElreath that the seminar is based on.

## Correlation
In base, you can use function [cor()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.html) to compute correlation. You have a choice of `method`, either `"pearson"` (default, [Pearson's product moment correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), $\rho$), or rank-based `"kendall"` ([Kendall's  rank correlation coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient), $\tau$) or `"spearman"` ([Spearman's rank correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient), $\rho$).

```{r}
df <- 
  tibble(x = rnorm(100)) %>%
  mutate(y = rnorm(n(), x, 0.5))
cor(df$x, df$y, method="pearson")
```

Alternatively, you can use [cor.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.test.html) that also computes test statistics and significance.
```{r}
cor_result <- cor.test(df$x, df$y, method="pearson")
cor_result
```

`cor.test` returns a list, so you can access its individual elements^[Use [names()](https://stat.ethz.ch/R-manual/R-devel/library/base/html/names.html) function to get names of all elements.] via a double-bracket or `$` notation.
```{r}
cor_result$p.value
```

Bayesian correlation with a full posterior distribution for the correlation coefficient and Bayes Factor as a measure of significance, can be computed via `correlationBF()` function, which is part of [BayesFactor](https://richarddmorey.github.io/BayesFactor) package.
```{r message=FALSE, warning=FALSE}
library(BayesFactor)
correlationBF(df$x, df$y)
cor_bf <- correlationBF(df$x, df$y, posterior = TRUE, iterations=1000)
ggplot(data=NULL, aes(x=c(cor_bf[, "rho"]))) + 
  geom_histogram(bins=50, ) + 
  xlab("Pearson's rho")
```

## Correlation in practice
Let us practice computing and understanding the correlation using randomly generating data. First, let us compute Pearson's correlation coefficient by hand. The formulat is
$$
r_{xy} = \frac{\sum_{i=1}^{n}(x_i-\hat{x})(y_i-\hat{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\hat{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\hat{y})^2}}
$$
where $\hat{x}$ and $\hat{y}$ are sample means for the corresponding variable. Thanks to vectorized nature of R, you _do not_ need to use loops! Generate random data from normal distribution the way I did above. Generate variable `x` from standard normal distribution ([rnorm()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Normal.html), $\mu=0$, $\sigma=1$), generate a second variable from a normal distribution with $\mu$ defined by `x` and use a value of your choice for $\sigma$. Compare your results with those of  [cor()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.html) function.

```{r echo=FALSE}
x <- rnorm(100)
y <- rnorm(100, x, 0.5)
rxy <- sum((x - mean(x)) * (y - mean(y))) / (sqrt(sum((x - mean(x))**2)) * sqrt(sum((y - mean(y))**2)))
cat(glue::glue("cor(x, y) = {cor(x, y)}, rxy = {rxy}\n"))
```

::: {.infobox .practice}
Do exercise 1.
:::

When interpreting correlation between two variables, your are interested not only the estimate itself but also about your confidence about it. The [cor.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.test.html) function helps you in this by returning a 95% confidence interval. Let us replicate this via resampling approach you have learned about the last time. Generate `x` and `y` as in exercise 1 and compute a percentile 95% confidence interval. For this, you will need to sample _pairs_ of values with replacement. In other words, you need to sample _index_ of elements. You can do it yourself or rely on [boot()](https://stat.ethz.ch/R-manual/R-devel/library/boot/html/boot.html)). Once you computed correlation coefficients for 1000 samples drawn from original variables, compute and compare the 95% confidence interval with one reported by [cor.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.test.html).

```{r echo=FALSE}
x <- rnorm(100)
y <- rnorm(100, x, 0.2)

resample_correlation <- function(x, y){
  index <- sample(1:length(x), replace = TRUE)
  cor(x[index], y[index])
}
rxys <- replicate(1000, resample_correlation(x, y))
cat(glue::glue("95% CI via resampling: {round(quantile(rxys, (1-0.95)/2), 6)}..{round(quantile(rxys, 1-(1-0.95)/2), 6)}\n\n"))
cor.test(x, y)
```

::: {.infobox .practice}
Do exercise 2.
:::

Next, let us see how noise in `y` (second variable) affect it strength. The approach  will be very similar to how we explored with dependence between number of samples and our (un)certainty about the mean in the previous seminar. Write a function that will take two parameters: 1) number of samples and 2) standard deviation of noise. Generate variable `x` and `y` the same way as in exercise 1 but use the function parameter for $\sigma$ when generating `y`. Return the correlation coefficient. Next, create a tibble with various levels of standard deviation of noise with 1000 iterations (rows) per level. Compute correlation coefficient for every row (I have used `Nsamples = 30`) and then mean and 89%^[Why 89%? Because 89 is a prime number.] confidence interval for each level of noise. Plot it, so it looks similar to the plot below.
```{r echo=FALSE, cache=TRUE}
simulate_correlation <- function(sigma, N){
  x <- rnorm(N, 0, 1)
  cor(x, rnorm(N, x, sigma))
}

expand_grid(sigma_noise = seq(0, 10, length.out=100),
            iteration = 1:1000) %>%
  group_by(sigma_noise, iteration) %>%
  mutate(rho = simulate_correlation(sigma_noise, 30)) %>%
  group_by(sigma_noise) %>%
  summarise(avgRho = mean(rho),
            lowerCI = quantile(rho, (1-0.89)/2.0),
            upperCI = quantile(rho, 1- (1-0.89)/2.0)) %>%
  ggplot(aes(x=sigma_noise, y=avgRho)) + 
  geom_ribbon(aes(ymin=lowerCI, ymax=upperCI), alpha=0.25) +
  geom_line() +
  geom_hline(yintercept = 0, color = "red") +
  ylab("ρ, average + 89% CI") + 
  xlab("Standard deviation of noise")
```

::: {.infobox .practice}
Do exercise 3.
:::

The analysis above explored how increased noise masks correlation between two samples of a _fixed_ size. However, when you design a study you are more likely to be interested in an opposite questions: what is your confidence about a _fixed strength_ correlation given an increasing number of samples. This is the same logic that we applied to our confidence about mean. I have used $\sigma_{noise} = 5.0$ for the plot below.

```{r echo=FALSE}
expand_grid(n_samples = 10:100,
            iteration = 1:1000) %>%
  group_by(n_samples, iteration) %>%
  mutate(rho = simulate_correlation(5, n_samples)) %>%
  group_by(n_samples) %>%
  summarise(avgRho = mean(rho),
            lowerCI = quantile(rho, (1-0.89)/2.0),
            upperCI = quantile(rho, 1- (1-0.89)/2.0)) %>%
  ggplot(aes(x=n_samples, y=avgRho)) + 
  geom_ribbon(aes(ymin=lowerCI, ymax=upperCI), alpha=0.25) +
  geom_line() +
  geom_hline(yintercept = 0, color = "red") +
  ylab("ρ, average + 89% CI") + 
  xlab("Standard deviation of noise")
```

::: {.infobox .practice}
Do exercise 4.
:::

## Pairwise comparisons
For pairwise comparisons for normally distributed data, you can use Student's t-Test via [t.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/t.test.html). Here, I generate to `x` as a normally distributed normal variable and `y`, as a normally distributed variable random whose mean is `x + 0.3`. I am using library [ggbeeswarm](https://github.com/eclarke/ggbeeswarm) to generate the nicely looking cloud of dots.

```{r}
set.seed(14454)
df <- 
  tibble(x = rnorm(100, mean = 1, sd = 1)) %>%
  mutate(y = rnorm(100, mean = x + 0.3, sd = 1))
```
```{r echo=FALSE}
library(ggbeeswarm)
tibble(Value = c(df$x, df$y),
       Variable = c(rep("x", nrow(df)), rep("y", nrow(df)))) %>%
  ggplot(aes(x=Variable, y=Value)) + 
  geom_boxplot(outlier.shape = NA) + 
  geom_quasirandom(method="tukeyDense")  
```

You can perform _t-test_ assuming that samples in two variables are independent 
```{r}
t.test(df$x, df$y, paired = FALSE)
```

Or, that they are paired (this matches how I generated the data), i.e., repeated-measures design (note the change in estimates, statistics, and significance).
```{r}
t.test(df$x, df$y, paired = TRUE)
```

A Bayesian version is provided via `ttestBF()` function, which is part of the [BayesFactor](https://richarddmorey.github.io/BayesFactor) package.
```{r}
library(BayesFactor)
ttestBF(x = df$x, y=df$y, paired=TRUE)
```

For a non-parametric pairwise test, you can use Wilcoxon Rank Sum and Signed Rank Tests [wilcox.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/wilcox.test.html). Package [coin](http://coin.r-forge.r-project.org/) implements many tests, including permutation tests, such as Two- and K-sample Fisher-Pitman permutation test via `oneway_test()` that tests for the equality of the distributions in _independent_ groups, see [vignette](https://cran.r-project.org/web/packages/coin/vignettes/coin.pdf) for details.

## T-tests in practice
As with the correlation coefficient, let us generate data and compute the t-statistics by hand. The formula is
$$t = \frac{\bar{X} - \mu_0}{s_D / \sqrt{n}}$$

where $\bar{X}$ and $s_D$ are the average and standard deviation of _difference_ between all pairs, $n$ is the number of pairs, and $\mu_0$ is the null hypothesis for difference. In our case, we would like to know whether the difference is different from zero, thus, $\mu_0 = 0$.

Generate random data similarly to how I did it in the example above. Compute t-statistic and compare it to the output of a _paired_ [t.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/t.test.html).

```{r echo=FALSE}
x <- rnorm(100, mean = 1, sd = 1)
y <- rnorm(100, mean = x + 0.3, sd = 1)
t <- mean(x-y) / (sd(x-y) / sqrt(length(x)))
cat(glue("t-statistic: {t}\n\n"))
t.test(x, y, paired = TRUE)
```

::: {.infobox .practice}
Do exercise 5.
:::

Let us use resampling approach to replicate analytically derived confidence interval of difference between the means and the p-value. We start with the 95% confidence interval for the difference. The idea is the same as before when we computed the confidence interval for correlation: We sample both variables _pairwise_, compute the average difference, store it, repeat this 1000 times. Once we have 1000 resampled average distances, compute a percentile interval and compare it to one reported by [t.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/t.test.html).

```{r echo=FALSE}
x <- rnorm(100, mean = 1, sd = 1)
y <- rnorm(100, mean = x + 0.3, sd = 1)

compute_resampled_difference <- function(x, y) {
  index = sample(1:length(x), replace = TRUE)
  mean(x[index] - y[index])
}
xy_diff <- replicate(1000, compute_resampled_difference(x, y))

cat(glue::glue("resampled 95% percentile confidence interval: {quantile(xy_diff, (1-0.95)/2)}..{quantile(xy_diff, 1-(1-0.95)/2)}\n\n"))
t.test(x, y, paired = TRUE)
```

::: {.infobox .practice}
Do exercise 6.
:::

Next comes the statistical significance testing via a _permutation test_. Here, we compute the t-statistic for the sample (you already know how to do that). Then, we pool values from both `x` and `y` into a single vector, shuffle that single vector and  divide it back into permuted x (first half of the pooled vector) and permuted y (second half of the pooled vector). Then we compute the t-statistics for the difference between permuted vectors. We repeat this 10000 times to obtain a distribution of t-statistics that we would observe when pairing between `x` and `y` was random.  Finally, we compute the proportion of values that are more extreme (smaller than, in our case) than the original value. This, effectively, is the probably of observing such extreme t-statistic by pure chance and is interpreted the same way as the p-value. Compare this to the output of the paired t-test using `alternative = "less"`:

```{r echo}
x <- rnorm(100, mean = 1, sd = 1)
y <- rnorm(100, mean = x + 0.1, sd = 1)

t_statistic <- function(x, y){
  mean(x-y) / (sd(x-y) / sqrt(length(x)))
}

shuffle_and_t <- function(x, y){
  xy <- sample(c(x, y))
  x <- xy[1:length(x)]
  y <- xy[(length(x)+1) : (length(x) + length(y))]
  t_statistic(x, y)
}

t_original <- t_statistic(x, y)
t_permuted <- replicate(10000, shuffle_and_t(x, y))


ggplot() + 
  geom_histogram(aes(x=t_permuted), bins=100) +
  geom_vline(xintercept = t_original, color="red")

cat(glue::glue("Proportion of t-statisic for permuted pairs < t-stat for original: {mean(t_permuted < t_original)}\n\n"))

t.test(x, y, paired = TRUE, alternative = "less")
```

::: {.infobox .practice}
Do exercise 7.
:::

## Formula notation
Using statistical models in R is particularly easy because most packages make use of a [formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) to describe a model. Different functions and packages interpret the formula _mostly_ the same way with differences arising due to how random effects or additional parameters are described. Here is an example of a formula:
```r
y ~ 1 + x1 + x2 + x1:x2
```

It says that the outcome variable `y` should be a modeled as a linear combination of an intercept `1` (can be omitted in a formula), predictor variables `x1` and `x2`, plus their interaction `x1:x2`. This also assumes that all these variables are in a single table that you also supply to the function (that parameter is typically called `data`). Same formula can be shortened by using `*` which means "both predictors and their interaction" and omitting redundant intercept (it is always used unless you explicitly exclude it)
```r
y ~ x1*x2
```

You can also _exclude_ specific terms via `-`. So, if you insist that the intercept _must_ go through 0, you write "exclude intercept term" as `-1`
```r
y ~ x1*x2 - 1
```

Or you can exclude a specific term or an interaction. The two formulas below are equivalent with a main effect for `x2` and an interaction term but no main effect for `x1`. However, I would generally discourage you from using `-` as the first formula is much harder to understand (or, conversely, much easier to misunderstand).
```r
y ~ x1*x2 - x1
y ~ x2 + x1:x2
```

## Using t-test via formula
When we used [t.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/t.test.html) above, we specified two vectors. However, you can also use a table where one column contains _values_ and another one _group labels_. Here, we estimate difference in mean `x` between two `group`s. 
```{r}
yA <- rnorm(100, mean = 1, sd = 1)
yB <- rnorm(100, mean = yA + 0.1, sd = 1)
df_group <-
  tibble(y = c(yA, yB),
         group = factor(c(rep("A", 100), rep("B", 100))))
```
```{r echo=FALSE}
df_group %>%
  head(4) %>%
  knitr::kable()
```
```{r}
t.test(y ~ group, data=df_group, paired=TRUE)
```

## ANOVA
**AN**alysis **O**f **VA**riance is probably the most widely used analysis in social sciences. However, I would strongly suggest considering [generalized linear mixed models](#GLMM) instead. Unlike ANOVA they can work when residuals are not normally distributed (binomial, count, Likert-scale ordered categorical, proportions data, etc.), they can tolerate missing values, and they tend to overfit less (their results are likely to better generalize to future data) by assuming that individual participants are more average than they appear in the raw data (so-called shrinkage).

First, let us simulate data for ten participants and their responses, assuming that condition `B` increases their responses by 2 (arbitrary) units and `C` by 3^[[set.seed()](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Random.html) function initializes random numbers generator to a specific state to get a reproducible but random (as in, sequentially uncorrelated) sequence of values.].
```{r}
set.seed(519264)
df_anova <-
  # generate ten participants with five trials per condition
  expand_grid(Participant  = factor(1:10),
              Condition = factor(c(rep("A", 3), rep("B", 3), rep("C", 3)))) %>%
  
  # decide on a SINGLE baseline (intercept) for each participant
  group_by(Participant) %>%
  mutate(Intercept = rnorm(1, 5, 2.5)) %>%
  
  # simulate normally distributed responses, assuming that are 3 units higher for condition "B"
  ungroup() %>%
  mutate(Response = rnorm(n(), Intercept + 2 * as.integer(Condition == "B") + 3 * as.integer(Condition == "C"), 1.5)) %>%
  select(-Intercept)
```
```{r echo=FALSE}
ggplot(df_anova, aes(x=Condition, y=Response, color=Condition)) + 
  geom_point() + 
  facet_grid(.~Participant) + 
  theme(legend.position = "none")
```
The base R ANOVA function is called [aov()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/aov.html), however, it does not support repeated measures. Instead, you can use function `anova_test` from package [rstatix](https://github.com/kassambara/rstatix)

For the _repeated-measures_ ANOVA, we need to specify column with identity of participants via parameter `wid`
```{r message=FALSE, warning=FALSE}
library(rstatix)
anova_test(data=df_anova, Response ~ Condition, wid=Participant)
```


You can perform various pairwise post-hoc tests, e.g. Tukey
```r
tukey_hsd(df_anova, Response ~ Condition)
```
```{r}
tukey_hsd(df_anova, Response ~ Condition) %>%
  knitr::kable()
```

A Bayesian ANOVA with posterior distributions for individual coefficients and significance via Bayes Factor can be performed 
via `anovaBF` function from [BayesFactor](https://richarddmorey.github.io/BayesFactor) package.

```{r}
anovaBF(Response ~ Condition + Participant, whichRandom="Participant", data=data.frame(df_anova))
```

If you decide for repeated-measure ANOVA, I would suggest using and reporting results for both frequentist and Bayesian ANOVA, as it will demonstrate that they do not depend on the choice of the statistical approach.

## (Almost) everything is a linear regression model
Even though linear models did not appear as the first on the list, they are. All approaches that we covered so far, correlation, t-test, ANOVA, and many more (MANOVA, ANCOVA, etc.) a linear regression models. They may not _look_ like linear regression models and even the formulas that we used, e.g., for t-test, do not look as if they have anything to do with linear regression, but they are. Here is a simple demonstration where I use the same code to generate data for running a t-test on a table. For a single two-level categorical variable, a t-test is equivalent to one-way ANOVA (note matching p-values). 
```{r}
yA <- rnorm(100, mean = 1, sd = 1)
yB <- rnorm(100, mean = yA + 0.1, sd = 1)
df_group <-
  tibble(y = c(yA, yB),
         group = factor(c(rep("A", 100), rep("B", 100))))
t.test(y ~ group, data = df_group)
summary(aov(y ~ group, data = df_group))
```

However, we can ask R to show summary _as if_ it was a linear model via `summary.lm()` function (`.lm` means that we call summary method for an `lm` linear model object). Again, note matching p-value for `group` parameter but also that now we can see the slope estimate for it. 
```{r}
summary.lm(aov(y ~ group, data = df_group))
```
The take home message here is that virtually all of the commonly used statistical analyses are linear regression custom tailored for a specific usage case and given a cool label. Because of that it might _look_ like there are big differences between methods but in reality they differ mostly in kind (only categorical, only continuous, both) and number of variables, plus on which parameters  you interpret and which you "ignore" as random effects. Again, this is a very large topic in itself and I can only encourage you once again to make a deeper dive in statistics via [Statistical Rethinking](https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919) book (or, if you a student at Uni Bamberg, join my seminar).

## (Generalized) Linear Models {#GLMM}

Base R provides function to perform linear regression [lm()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html) and generalized linear models [glm()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/glm.html) for binomial, count, and other types of data.

Let us generate a simple linear dependence between two parameters and see how `lm()` will infer the dependence. Our formula will be 
$$ y = 20 + 4 * x + \epsilon$$
where $\epsilon$ is normally distributed noise.

```{r}
df_lm <-
  tibble(x = 1:100) %>%
  mutate(y = rnorm(n(), 20 + 4 * x, 50))
```
```{r echo=FALSE}
ggplot(data=df_lm, aes(x=x, y=y)) + 
  geom_point() + 
  geom_abline(intercept = 20, slope = 4)
```

Now let us fit the linear model using formula `y ~ x` and use [summary()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/summary.lm.html) function to see the details.
```{r}
lm_fit <- lm(y ~ x, data=df_lm)
summary(lm_fit)
```

As you can see, values for both intercept and the slope are very close to our original. If you need to extract information about individual coefficients, I recommend [tidy()](https://broom.tidymodels.org/reference/tidy.lm.html) function from [broom](https://github.com/tidymodels/broom) package that returns information about the model in a "tidy" format:
```r
library(broom)
tidy(lm_fit)
```
```{r echo=FALSE}
library(broom)
tidy(lm_fit) %>%
  knitr::kable()
```

## Resampling a linear model
same rules still apply when We perform linear regression explicitly as linear regression (and not packaged as some statistical test): We are not interested in estimates alone but in our confidence (uncertainty) about them and about probability of a null hypothesis (e.g., an actual parameter value is zero). [lm()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html) function produces analytically derived estimates based on the same idea of drawing (infinitely) more samples from the "true" distribution. Let us repeat the same resampling (for confidence interval) and permutation (for statistical significance) tricks.

Generate a table with data with 30 values for `x` drawn from a normal distribution with zero mean and standard deviation of 10. $y = 20 +3 \cdot x + \epsilon$, where $\epsilon ~ Normal(0, 50)$, i.e., noise comes from a normal distribution centered at zero with standard deviation of 50.
```{r echo=FALSE}
df_lm <-
  tibble(x = rnorm(30, 0, 20)) %>%
  mutate(y = rnorm(n(), 20 + 2* x, 50))

ggplot(df_lm, aes(x=x, y=y)) + 
  geom_point()

summary(lm(y~x, df_lm))
```

Our first task is to resample `x` and `y` pairwise ([slice_sample()](https://dplyr.tidyverse.org/reference/slice.html) is one way to do this), fit lenear model to the resampled data, and extract coefficients via [tidy()](https://broom.tidymodels.org/reference/tidy.lm.html). Repeat that 1000 times to obtain a table with parameters and their estimates. There are different ways you can implement this. I have opted repeat it 1000 times via [map_dfr()](https://purrr.tidyverse.org/reference/map.html) that combines all tidy coefficient tables into a single table by row. Note that there was no need from me to write a separate function, as I have simply nested calls to [tidy()](https://broom.tidymodels.org/reference/tidy.lm.html), [lm()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html), and [slice_sample())]((https://dplyr.tidyverse.org/reference/slice.html)). Once you have samples it is a simple task to compute confidence interval for `estimate` of each parameter. 

```{r echo=FALSE, cache=TRUE}
resampled_est <- purrr::map_dfr(1:1000, ~tidy(lm(y ~ x, slice_sample(df_lm, replace=TRUE, prop=1))))

resampled_est %>%
  group_by(term) %>%
  summarize(`Lower 95% CI` = quantile(estimate, (1-0.95)/2),
            `Upper 95% CI` = quantile(estimate, 1-(1-0.95)/2),
            .groups="drop") %>%
  knitr::kable()
```

::: {.infobox .practice}
Do exercise 8.
:::

Next stop, statistical significance. Here, we want to shuffle one of the variables before fitting the model. Once we have our 1000 samples, we can compute confidence intervals on `statistic` and compute the proportion of values

```{r echo=FALSE, cache=TRUE}
permuted_stats <- purrr::map_dfr(1:2000, ~tidy(lm(y ~ x, df_lm %>% mutate(x = sample(x)))))


original_stats <-
  tidy(lm(y~x, df_lm)) %>%
  rename(original_statistic = statistic) %>%
  select(term, original_statistic)

left_join(permuted_stats, original_stats, by="term") %>%
  group_by(term) %>%
  summarize(p = mean(abs(statistic) > abs(original_statistic)), .groups="drop")

ggplot(permuted_stats, aes(x=statistic)) +
  geom_histogram(bins=50) +
  facet_wrap(.~term, scales="free")
```

## (Generalized) Linear Mixed Models
Generalized linear mixed models allow you to incorporate information about random factors into the model. One of the most popular packages in R is [lme4](https://github.com/lme4/lme4). Let us use LMM on data we generated for ANOVA. Here, we specify that we would like to have individual slopes for each participants via `(1|Participant)` notation^[Here, each participant must have their own intercept but they all share same single slope for `Condition`. You can also specify that they individual slopes that are either correlated or uncorrelated with their intercept.].

```{r message=FALSE, warning=FALSE}
library(lme4)
lmer_fit <- lmer(Response ~ Condition + (1|Participant), data=df_anova)
summary(lmer_fit)
```

To get tidy summary you need to use function `tidy` but from [broom.mixed](https://github.com/bbolker/broom.mixed)
```r
library(broom.mixed)
tidy(lmer_fit)
```
```{r echo=FALSE}
library(broom.mixed)
tidy(lmer_fit) %>%
  knitr::kable()
```

To also get information about formal statistical significance, you can use an extension package [lmerTest](https://github.com/runehaubo/lmerTestR)^[I use `lmerTest::` to tell R that I need function `lmer()` from package `lmerTest` and not from `lme4` as before].
```{r message=FALSE, warning=FALSE}
library(lmerTest)
lmert_fit <- lmerTest::lmer(Response ~ Condition + (1|Participant), data=df_anova)
summary(lmert_fit)
```

Again, we can use `tidy` to get coefficients information in a single table.
```r
tidy(lmert_fit)
```
```{r echo=FALSE}
tidy(lmert_fit) %>%
  knitr::kable()
```

Note that `lmer()` functions provide information about difference of each condition to the baseline (condition `A`) not no ANOVA-style significance for the "overall" effect of condition. For this, you can use function [drop1](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/add1.html), which test for a variable significance by dropping it from a model and checking whether it performed significantly worse _without_ it.
```{r}
drop1(lmert_fit)
```

You can also use Bayesian generalized linear modeling via [rstanarm](https://mc-stan.org/rstanarm/) and [BRMS](https://github.com/paul-buerkner/brms) packages. The former is somewhat faster, as it does not require model compilation (makes big difference when data is small but you test a lot of models), but is less flexible.

