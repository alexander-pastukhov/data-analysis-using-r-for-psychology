# Statistical modeling {#seminar10}
I suspect that this is a seminar that you were waiting for the most as it finally tells you how to call statistical functions in R. However, from my perspective, it is the least useful seminar of the entire course because if you know statistics and you know which specific tool you need, figuring out how to use it in R is fairly trivial. Conversely, if your knowledge of statistics is approximate, knowing how to call functions will do you little good. The catch about statistical models is that they are very easy to run (even if you implement them by hand from scratch) but they are easy to misuse and very hard to interpret^[In the Statistical Rethinking seminar we spend three seminars learning how to understand and interpret a simple linear multiple regression with just two predictors. And the conclusion is that even in this simple case, you are not guaranteed to fully understand it. And if you think that you can easily interpret an interaction term for two continuous predictors...].

To make things worse, computers and algorithms do not care. In absolute majority of cases, statistical models will happily accept any input you provide, even if it is completely unsuitable, and spit out numbers. Unfortunately, it is on you, not on the computer, to know what you are doing and whether results even make sense. The only solution to this problem: do not spare an effort to learn statistics. Having a solid understanding of a basic regression analysis will help you in figuring out which statistical tools are applicable and, even more importantly, which will definitely misguide you. This is why I will not explain when and why you should use a particular tool. Want to know more? Attend my Statistical Rethinking seminar or read an excellent [book](https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919) by Richard McElreath that the seminar is based on.

## Formula notation
Using statistical models in R is particularly easy because most packages make use of a [formula](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) to describe a model. Different functions and packages interpret the formula _mostly_ the same way with differences arising on how random effects or additional parameters are described. Here is an example of a formula:
```r
y ~ 1 + x1 + x2 + x1:x2
```

It says that the outcome variable `y` should be a modeled as a linear combination of an intercept `1` (can be omitted in a formula), predictor variables `x1` and `x2`, plus their interaction `x1:x2`. This also assume that all these variables are in a single table that you also supply to the function (typically called `data` parameter). Same formula can be shortened by using `*` which means "all predictors and their interaction", thus (omitting redundant intercept)

```r
y ~ x1*x2
```

You can also _exclude_ specific terms via `-`. So, if you insist that the intercept _must_ go through 0, you write "exclude intercept term" as `-1`
```r
y ~ x1*x2 - 1
```

Or you can exclude a specific term or an interaction. The two formulas below are equivalent with a main effect for `x2` and an interaction term but no main effect for `x1`. However, I would generally discourage you from using `-` as the first formula is much harder to understand. 
```r
y ~ x1*x2 - x1
y ~ x2 + x1:x2
```

## Correlation
To compute correlation, use function [cor()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.html). You can choose a `method`, either `"pearson"` (default, Pearson's product moment correlation coefficient), or rank-based `"kendall"` (Kendall's tau) or `"spearman"` (Spearman's rho).

```{r}
df <- 
  tibble(x = rnorm(100)) %>%
  mutate(y = rnorm(n(), x, 0.5))
cor(df$x, df$y, method="pearson")
```
Alternatively, you can use [cor.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.test.html) that also computes test statistics and significance.
```{r}
cor_result <- cor.test(df$x, df$y, method="pearson")
cor_result
```

`cor.test` returns a list, so you can access its individual elements via a double-bracket or `$` notation. 
```{r}
cor_result$p.value
```

Bayesian correlation with a posterior distribution for the correlation coefficient and Bayes Factor as a measure of significance, can be computed via `correlationBF()` function, which is part of [BayesFactor](https://richarddmorey.github.io/BayesFactor) package.
```{r}
library(BayesFactor)
correlationBF(df$x, df$y)
cor_bf <- correlationBF(df$x, df$y, posterior = TRUE, iterations=1000)
ggplot(data=NULL, aes(x=c(cor_bf[, "rho"]))) + 
  geom_histogram(bins=50, ) + 
  xlab("Pearson rho")
```


## Pairwise comparisons
Student's t-Test [t.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/t.test.html)

Wilcoxon Rank Sum and Signed Rank Tests [wilcox.test()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/wilcox.test.html)

Package [coin](http://coin.r-forge.r-project.org/) implements many tests, including permutation tests, such as Two- and K-sample Fisher-Pitman permutation test via `oneway_test()`, see [vignette](https://cran.r-project.org/web/packages/coin/vignettes/coin.pdf) for details. 

```{r}
diffusion <- data.frame(
    pd = c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46,
           1.15, 0.88, 0.90, 0.74, 1.21),
    age = factor(rep(c("At term", "12-26 Weeks"), c(10, 5)))
)

(wt <- wilcox_test(pd ~ age, data = diffusion,
                   distribution = "exact", conf.int = TRUE))

## Extract observed Wilcoxon statistic
## Note: this is the sum of the ranks for age = "12-26 Weeks"
statistic(wt, type = "linear")


oneway_test(pd ~ age, data = diffusion)
```


## (Generalized) Linear Models 

[lm()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html)

Generalized linear models
[glm()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/glm.html)

Bayesian regression via [rstanarm](https://mc-stan.org/rstanarm/) and [BRMS](https://github.com/paul-buerkner/brms).

## ANOVA

Basic analysis of variance [aov()](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/aov.html)

Repeated-measures ANOVA via `anova_test` from [rstatix](https://github.com/kassambara/rstatix) package.

Bayesian ANOVA via `anovaBF` function from [BayesFactor](https://richarddmorey.github.io/BayesFactor) package.


## (Generalized) Linear Mixed Models

Package [lme4](https://github.com/lme4/lme4) and [lmerTest](https://github.com/runehaubo/lmerTestR)

Bayesian hierarchical models via [rstanarm](https://mc-stan.org/rstanarm/) and [BRMS](https://github.com/paul-buerkner/brms).


