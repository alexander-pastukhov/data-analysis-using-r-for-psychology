[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"book (try ) teach perform typical data analysis tasks: reading data, transforming cleaning can visualize perform statistical analysis. covers topics need get started cover . One advantage R sheer size ecosystem new incredible libraries appearing much daily basis. Covering beyond scope book, instead concentrate (trying ) building solid understanding things need extend R knowledge. early chapters (e.g., vectors, tables, functions) might feel boring technical making wonder didn’t start exciting useful analysis, working way finer details. tried that1 , unfortunately, philosophy R many almost identical ways achieving end. learn finer details, waste time wondering seemingly code works one case fails mysterious ways one2. Therefore, please bear struggle vectors (everywhere!), oddities inconsistencies subsetting, learning write function even started use properly. can promise , personal experience, definitely worth effort.important note: book teach statistics machine learning beyond several examples end. reason teaches data preparation statistics machine learning 90% data preparation. obvious machine learning data acquisition, cleaning, feature engineering, etc. make suitable analysis take time. actual machine learning part boils trying various3 standard machine learning methods picking one gives best --sample performance. last part automated now requires little knowledge beyond details specific package. Knowing methods work obviously beneficial , hate write , critical machine learning (statistics deep learning!). true statistical methods, although time split preparing data statistical analysis interpreting comparing models. machine learning, running statistical models easy automatic. know (enough ) statistics, little trouble understanding work packages functions. , amount reading manuals make clearer.","code":""},{"path":"index.html","id":"goal-of-the-book","chapter":"Introduction","heading":"Goal of the book","text":"goal able program typical analysis pipelineLoading data csv/excel/online files data manipulation programs like SPSSPreprocessing data entire table per group (certain combination columns)\nmanipulating text information\nconverting variables factors\ntransforming numeric data\nfiltering data\nhandling missing data\nmanipulating text informationconverting variables factorstransforming numeric datafiltering datahandling missing dataSimplifying data structure \nselecting relevant columns\nreshaping table long (tidy) wide (human-readable) formats\nselecting relevant columnsreshaping table long (tidy) wide (human-readable) formatsSummarizing dataPlotting data using ggplot2 extensionsPerforming basic statistical analysis, reporting visualizing model predictions alongside data.Putting together single notebook can compiled (“knitted”) final document: report, presentation, seminar work, thesis, etc.programming concepts learn includeVariables data typesVectors, lists, tables foundation R dataWriting functionsConditional statementsLoopsFunctional programming, .e., applying function multple data points timePlease note exercises distributed chapter embedded text time point. designed clarify concepts apply knowledge presented , immediately helpful","code":""},{"path":"index.html","id":"why-r","chapter":"Introduction","heading":"Why R?","text":"many software tools allow preprocess, plot, analyze data. cost money (SPSS, Matlab), free just like R (Python, Julia). Moreover, can replicate analyses perform using Python combination Jupyter notebooks (reproducible analysis), Pandas (Excel-style table), statmodels (statistical analysis). R hardly perfect. example, subsetting system confusing appears follow “convenience safety” approach sit particularly well . However, R combination piping (easy way perform series computations table) Tidyverse family packages makes incredibly easy write simple, powerful expressive code, easy understand (huge plus, discover). run circles around trying replicate analysis Python Matlab (base R). addition, R loved mathematicians statisticians, tends implementations cutting edge statistical methods (Python go machine deep learning).","code":""},{"path":"index.html","id":"why-tidyverse","chapter":"Introduction","heading":"Why Tidyverse","text":"material heavily skewed towards using Tidyverse family packages. looks different enough base R point one might call “dialect” R4. Learning Tidyverse means twice many things learn: always introduce base R Tidyverse version. Tidyverse main reason use R (rather Python Julia) makes data analysis breeze makes life much easier. want learn ways. time, plenty useful code uses base R, need know understand well.matter fact, R rich flexible many dialects , therefore, plenty opinion differences 5. example, data.table package re-implements functionality base R Tidyverse compact way. fit style might something feels natural , encourage take look. also packages handle things like laying figures working summary tables might suit better. Point , material barely scratches surfaces terms tools approaches can use. View starting point exploration complete map.Another thing keep mind Tidyverse active development. means parts material outdated time read . E.g., dplyr () verb superseded group_modify() function, warning generated readr package adapted humans now requires extra step used column specification, now third set pivoting functions, etc. None changes breaking deprecation process deliberately slow (e.g., () still works), even outdated code book still work quite time. However, keep mind things might changed, good idea check official manual time time.","code":""},{"path":"index.html","id":"about-the-seminar-itself","chapter":"Introduction","heading":"About the seminar itself","text":"material Applied data analysis psychology using open-source software R seminar taught Institute Psychology University Bamberg. chapter covers single seminar, introducing necessary ideas accompanied notebook exercises, need complete submit. pass seminar, need complete assignments. need complete provide correct solutions exercises pass course information points exercises converted actual grade (need one) “pass” available seminar.material assumes foreknowledge R programming general reader. purpose gradually build knowledge introduce typical analysis pipeline. based data typical field (repeated measures, appearance, accuracy response time measurements, Likert scale reports, etc.) welcome suggest data set analysis. Even already performed analysis using program, still insightful compare different ways , perhaps, might gain new insight. Plus, engaging work data.Remember throughout seminar can (!) always ask whenever something unclear, understand concept logic behind certain code, simply got stuck. hesitate write team (better) directly chat (latter case, notifications harder miss don’t spam others conversation).","code":""},{"path":"index.html","id":"thinking-like-a-computer","chapter":"Introduction","heading":"Thinking like a computer","text":"exercises writing code reading understanding . job case “think like computer”. advantage computers dumb, instructions must written simple, clear, unambiguous way6. means , practice, reading code easy human. Well, reading well-written code easy, eventually encounter “spaghetti-code” easier rewrite scratch understand. case, simply go code line--line, computations hand writing values stored variables (many keep track ). go code manner, completely transparent . mysteries remain, doubts uncertainty (!) line. Moreover, can run code check values getting computer match . difference means made mistake code working differently think . case, 100% sure line code, ask , can go together!sense, important programming skill. impossible learn write, read code first! Moreover, programming probably spend time reading code making sure works correctly writing new code. Thus, use opportunity practice never use code understand completely. Thus, nothing wrong using stackoverflow never use code understand (blindly copy-paste)!","code":""},{"path":"index.html","id":"about-the-material","chapter":"Introduction","heading":"About the material","text":"material free use licensed Creative Commons Attribution-NonCommercial-NoDerivatives V4.0 International License.","code":""},{"path":"software.html","id":"software","chapter":"Software","heading":"Software","text":"","code":""},{"path":"software.html","id":"install-r","chapter":"Software","heading":"Installing R","text":"Go r-project.org download current stable version R platform. Run installer, accepting defaults.","code":""},{"path":"software.html","id":"install-rstudio","chapter":"Software","heading":"Installing R-Studio","text":"Go posit.co download RStudio Desktop edition platform. Install using defaults. R-Studio integrated development environment R need install R separately first! R-Studio automatically detect latest R , case several versions R installed, able alter choice via Tools / Global Options… menu.explain necessary details using R-Studio throughout seminar official cheatsheet excellent, compact, --date source information. fact, R Studio numerous cheatsheets describe individual packages compact form.","code":""},{"path":"software.html","id":"install-rtools","chapter":"Software","heading":"Installing RTools","text":"using Windows, might need Rtools building running packages. need install beginning, need later, just following link , download latest Rtools version, run installer using defaults follow instructions page put Rtools PATH!","code":""},{"path":"software.html","id":"install.packages","chapter":"Software","heading":"Installing packages","text":"real power R lies vast community-driven family packages suitable occasion. default repository used R R-Studio Comprehensive R Archive Network (.k.. CRAN). strict requirements submitted packages, makes annoying authors ensures high quality . use CRAN sole source packages. However, alternatives, Bioconductor might package missing CRAN. Bioconductor relies package manager, need consult latest manual website.install CRAN package two alternatives: via command line function via R-Studio package manager interface (call function ). former case, go Console tab type install.packages(\"package-name\"), example install.packages(\"tidyverse\"), press Enter.Alternatively, go Packages tab, click Install button, enter package name window (autocomplete help ), press Install.Sometimes, R ask whether want install packages source. case, grab source code compile package, takes time requires RTools. cases, can say “” install pre-build binary version. binary version slightly outdated emphasis slightly.occasions, R-Studio suggest restarting R Session packages need updated use. can , experience, become repetitive experience one packages used R Studio (starts new session, realizes use, suggests restart session, etc.) solution close R Studio windows use R directly. Windows, can find Start Menu, just make sure using correct version. , use install.packages() install update everything need.","code":""},{"path":"software.html","id":"minimal-setup-of-r-studio","chapter":"Software","heading":"0.1 Minimal setup of R-Studio","text":"big advantage R-Studio comes turn-key solution reasonable default settings. However, one convinience feature strongly recommend turning : automatic storing saving workspace. Go Tool / Global Options… General setting tab unselect “Restore .RData workspace startup” set “Save workspace .RData exit:” Never.feature just turned sounds great paper: enabled entire workspace (state program) saved automatically restored start RStudio . convenient, start state, loaded libraries variables/tables/functions left. However, experience, price pay may longer remember get state, common try things writing editing analysis. leaves libraries loaded (explicitly code), variables created modified hand (trace program), etc. things continue work computer, notebook code fail different one. debugging problems like one toughest cases taking ages solve every time make mistake like . price pay computation must repeated advantage gives another opportunity check analysis works previous results fluke.","code":""},{"path":"software.html","id":"minimal-set-of-packages","chapter":"Software","heading":"Minimal set of packages","text":"Please install following packages:tidyverse : includes packages data creation (tibble), reading (readr), wrangling (dplyr, tidyr), plotting (ggplot2). Plus type specific packages (stringr strings, forcats factors) functional programming (purrr).rmarkdown : package working RMarkdown notebooks, use create reproducible analysis.fs : file system utilities.","code":""},{"path":"software.html","id":"installr","chapter":"Software","heading":"Keeping R and packages up-to-date","text":"R packages getting constantly improved, good idea regularly update . packages, can use Tools / Check Packages Updates… menu R-Studio. Windows, update R , optionally, packages, can use installr package can install newest R (keeps old version!) optionally copying entire library packages, updating packages, etc. easy use even R , creates extra menu make interface simpler. R-Studio , use Help / Check Updates menu install newer version, available (generally good idea keep R-Studio newest state).","code":""},{"path":"reproducable-research.html","id":"reproducable-research","chapter":"1 Reproducable Research: Projects and Markdown Notebooks","heading":"1 Reproducable Research: Projects and Markdown Notebooks","text":"aim create reproducible research analysis. crucial component open science movement even important research study projects. want create self-contained well-documented easy--understand reproducible analysis. complete self-sufficient code others , importantly, future-can easily understand saves time gives deeper insight results (less mystery better cases like ). also makes easier communicate results researchers fellow students.also always consider posting results analysis online public repositories OSF, Science Data Bank, GitHub. help others forces making data + analysis archive thoroughly. , turns, makes easier future-return data analysis. Using GitHub (private) repository good idea even planning collaborate others gives version-controlled code cloud makes synchronizing different machines easy.","code":""},{"path":"reproducable-research.html","id":"projects","chapter":"1 Reproducable Research: Projects and Markdown Notebooks","heading":"1.1 Projects","text":"One annoying features R looks files folders relative “working directory”, set via setwd(dir) function. makes particularly confusing currently open file may folder. simply use File / Open, navigate file open , change working directory. Similarly, R-Studio can navigate file system using Files tab open folder interested make working directory. need click Set Working Directory make work (trick won’t work opened file).short, may look working particular folder R opinion . Whenever happens, really confusing involves lot cursing R find files can clearly see eyes. avoid , organize program, project seminar R Project, assumes necessary files project folder, also working directory. R Studio nice project-based touches well, like keeping tracking files open, providing version control, etc. Bottom line, always create new R-project organize , even involves just single file try something . Remember, “Nothing permanent temporary solution!” always write code, long term project (good style, comprehensible variable names, comments, etc.), otherwise temporary solution grows permanent incomprehensible spaghetti code.Let us create new project seminar. Use File / New Project…, give options creating new directory (get come name), using existing directory (project named directory), check remote repository (know git, convenient). can either way. project folder seminar need put notebooks external data files folder. Next time need open , can use File / Recent Projects menu, File / Open Project…_ menu, simply open .Rproj` file folder.","code":""},{"path":"reproducable-research.html","id":"rmarkdown","chapter":"1 Reproducable Research: Projects and Markdown Notebooks","heading":"1.2 Quattro and RMarkdown notebooks","text":"two similar notebook formats R: older R Markdown new Quatro. one hand, latter future, make sense use Quatro notebooks. hand, little practical difference , R-end-user point view, differ mostly specify chunk options (later).RMarkdown Quattro rely markdown language combine formatted text, figures, references (via bibtex) cross-references code7. notebook knitted, code ran output, tables figures, inserted final document. allows combine narrative (background, methodology, comments, discussion, conclusions, etc.) actual code implements described. , can sure figures numbers latest correct version.Notebooks can knitted variety formats including HTML, PDF, Word document, EPUB book, etc. Thus, instead creating plots tables save separate files can copy-paste Word file (redoing , something changed, trying find correct code used last time, wondering run anymore…), simply “knit” notebook get current complete research report, semester work, presentation, etc. Even importantly, goes others, also can knit notebook generate latest version format need. exercises involve using RMarkdown notebooks, need familiarize .start learning markdown, family human-oriented markup languages. Markup plain text includes formatting syntax can translated visually formatted text. example, HTML LaTeX markup languages. advantage markup need special program edit , plain text editor suffice. However, need special program turn plain text document. example, need Latex compile PDF browser view HTML properly. However, anyone can read original file even Latex, PDF reader, browser installed (need Word read Word file!). Markdown markup language designed make formatting simple unobtrusive, plain document easier read (can read HTML hardly fun!). feature-rich HTML LaTeX covers usual needs easy learn!Create new markdown file via File / New File / Quatro Document… menu. Use Seminar 1: Markdown title HTML default output format. new RStudio makes life easier giving WYSIWYG visual editor option, can use later, seminar disable learn use markdown directly\nSave file (press Ctrl + S use File/Save menu) calling seminar-01 (R Studio add .qmd extension automatically). file created empty, R Studio kind enough provide example (interestingly, get different example create RMarkdown file instead). Knit notebook clicking Knit button pressing Ctrl+Shift+K see properly typeset text look (appear Viewer tab).","code":""},{"path":"reproducable-research.html","id":"rmarkdown-template","chapter":"1 Reproducable Research: Projects and Markdown Notebooks","heading":"1.2.1 RMarkdown template","text":"Let us go default RMarkdown notebook R Studio created us (content default Quatro document) can see can create via File / New File / R Markdown… (R notebook!).\ntop part two sets --- notebook header various configuration options written YAML (yes, two different languages one file). title, author, date self-explanatory. output defines kind output document knitr generate. can specify hand (e.g., word_document) just click drop next Knit button pick option like (use default HTML time). sufficient us numerous options can specify, example, enable indexing headers. can read yihui.org/knitr.next section “setup code chunk” specifies default options code chunks treated default (whether executed, whether output, warnings, messages shown, etc.). default code chunks run output shown (echo = TRUE) can change behavior per-chunk basis pressing gear button top-right. setup chunk also good place import libraries (talk later) always run chunks (, even forgot run load libraries, R Studio ).Next, plain text rmarkdown, gets translated formatted text click Knit button. can write like anywhere outside code chunks explain logic analysis. write analysis performed leave technical details programming chunk , can comment code.Finally, first “proper” chunk code (“setup” chunk special case). code chunk simply code embedded \n```{r <name chunk} seconds set ticks ```. r specifies code inside written R language can use languages Python (via reticulate package), Stan, SQL. name chunk optional recommend specify , reminds code makes easier navigate large notebooks. bottom-left corner, can see chunk section currently , click , can quickly navigate different chunk. chunks explicitly named, get labels Chunk 1, Chunk 2, etc. making hard distinguish .additional options can specify per chunk (whether run code, show output, size figures , etc.). Generally won’t need options can get idea looking official manual. can create chunk hand click “Create chunk” drop-list (case, create chunk position cursor)Finally, run code chunk clicking Run current chunk button top-right corner chunk pressing Ctrl+Shift+Enter inside chunk. However, can also run just single line selected lines pressing Ctrl+Enter. cool thing RMarkdown RStudio see output chunk right . means can write code chunk--chunk, ensure works intended knit entire document. Run chunks notebook see mean.","code":""},{"path":"reproducable-research.html","id":"quatro-template","chapter":"1 Reproducable Research: Projects and Markdown Notebooks","heading":"1.2.2 Quatro template","text":"Working Quatro notebook similar certain YAML header options different (e.g., format: html instead output: html_document) chunk settings inside chunk (#| echo: false) instead inside curly brackets ({r echo=FALSE}). differences become important go deeper want build website write book using Quatro won’t matter much seminar.","code":""},{"path":"reproducable-research.html","id":"exercise","chapter":"1 Reproducable Research: Projects and Markdown Notebooks","heading":"1.3 Exercise","text":"today’s exercise, want familiarize markdown. Create Quatro notebook . Go markdownguide.org look basic extended syntax (cheat sheet also good). Write text want uses formatting submit .qmd file MS Teams. Please note use external image, must submit (zip everything single file) path use must relative. Remember, file path “c:/Documents/R-seminar/funny.png” chances file set folders computer markdown won’t render .","code":""},{"path":"vectors.html","id":"vectors","chapter":"2 Vectors! Vectors everywhere!","heading":"2 Vectors! Vectors everywhere!","text":"reading chapter, please download exercise notebook (Alt+Click download right-click Save link …), put seminar project folder, open project. need text notebook exercises open, switching .can start using R analysis, need learn vectors. key concept R, understanding determine easy use R general. exercises hesitate ask whenever something unclear. Remember, need master vectors can master R!","code":""},{"path":"vectors.html","id":"variables","chapter":"2 Vectors! Vectors everywhere!","heading":"2.1 Variables as boxes","text":"programming, concept variable often described box can put something . box name tag , name variable. Whatever put value store.“putting ” concepts reflected R syntaxHere, number_of_participants name variable (name tag box using), 10 value store, <- means “put 10 variable number_of_participants”. know programming languages, probably expected usual assignment operator =. Confusingly, can use R well, subtle, yet important, differences operate behind scenes. meet = talking functions , particular, Tidyverse way things now use <- operator!","code":"\nnumber_of_participants <- 10"},{"path":"vectors.html","id":"assignment-statement","chapter":"2 Vectors! Vectors everywhere!","heading":"2.2 Assignment statement in detail","text":"One important thing remember assignment statement <variable> <- <value>: right side evaluated first final value established , , stored <variable> specified left side. means can use variable sides. Take look exampleWe storing value 2 variable x. next line, right side evaluated first. means current value x substituted place right side: x + 5 becomes 2 + 5. expression computed get 7. Now, right side fully evaluated, value can stored x replacing (overwriting) original value .R’s use <- makes easier memorize right side fully evaluated first rule. However, noted , meet = operator one makes look like mathematical equation. However, assignments (storing values variable) nothing common mathematical equations (finding values variables ensure equality)!exercise 1.","code":"\nx <- 2\nprint(x)## [1] 2\nx <- x + 5\nprint(x)## [1] 7"},{"path":"vectors.html","id":"vectors-scalars","chapter":"2 Vectors! Vectors everywhere!","heading":"2.3 Vectors and singluar values (scalars, which are also vectors)","text":"box metaphor ’ve just learned, doesn’t quite work R. Historically, R developed language statistical computing, based concepts linear algebra instead “normal” programming language like Python C. means conceptual divide single values containers (arrays, lists, dictionaries, etc.) hold many single values. Instead, primary data unit R vector. computer science point view, vector just list numbers (values, learn later). means “single values” R, vectors variable length. Special cases vectors length one, called scalars 8 (still vectors) zero length vectors , sort , Platonic idea vector without actual values. respect “box metaphor”, means always box indexed (numbered) slots . simple assignment makes sure “box” many slots values want put stores values one another starting slot #19. Therefore, example number_of_participants <- 10 creates vector variable one (1) slot stores value ., noted , single value (vector length one) special case. generally write:, create variable (box) named response three slots want store three values. put values 1, 7, 3 slots #1, #2, #3. c(1, 7, 3) notation create vector R concatenating (combining) values10. figure illustrates idea:Building box metaphor: can store something box, can take ! world computers works even better, rather taking something , just make copy store copy somewhere else use compute things. Minimally, like see inside box. , can use print function:, can make copy values one variable store another:, create 3-slot variable x can put vector length 3 created via concatenation c(3, 6, 9). Next, make copy three values store different variable y. Importantly, values variable x stayed . Take look figure , graphically illustrate :\nexercise 2.Remember, everything vector! means c(3, 6, 9) concatenate numbers, concatenates three length one vectors (scalars) 3, 6, 9. Thus, concatenation works longer vectors exactly way:exercise 3.","code":"\nresponse <- c(1, 7, 3)\nresponse <- c(1, 7, 3)\nprint(response)## [1] 1 7 3\nx <- c(3, 6, 9)\ny <- x \n\nprint(x)## [1] 3 6 9\nprint(y)## [1] 3 6 9\nx <- c(1, 2, 3)\ny <- c(4, 5)\nprint(c(x, y))## [1] 1 2 3 4 5"},{"path":"vectors.html","id":"vector-index","chapter":"2 Vectors! Vectors everywhere!","heading":"2.4 Vector indexes (subsetting)","text":"vector ordered list values (box slots) , sometimes, need one values. value (slot box) index 1 till N, N length vector. access slot use square brackets some_vector[index]. can get set value individual slots way whole vector.exercise 4.Unfortunately, vector indexing R behaves way may11 catch surprise. , even worse, even notice indexing work screwed analysis. vector contains five values, expect index 0 (negative indexes special discussed ) 5 generates error. R! Index 0 special case produces empty vector (vector zero length).try get vector element using index larger vector length (6 5 element vector), R return NA (“Available” / Missing Value).cases, won’t generate error even warn !setting value index, using 0 produce effect, trying put value vector “slots”. Oddly enough, also generate neither error warning, beware!set element index larger vector length, vector automatically expanded length elements old values new one NA (“Available” / Missing Value).may sound technical want learn R conventions different programming languages , also, intuitively expect. aware highly peculiar rules, may never realize code working properly , remember, never see error even warning! also make cautious careful programming R. powerful language allows flexible expressive. Unfortunately, flexibility means base R won’t stop shooting foot. Even worse, sometimes won’t even notice foot bleeding R won’t generate either errors warnings, examples . Good news things far restricted consistent Tidyverse.exercise 5.can also use negative indexes. case, exclude value index return modify rest12.Given negative indexing returns everything indexed value, think happen ?exercise 6.Finally, somewhat counterintuitively, entire vector returned specify index square brackets. , lack index means “everything”.","code":"\nx <- c(1, 2, 3)\n# set SECOND element to 4\nx[2] <- 4\n\n# print the entire vector\nprint(x)## [1] 1 4 3\n# print only the third element\nprint(x[3])## [1] 3\nx <- c(1, 2, 3)\nx[0]## numeric(0)\nx <- c(1, 2, 3)\nx[5]## [1] NA\nx <- c(1, 2, 3)\nx[0] <- 5\nprint(x)## [1] 1 2 3\nx <- c(1, 2, 3)\nx[10] <- 5\nprint(x)##  [1]  1  2  3 NA NA NA NA NA NA  5\nx <- c(1, 2, 3, 4, 5)\n# this will return all elements but #3\nx[-3] ## [1] 1 2 4 5\nx <- c(1, 2, 3, 4, 5)\n# this will assign new value (by repeating length one vector) to all elements but #2\nx[-2] <- 10\nx## [1] 10  2 10 10 10\nx <- c(10, 20, 30, 40, 50)\nx[-10]\nx <- c(10, 20, 30, 40, 50)\nx[]## [1] 10 20 30 40 50"},{"path":"vectors.html","id":"names","chapter":"2 Vectors! Vectors everywhere!","heading":"2.5 Names as an Index","text":"’ve just learned, every slot vector numeric (integer) index. However, number indicates index (position) slot tells nothing conceptually different slot different index. example, storing width height vector, remembering order may tricky: box_size <- c(<width>, <depth>, <height>) box_size <- c(<height>, <width>, <depth>)? Similarly, looking box_size[1] tells definitely using first dimension height width (depth)?R, can use names supplement numeric indexes. allows add meaning particular vector index, something becomes extremely important use tables. two ways assign names indexes, either creating index via c() function , afterwards, via names() function.create named vector via c() specify name value c(<name1> = <value1>, <name2> = <value2>, ...):Note names appearing value. can now use either numeric index name access value.Alternatively, can use names() function get set names. latter works via counterintuitive syntax names(<vector>) <- <vector--names>everything vector, names(<vector>) also vector, meaning can get set just one element .Finally, use name index, like using numeric index larger vector length. Just --range numeric index, neither error warning get NA back.exercise 7.","code":"\nbox_size <- c(\"width\"=2, \"height\"=4, \"depth\"=1) \nprint(box_size)##  width height  depth \n##      2      4      1\nbox_size <- c(\"width\"=2, \"height\"=4, \"depth\"=1) \nprint(box_size[1])## width \n##     2\nprint(box_size[\"depth\"])## depth \n##     1\n# without names\nbox_size <- c(2, 4, 1) \nprint(box_size)## [1] 2 4 1\n# with names\nnames(box_size) <- c(\"width\", \"height\", \"depth\")\nprint(box_size)##  width height  depth \n##      2      4      1\n# getting all the names\nprint(names(box_size))## [1] \"width\"  \"height\" \"depth\"\nbox_size <- c(\"width\"=2, \"height\"=4, \"depth\"=1) \n\n# modify SECOND name\nnames(box_size)[2] <- \"HEIGHT\"\nprint(box_size)##  width HEIGHT  depth \n##      2      4      1\nbox_size <- c(\"width\"=2, \"height\"=4, \"depth\"=1) \nprint(box_size[\"radius\"])## <NA> \n##   NA"},{"path":"vectors.html","id":"vector-index-slicing","chapter":"2 Vectors! Vectors everywhere!","heading":"2.6 Slicing","text":"far reading modifying either whole vector just one elements. However, index pass square brackets (’ve guessed ) also vector! means can construct vector indexes way construct vector values (restriction index values must integers mix negative positive indexes).constructing vector index, can put index values order require (normal ascending order, starting end , random order, etc.) use index .can also use several negative indexes exclude multiple values return rest. , neither order duplicate indexes matter. Regardless value exclude first many times exclude , still get rest vector default order.Note mix positive negative indexes R generate error (last!).Finally, including zero index makes difference generates neither error warning.can also use names instead numeric indexes.However, mix numeric indexes names. reason vector can hold values one type (next time), numeric values converted text (1 become \"1\") treated names rather indexes.","code":"\nx <- c(10, 20, 30, 40, 50)\nx[c(2, 3, 5)]## [1] 20 30 50\nx <- c(10, 20, 30, 40, 50)\nx[c(3, 5, 1, 1, 4)]## [1] 30 50 10 10 40\nx <- c(10, 20, 30, 40, 50)\nx[c(-4, -2, -2)]## [1] 10 30 50\nx <- c(10, 20, 30, 40, 50)\n\n# THIS WILL GENERATE AN ERROR: \nx[c(-4, 2, -2)]## Error in x[c(-4, 2, -2)]: only 0's may be mixed with negative subscripts\nx <- c(10, 20, 30, 40, 50)\nx[c(1, 0, 5, 0, 0, 2, 2)]## [1] 10 50 20 20\nbox_size <- c(\"width\"=2, \"height\"=4, \"depth\"=1) \nprint(box_size[c(\"height\", \"width\")])## height  width \n##      4      2"},{"path":"vectors.html","id":"colon-sequence","chapter":"2 Vectors! Vectors everywhere!","heading":"2.7 Colon Operator and Sequence Generation","text":"simplify vector indexing, R provides shortcut create range values. expression :B (.k..Colon Operator) builds sequence integers starting ending including(!) B13.Thus, can use easily create index , everything vector!, combine values.sequence increasing can also use colon operator construct decreasing one.colon operator limited sequences steps 1 (end value larger start value) -1 (end value smaller start value). flexibility can use Sequence Generation function: seq(, , , length.). starting ending values (just like colon operator) can specify either step via parameter (, “B C”) via length.parameter (many values want generate, effectively = ((- )/(length.- 1)).\nUsing version:sequence using length.version:probably spotted = symbol. , assignment used specify values parameters call function. Thus, still sticking <- outside function calls using = inside function calls.exercise 8.","code":"\n3:7## [1] 3 4 5 6 7\nx <- c(10, 20, 30, 40, 50)\nx[c(1, 3:5)]## [1] 10 30 40 50\nx <- c(10, 20, 30, 40, 50)\nx[c(5:2)]## [1] 50 40 30 20\nseq(1, 5, by = 2)## [1] 1 3 5\nseq(1, 5, length.out = 3)## [1] 1 3 5"},{"path":"vectors.html","id":"working-with-two-vectors-of-equal-length","chapter":"2 Vectors! Vectors everywhere!","heading":"2.8 Working with two vectors of equal length","text":"can also use mathematical operations several vectors. , vectors matched element-wise. Thus, add two vectors equal length, first element first vector added first element second vector, second element second, etc.exercise 9.","code":"\nx <- c(1, 4, 5)\ny <- c(2, 7, -3)\nz <- x + y\nprint(z)## [1]  3 11  2"},{"path":"vectors.html","id":"different-length-vectors","chapter":"2 Vectors! Vectors everywhere!","heading":"2.9 Working with two vectors of different length","text":"vectors different length? Unfortunately, R solution error warning. Rather, length longer vector multiple shorter vector length, shorter vector “recycled” , .e., repeated N-times (\\(N = length(longer~vector) / length(shorter~vector)\\)) length-matched vector used mathematical operation. Please note generate neither error, warning! another example “convenience safety” approach R, careful always double-check length vectors. Otherwise, code work incorrectly , lucky, might notice .see recycling works, take look results following computationHere, values y repeated three times match length x, actual computation c(1, 2, 3, 4, 5, 6) + c(2, 3, 2, 3, 2, 3). vector length 1 (scalar) special case integer multiple 1, single value repeated length(longer_vector) times operation performed., actual computation c(1, 2, 3, 4, 5, 6) + c(2, 2, 2, 2, 2, 2).length longer vector multiple shorter vector length, R repeat shorter vector N times, \\(N = ceiling(length(longer~vector) / length(shorter~vector))\\) (ceiling() rounds number ) truncates (throws away) extra elements need. Although R , also issue warning (yay!) mismatching objects’ (vectors’) lengths.Finally, combining vector null length vector produces null length vector14.One thing keep mind: R length-matching-via-vector-repetition automatically shows warning two lengths multiples . means vectors matched length even plan. Imagine vector, contains experimental condition (e.g. contrast stimulus), ten blocks participants performed vector responses , accidentally, block #1. R silently(!) replicate responses 10 times match length without ever(!) telling . Thus, make sure vectors matched length, caught surprised behavior (can use function length() ). Good news, much strict Tidyverse, designed make shooting foot much harder.exercise 10.","code":"\nx <- 1:6\ny <- c(2, 3)\nprint(x + y)## [1] 3 5 5 7 7 9\nx <- 1:6\ny <- 2\nprint(x + y)## [1] 3 4 5 6 7 8\nx <- c(2, 3)\ny <- c(1, 1, 1, 1, 1)\nprint(x + y)## Warning in x + y: longer object length is not a multiple of shorter object\n## length## [1] 3 4 3 4 3\nx <- c(2, 3)\ny <- c(1, 1, 1, 1, 1)\nprint(x + y[0])## numeric(0)"},{"path":"vectors.html","id":"applying-functions-to-a-vector","chapter":"2 Vectors! Vectors everywhere!","heading":"2.10 Applying functions to a vector","text":"mention everything vector? means using function, always applying vector. , turn, means apply function values one go. example, can compute cosine values vector.contrast, Python C need loop values compute cosine one value time (matrix-based NumPy library different story). think Excel, need extend formula rows row computed independently (can deliberately accidentally miss rows). R, everything vector, function applied every value automatically. Similarly, using aggregating functions, mean() max(), can pass vector return length-one vector value.","code":"\ncos(c(0, pi/4, pi/2, pi, -pi))## [1]  1.000000e+00  7.071068e-01  6.123032e-17 -1.000000e+00 -1.000000e+00\nmean(c(1, 2, 6, 9))## [1] 4.5"},{"path":"vectors.html","id":"wrap-up","chapter":"2 Vectors! Vectors everywhere!","heading":"2.11 Wrap up","text":"now learned vectors, vector indexing, vector operations R probably bargained . Admittedly, exciting topic. top , single word psychology data analysis! However, R obsessed vectors (everything vector!) understanding make easier understand lists (polyamorous cousin vector), tables (special kind lists made vectors), functional programming (R built ). Finish seminar remaining exercises. Let’s see whether R can still surprise !exercises 11-17.","code":""},{"path":"tables.html","id":"tables","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3 Tables and Tibbles (and Tribbles)","text":"Please download exercise notebook (Alt+Click download right-click Save link …), put seminar project folder open project. need text notebook exercises open, switching .","code":""},{"path":"tables.html","id":"primary-types","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.1 Primary data types","text":"Last time talked fact everything15 vector R. examples chapter used numeric vectors two four primary types R.numeric: Real numbers (double precision floating point numbers precise) can written decimal notation without decimal point (123.4 42) scientific notation (3.14e10). two special values specific real numbers: Inf (infinity, get divide non-zero number zero) NaN (number). latter looks similar NA (Available / Missing Value) different special case generated trying perform mathematically undefined operation like diving zero zero computing sine infinity (see R documentation details).integer: Integer numbers can specified adding L end integer number 5L. Without L real value created (5 stored 5.0).logical: Logical Boolean values TRUE FALSE. can also written T F practice discouraged Tidyverse style guide.character: Character values (strings) hold text pair matching \" ' characters. two options mean can surround text ' need put quote inside: '\"never let schooling interfere education.\" Mark Twain' \" need apostrophe \"participant's response\". Note string vector characters. make lot sense . indexing slicing work need use special functions cover later chapter.can convert one type another check whether particular vector specific type. Note vector converted specified type, “converted” NA instead.Convert integer via .integer(), use .integer() check whether vector consist integers. converting\nreal number fractional part discarded, .integer(1.8) → 1 .integer(-2.1) → 2\nlogical value .integer(TRUE) → 1 .integer(FALSE) → 0\nstring properly formed number, e.g., .integer(\"12\") → 12 .integer(\"_12_\") NA. Note real number string converted first real number integer .integer(\"12.8\") → 12.\nNA → NA\nreal number fractional part discarded, .integer(1.8) → 1 .integer(-2.1) → 2from logical value .integer(TRUE) → 1 .integer(FALSE) → 0from string properly formed number, e.g., .integer(\"12\") → 12 .integer(\"_12_\") NA. Note real number string converted first real number integer .integer(\"12.8\") → 12.NA → NAConvert real number via .numeric() / .double(), check value via .double() (avoid .numeric() Hadley Wickham writes think ).\nlogical value .double(TRUE) → 1.0 .double(FALSE) → 0.0\nstring properly formed number, e.g. .double(\"12.2\") → 12.2 .double(\"12punkt5\") NA\nNA → NA\nlogical value .double(TRUE) → 1.0 .double(FALSE) → 0.0from string properly formed number, e.g. .double(\"12.2\") → 12.2 .double(\"12punkt5\") NAfrom NA → NAConvert logical TRUE/FALSE via .logical check value via .logical().\ninteger real, zero (0 0.0) FALSE, non-zero value TRUE\nstring, TRUE \"TRUE\", “True\", \"true\", \"T\" NA \"t\" \"TRue\", \"truE, etc. goes FALSE.\nNA → NA\ninteger real, zero (0 0.0) FALSE, non-zero value TRUEfrom string, TRUE \"TRUE\", “True\", \"true\", \"T\" NA \"t\" \"TRue\", \"truE, etc. goes FALSE.NA → NAConvert character string via .character() check via .character()16\nnumeric values converted string representation scientific notation used large numbers.\nlogical TRUE/T FALSE/T converted \"TRUE\" \"FALSE\".\nNA → NA\nnumeric values converted string representation scientific notation used large numbers.logical TRUE/T FALSE/T converted \"TRUE\" \"FALSE\".NA → NADo exercise 1.","code":""},{"path":"tables.html","id":"vectors-are-homogeneous","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.2 All vector values must be of the same type","text":"values vector must type — integer, double, logical, strings, also called atomic vectors. ensures can apply function operation entire vector without worrying type compatibility. However, means mix different value types vector. try concatenate vectors different types, values converted general / flexible type. Thus, mix numbers logical values, end vector numbers. Mixing anything strings convert entire vector string. Mixing NA change vector type.exercise 2.","code":""},{"path":"tables.html","id":"lists","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.3 Lists","text":"(Atomic) vectors homogeneous lists primary types items, whereas lists lists can contain item including vectors different primary type data, lists, objects17. Otherwise, work almost way vectors. create list via list() function can concatenate lists via c() function, access individual elements via numeric index names, use slicing, etc. …","code":""},{"path":"tables.html","id":"subsetting","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.4 Subsetting, a fuller version","text":"Unfortunately, point need talk subsetting (accessing individual elements vector list via indexes names) yet . R several18 different ways look similar , make things worse, sometimes produce identical results. expect memorize even fully appreciate intricate details (even sure know ). need understand fundamental difference two ways subsetting aware potential issues can easily cause confusion. latter case, return section, read official manual, read subsetting section “Advanced R” book Hadley Wickham.already know subsetting via single square brackets: x[1]. called preserving subsetting returns part object (vector list) requested . .e., used access part vector, get vector. used access list table, get list table.addition, can use double square brackets: x[[1]]. called simplifying subsetting extract content location. list vectors l l[2] (preserving) return single item list (vector item inside list) l[[2]] return vector stored location. metaphor based @RLangTip: “list x train carrying objects, x[[5]] object car 5; x[5] train consisting car 5.”Note simplifying notation allows extract content one item. Therefore, use slicing (extracting content many items), negative indexes (even end just one index), zero index. good news R actually generate error message instead failing silently, although error messages mysterious different depending exact circumstances. deepen feeling arbitrariness R design: using non-existent name index generate error vectors lists (get NULL)19.Confusing? Let us go examples, can see difference two kinds brackets make vectors lists.","code":""},{"path":"tables.html","id":"subsetting-for-atomics-vectors","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.4.1 Subsetting for (atomics) vectors","text":"First, create named vectorPreserving subsetting via [] returns part original vector, notice name associated index retained.Simplifying subsetting via [[]] extracts value location. name associated index, content, gets stripped .Trying extract multiple items fail simplifying subsetting.true negative indexes, even though excluding two three indexes, end content just one item.zero index generates error well error message different.Using non-existent name generate error simplifying preserving subsetting.also works strips name .fails silently returning NA.generate error.General rule: using vectors, always use []. potential usage-case can see wanting strip name single item (sure ever needed ). , reason, need, write comment explaining . Otherwise, everyone including future-confused think typo.","code":"\nx <- c(\"a\"=1, \"b\"=2, \"c\"=3)\nx[2]## b \n## 2\nx[[2]]## [1] 2\nx[[1:2]]## Error in x[[1:2]]: attempt to select more than one element in vectorIndex\nx[[-(1:2)]]## Error in x[[-(1:2)]]: attempt to select more than one element in vectorIndex\nx[[0]]## Error in x[[0]]: attempt to select less than one element in get1index <real>\nx[\"a\"]## a \n## 1\nx[[\"a\"]]## [1] 1\nx[\"d\"]## <NA> \n##   NA\nx[[\"d\"]]## Error in x[[\"d\"]]: subscript out of bounds"},{"path":"tables.html","id":"list-subsetting","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.4.2 Subsetting for lists","text":"First, create named list includes another list one itemsPreserving subsetting via [] returns list part original list, name associated index, retained returned list.Simplifying subsetting via [[]] extracts value location. get vector (length 1).list inside list position 3? Using preserving subsetting give us list inside list (car train). , note top level list retains name.Using simplifying subsetting return list inside list location (object inside car). Note , vector , name gets stripped , belongs index (car train) content (object inside car)., using multiple indexes fail.negative indexe.zero index (notice different error message).Using non-existent name index lists preserving subsetting ([]) return single item list NULL content20 just NULL simplifying one ([[]]). none generate error.General rule: use [[]] interested list table content, e.g., need column table. Use [] interested using smaller (part original) list table.point want start running around screaming bang head screen, please tell , cause definitely join . Let us exercises might help build intuition. However, probably noticed already, subsetting R fairly haphazard, probably still end confused code work seemingly mysterious reasons.exercise 3.","code":"\nl <- list(\"a\"=1, \"b\"=2, \"c\"=list(3, 4, 5))\nl[2]## $b\n## [1] 2\nl[[2]]## [1] 2\nl[\"c\"]## $c\n## $c[[1]]\n## [1] 3\n## \n## $c[[2]]\n## [1] 4\n## \n## $c[[3]]\n## [1] 5\nl[[\"c\"]]## [[1]]\n## [1] 3\n## \n## [[2]]\n## [1] 4\n## \n## [[3]]\n## [1] 5\nl[[1:2]]## Error in l[[1:2]]: subscript out of bounds\nl[[-(1:2)]]## Error in l[[-(1:2)]]: attempt to select more than one element in integerOneIndex\nl[[0]]## Error in l[[0]]: attempt to select less than one element in get1index <real>\nl[\"d\"]## $<NA>\n## NULL\nl[[\"d\"]]## NULL"},{"path":"tables.html","id":"dollar-subsetting","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.5 Yet another subsetting via $","text":"terribly sorry need deepen confusion despair telling R yet another way subsetting via $ sign. fact, common way subsetting lists tables encounter almost everywhere. shortcut version partial matching simplifying subsetting: l$c writing l[[\"c\", exact = FALSE]]. last new part exact = FALSE means can use incomplete name $ R guess mean. , made typo meaning completely different column name, R second-guess intentions give variable beginning (error, don’t even hope).good news won’t autocomplete ambiguous cases. bad news won’t generate error return NULL (just like [[]] ).advice stick [[]] notation despite ubiquitous use $ . [[]] slightly strict universal: can hard code column/item name put variable indexing. hard-coding works $.","code":"\nl <- list(\"aa\" = 1, \"bb\" = 2, \"bc\" = 3)\nl$a## [1] 1\nl$b## NULL\nl[['b']]## NULL\nl[[\"bb\"]]## [1] 2\ncolumn_of_interest <- \"bb\"\nl[[column_of_interest]]## [1] 2"},{"path":"tables.html","id":"data.frame","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.6 Tables, a.k.a. data frames","text":"R tables mix lists matrices. can think list vectors (columns) identical length. default way construct table, called data frames R, via data.frame() function.create table, appear environment, can see Environment tab view clicking typing View(table_name) console (note capital V View()).\nexercise 4.columns table must number items (rows). vectors different length, R match length via recycling shorter vectors. similar process matching vectors’ length learned last time. However, works automatically length vectors multiple longest length. Thus, example work, longest vector (numeric_column) 6, character_column length 3, repeated twice, logical_column length 2 repeated thrice.simple multple--length rule work, R generates error (finally!).exercise 5.","code":"\nour_first_table <- data.frame(numeric_column = c(1, 2, 3, 4), \n                              character_column = c(\"A\", \"B\", \"C\", \"D\"),\n                              logical_column = c(TRUE, F, T, FALSE))\n\nour_first_table##   numeric_column character_column logical_column\n## 1              1                A           TRUE\n## 2              2                B          FALSE\n## 3              3                C           TRUE\n## 4              4                D          FALSE\nthe_table <- data.frame(numeric_column = 1:6,                  # length 6 \n                        character_column = c(\"A\", \"B\", \"C\"),   # length 3\n                        logical_column = c(TRUE, FALSE))       # length 2\nthe_table##   numeric_column character_column logical_column\n## 1              1                A           TRUE\n## 2              2                B          FALSE\n## 3              3                C           TRUE\n## 4              4                A          FALSE\n## 5              5                B           TRUE\n## 6              6                C          FALSE\n# this will generate an error: arguments imply differing number of rows\nthe_table <- data.frame(numeric_column = 1:7,                 # length 7\n                        character_column = c(\"A\", \"B\", \"C\"))  # length 3, cannot be multiplied by an integer to get 7## Error in data.frame(numeric_column = 1:7, character_column = c(\"A\", \"B\", : arguments imply differing number of rows: 7, 3"},{"path":"tables.html","id":"table-subsetting","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.7 Subsetting tables","text":"One way think table list columns (vectors). Hence, preserving ([]) simplifying ([[]]) subsetting work expect returning either data.frame ([]) vector column interested ([[]]).preserving returns table one column.simplifying returns vector.new thing , tables two-dimensionsional, can use preserving subsetting extract access rectangular region within table. select subset rows columns write table[rows, columns]. omit either rows columns implies want rows columns.exercise 6.","code":"\nour_first_table <- data.frame(numeric_column = c(1, 2, 3), \n                              character_column = c(\"A\", \"B\", \"C\"),\n                              logical_column = c(TRUE, F, T))\n\n# via index\nour_first_table[1]##   numeric_column\n## 1              1\n## 2              2\n## 3              3\n# via name \nour_first_table['numeric_column']##   numeric_column\n## 1              1\n## 2              2\n## 3              3\n# via slicing\nour_first_table[1:2]##   numeric_column character_column\n## 1              1                A\n## 2              2                B\n## 3              3                C\n# via $ notation\nour_first_table$numeric_column## [1] 1 2 3\n# via name and double square brackets\nour_first_table[['numeric_column']]## [1] 1 2 3\n# via index and double square brackets\nour_first_table[[1]]## [1] 1 2 3\n# getting ALL rows for the FIRST column -> confusingly this gives you a VECTOR, \n# so even though you used [] they work as simplifying subsetting\nour_first_table[, 1]## [1] 1 2 3\n# getting FIRST row for ALL columns -> this gives you DATA.FRAME\nour_first_table[1, ]##   numeric_column character_column logical_column\n## 1              1                A           TRUE\n# ALL rows and ALL columns, equivalent to just writing `our_first_table` or `our_first_table[]`\n# this gives you DATA.FRAME\nour_first_table[,]##   numeric_column character_column logical_column\n## 1              1                A           TRUE\n## 2              2                B          FALSE\n## 3              3                C           TRUE\n# getting SECOND element of the THIRD column, returns a VECTOR\nour_first_table[2, 3]## [1] FALSE\n# getting first two elements of the logical_column, returns a VECTOR\nour_first_table[1:2, \"logical_column\"]## [1]  TRUE FALSE"},{"path":"tables.html","id":"library","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.8 Using libraries","text":"better way construct table use , need first import library implements . modern programming languages, real power R comes bundled (little, matter fact) community developed libraries extend . already discussed install libraries.use library code, use library() function21. , use tidyverse library already installed, simply writeOne thing keep mind import two libraries function name, function latter package overwrite (mask) function former. get warning miss , may confusing. favorite stumbling block functions filter() dplyr package (use extensively, filters table row) filter() function signal package (applies filter time-series)22. overwriting one function another can lead odd looking mistakes. case think using dplyr::filter() get confused error messages get (really informative). first time , took hour figure . warnings paid attention .Thus, keep mind , better still, explicitly mention package function coming via library::function() notation. case, use function interested need worry functions name may conflict . general, good idea always disambiguate function via library practice may make code hard read cluttering library:: prefixes. Thus, need find balance disambiguation readability.using notebook (, case, always) put libraries first chunk notebook. case Quatro Rmarkdown notebooks, call chunk “setup”, RStudio run least ones chunk, ensuring libraries always initialized (take look first chunk exercise notebook). Word advice, keep library list alphabetical order. Libraries specialized, need quite typical analysis. Keeping alphabetically organized makes easier see whether imported required library whether need install new one.Another note, sometimes may need install packages might tempting include install.packages(\"library--needed\") notebook R script. , run installation every time somebody else run chunk/script knit notebook, waste time, lead errors (offline), mess packages, etc. RStudio smart enough detect missing packages display alert , offering install .\n","code":"\nlibrary(tidyverse)\n# or\nlibrary(\"tidyverse\")library(signal)\n\n\nAttaching package: ‘signal’\n\nThe following object is masked from ‘package:dplyr’:\n\n    filter\n\nThe following objects are masked from ‘package:stats’:\n\n    filter, poly\nlibrary(tibble)\n\n# imported from the library into the global environment\nprint(tribble(~a, 1))## # A tibble: 1 × 1\n##       a\n##   <dbl>\n## 1     1\n# used directly from the package\ntibble::tribble(~a, 1)## # A tibble: 1 × 1\n##       a\n##   <dbl>\n## 1     1"},{"path":"tables.html","id":"tibble","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.9 Tibble, a better data.frame","text":"Although data.frame() function default way creating table, legacy implementation numerous shortcomings. Tidyverse implemented version table called tibble() provides rigorous control consistent behavior. example, allows use symbol columns names (including spaces), prints beginning table rather entire table, etc. also gives warnings. try access non-existing column data.frame() tibble() return NULL former silently, whereas latter give warning use $ notation23.short, tibble() provides robust version data.frame otherwise behaves (mostly) identically . Thus, default choice table.","code":"\nlibrary(tibble)\n\n# data.frame will return NULL silently\ndf <- data.frame(b = 1)\nprint(df[[\"A\"]])## NULL\n# tibble will return NULL for a variable that does not exist for [[]]\ntbl <- tibble(b = 1)\nprint(tbl[[\"A\"]])## NULL\n# but a warning here\ntbl$A## Warning: Unknown or uninitialised column: `A`.## NULL"},{"path":"tables.html","id":"tribble","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.10 Tribble, table from text","text":"tibble package also provides easier way constructing tables via tribble() function. , use tilde specify column names, write content row--row.exercise 7.","code":"\ntribble(\n    ~x, ~y,\n    1,  \"a\",\n    2,  \"b\"\n)## # A tibble: 2 × 2\n##       x y    \n##   <dbl> <chr>\n## 1     1 a    \n## 2     2 b"},{"path":"tables.html","id":"data","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.11 Reading example tables","text":"One great things R packages come example data set illustrates function. can see list . case example data set, need import library part load writing data(tablename). example, use use mpg data fuel economy ggplot2 package, need import library first, call data(mpg).","code":"\nlibrary(ggplot2)\ndata(mpg) # this creates a \"promise\" of the data\nprint(mpg) # any action on the promise leads to data appearing in the environment## # A tibble: 234 × 11\n##    manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n##    <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n##  1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n##  2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n##  3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n##  4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n##  5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n##  6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n##  7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n##  8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n##  9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n## 10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n## # ℹ 224 more rows"},{"path":"tables.html","id":"readr","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.12 Reading csv files","text":"far covered creating table hand via data.frame(), tibble(), tribble() functions loading example table package via data() function. commonly, need read table external file. files can come many formats generated different experimental software. , see handle recommendation always store data csv (Comma-separated values) files. simple plain text files, means can open text editor, line representing single row (typically, top row contains column names) individual columns separated symbol symbols. Typical separators comma (hence, name), semicolon (frequently used Germany, comma serving decimal point), tabulator, even space symbol. example fileThere several ways reading CSV files R. default way read.csv() function different versions optimized different combinations decimal point separator symbols, e.g. read.csv2() assumes comma decimal point semicolon separator. However, better way use readr library re-implements functions. Names functions slightly different underscore replacing dot, readr::read_csv() replacement read.csv(). faster (although noticeable large data sets), convert text factor variables (talk factors later default conversion read.csv() can confusing), etc.However, important difference read.csv() read_csv() latter can constrain content CSV file. read.csv() assumptions columns file value types . simply reads , silently guessing type.can use read_csv() way work way inform (warn) table structure deduced.annoying warning, gets even annoying need read many CSV files, reason: wants annoy ! can turn via show_col_types = FALSE strongly recommend . Instead, specify column structure via col_types parameter. simplest way via spec() function, suggested printout.specification reader prepared . can take look , adjust , necessary, copy-paste read_csv call. default, suggested double values Block Trial know integers, can copy-paste suggested structure, replace col_double() col_integer() read table without warning.may feel lot extra work just suppress annoying , ultimately, harmless warning. code work without , right? Well, hopefully probably want know work just hope . Imagine accidentally overwrote experimental data file data different experiment (happens often one want). still results.csv file project folder read.csv() read_csv() read (know file) analysis code fail mysterious ways much later point (, remember, try access column/variable exist table, just get NULL rather error). eventually trace back wrong data file cost time nerves. However, specify column structure read_csv() show warning, file match description. warn wrong column names (TheBlock example ) wrong type (like TRUE/FALSE column expected find integers , can see details via problems() function).Personally, prefer read_csv() loudly fail error cases like nice red warning already helpful quickly detect problem data (data wrong, whole analysis meaningless). Thus, always use read_ rather read. functions always specify table structure. lazy, preferred, way , first read file without specifying structure, run spec() function , copy-paste output code adjusting necessary.need face_rank.csv file exercise 8. Download place project folder. Warning, use Chrome Chromium-based browsers like MS Edge, Opera, etc. might, odd reason, automatically rename face_rank.xls download (happen write , issue point time). Just rename back face_rank.csv, file converted Excel, extension gets changed24.exercise 8.Note another option online files, can use URL -place filename R take care downloading (, obviously, online trick work).","code":"Participant,Block,Trial,Contrast,Correct\nA1,1,1,0.5,TRUE\nA1,1,2,1.0,TRUE\nA1,1,2,0.05,FALSE\n...\nresults <- read.csv(\"data/example.csv\")\nresults##   Participant Block Trial Contrast Correct\n## 1          A1     1     1     0.50    TRUE\n## 2          A1     1     2     1.00    TRUE\n## 3          A1     1     2     0.05   FALSE\nresults <- readr::read_csv(\"data/example.csv\")## Rows: 3 Columns: 5\n## ── Column specification ─────────────────────────────────────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): Participant\n## dbl (3): Block, Trial, Contrast\n## lgl (1): Correct\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nresults## # A tibble: 3 × 5\n##   Participant Block Trial Contrast Correct\n##   <chr>       <dbl> <dbl>    <dbl> <lgl>  \n## 1 A1              1     1     0.5  TRUE   \n## 2 A1              1     2     1    TRUE   \n## 3 A1              1     2     0.05 FALSE\nspec(results)## cols(\n##   Participant = col_character(),\n##   Block = col_double(),\n##   Trial = col_double(),\n##   Contrast = col_double(),\n##   Correct = col_logical()\n## )\nlibrary(readr)\nresults <- read_csv(\"data/example.csv\", \n                    col_types = cols(Participant = col_character(),\n                                     Block = col_integer(), # read_csv suggested col_double() but we know better\n                                     Trial = col_integer(), # read_csv suggested col_double() but we know better\n                                     Contrast = col_double(),\n                                     Correct = col_logical()))\nresults## # A tibble: 3 × 5\n##   Participant Block Trial Contrast Correct\n##   <chr>       <int> <int>    <dbl> <lgl>  \n## 1 A1              1     1     0.5  TRUE   \n## 2 A1              1     2     1    TRUE   \n## 3 A1              1     2     0.05 FALSE\nresults <- read_csv(\"data/example.csv\", \n                    col_types = cols(Participant = col_character(),\n                                     TheBlock = col_integer(), # read_csv suggested col_double() but we know better\n                                     Trial = col_integer(), # read_csv suggested col_double() but we know better\n                                     Contrast = col_double(),\n                                     Correct = col_integer()))## Warning: The following named parsers don't match the column names: TheBlock## Warning: One or more parsing issues, call `problems()` on your data frame for details, e.g.:\n##   dat <- vroom(...)\n##   problems(dat)\nproblems()\nread_csv(\"https://alexander-pastukhov.github.io/data-analysis-using-r-for-psychology/data/face_rank.csv\")"},{"path":"tables.html","id":"readxl","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.13 Reading Excel files","text":"several libraries allow read Excel files directly. personal preference readxl package, part Tidyverse. Warning, installed part Tidyverse (.e., typed install.packages(tidyverse)) still need import explicitly via library(readxl). Excel file many sheets, default read_excel() function reads first sheet can specify via sheet parameter using index read_excel(\"my_excel_file.xls\", sheet=2) name read_excel(\"my_excel_file.xls\", sheet=\"addendum\").exercise 9, need face_rank.xlsx file . Download use URL directly. Also, think library need perform task. Add library “setup” chunk beginning, exercise chunk !exercise 9.can read options package’s website generally discourage using Excel work , definitely, data analysis. , see, Excel smart can figure true meaning type columns . fact might disagree problem. Excel knows best . easiest way screw CSV file (least Germany) open Excel immediately save . file name remain Excel “adjust” content feels better (don’t need consulted ). think exaggerating, read article Verge Excel messed thousands human genome data tables turning values dates not25? now entire community renaming genes easier waste literally thousands man-hours fix Excel. short, friends don’t let friends use Excel.","code":""},{"path":"tables.html","id":"reading-files-from-other-programs","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.14 Reading files from other programs","text":"World diverse place, likely encounter wide range data files generated Matlab, SPSS, SAS, etc. two ways import data. First, recommend, use original program (Matlab, SPSS, SAS, etc.) export data CSV file. Every program can read write CSV, good common ground. Moreover, simple format embedded formulas (Excel), service structures, etc. Finally, store data CSV, need special program work . short, unless good reason, store data CSV files.However, sometimes file program (Matlab, SPSS, SAS, etc.) export data CSV. second way use various R libaries, starting foreign, can handle typical cases, e.g., SPSS, SAS, State, Minitab. problem programs differ internal file formats exactly included. example, importing SPSS sav-file via read.spss get list various components rather table (data.frame). can force function convert everything single table via .data.frame=TRUE option may lose information. Bottom line, need extra careful importing formats safest way ensure complete full export data CSV original program.next exercise, need band_members.sav file. need read.spss() function library foreign (, need import , “setup” chunk).exercise 10.","code":""},{"path":"tables.html","id":"rds","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.15 Writing and reading a single object","text":"another option saving reading data R via saveRDS() readRDS functions26. saveRDS() saves arbitrary object (vector, list, table, model fit, etc.), whereas readRDS reads back. useful, table large, using CSV inefficient, want save something table, , something like model fit later use. R-specific format, able use file program, probably use frequently good option keep mind.exercise 11.","code":""},{"path":"tables.html","id":"wrap-up-1","chapter":"3 Tables and Tibbles (and Tribbles)","heading":"3.16 Wrap up","text":"started vectors now extended tables. final stop, getting exciting visualizations, functions, important pillar R.","code":""},{"path":"functions.html","id":"functions","chapter":"4 Functions! Functions everywhere!","heading":"4 Functions! Functions everywhere!","text":"chapter learn functions R, second important concept R everywhere (just like vectors lists). also learn pipe computation series functions without creating mess temporary variables nested calls. Don’t forget download notebook.","code":""},{"path":"functions.html","id":"functions-1","chapter":"4 Functions! Functions everywhere!","heading":"4.1 Functions","text":"previous chapters, learned can store information variables –— “boxes slots” —– vectors tables (bundles vectors equal length). use stored values computation need functions. programming, function isolated code receives (optional) input, performs action , , optionally, returns value27. concept functions comes mathematics, might easier understand using R implementation mathematical functions. example, may remember sine function trigonometry. typically abbreviated sin, takes numeric value angle (radians) input returns corresponding value -1 1 (output): \\(sin(0) = 0\\), \\(sin(\\pi/2) = 1\\), etc.R, write function using following template28A sin function single parameter angle look something like thisOnce function, can use calling . simply write sin(0)29 get answer!hopefully remember, simple values (atomic) vectors, instead using scalar 0 (merely vector length one) can write apply function (compute sinus ) every element vector.can think functions parameters local function variables values set function called. function can number parameters, including zero30, one, many parameters. example, arctangent atan2 function takes 2D coordinates (y x, order!) returns corresponding angle radians.definition function look something like thisDo exercise 1.","code":"name_of_the_function <- function(parameter1, parameter2, parameter3, ...){\n  ...some code that computes the value...\n  return(value)\n}sin <- function(angle){\n  ...some math that actually computes sinus of angle using value of angle parameter ...\n  return(sin_of_angle)\n}\nsin(0)## [1] 0\nsin(seq(0, 3.141593, length.out = 5))## [1]  0.000000e+00  7.071068e-01  1.000000e+00  7.071066e-01 -3.464102e-07\natan2(c(0, 1), c(1, 1))## [1] 0.0000000 0.7853982atan2 <- function(y, x){\n  ...magic that uses values of y and x parameters...\n  ...to compute the angle_in_rad value...\n  return(angle_in_rad);\n}"},{"path":"functions.html","id":"writing-your-own-function","chapter":"4 Functions! Functions everywhere!","heading":"4.2 Writing your own function","text":"Let us start practicing computing things R writing functions time. begin implementing simple function doubles given number. write function steps. First, think name31 function (meaningful names path readable re-usable code!) many parameters . Write definition function without code inside wiggly brackets (, actual computation return statement end ).exercise 2.1Next, think code double--value based parameter. code eventually go inside wiggly brackets. Write code (just code, without bits exercise 2.1) exercise 2.2 test creating variable name parameter inside function. E.g., parameter name the_number, test asDo exercise 2.2By now formal function definition (exercise 2.1) actual code go inside (exercise 2.2). Now, just need combine putting code inside function returning value. can two ways:can store results computation separate local variable return variable,return results computation directlyDo ways exercises 2.3 2.4. Call function using different inputs test works.exercise 2.3 2.4More practice always good, write function converts angle degrees radians. formula (hint: Constants) \n\\[rad = \\frac{deg \\cdot \\pi}{180}\\]\nDecide whether want intermediate local variable inside function return results computation directly. personal preference writing function follow inside-sequence writing functions exercise 2. Decide function definition parameter names, create variables names parameters write debug code. sure code working, put function, restart R session clear workspace get rid temporary variables, run function check everything works should32.exercise 3","code":"the_number <- 1\n...my code to double the value usign the_number variable...# 1) first store in a local variable, then return it\n  result <- ...some computation you perform...\n  return(result)\n\n# 2) returning results of computation directly\n  return(...some computation you perform...)"},{"path":"functions.html","id":"scopes-global-versus-local-variables","chapter":"4 Functions! Functions everywhere!","heading":"4.3 Scopes: Global versus Local variables","text":"suggested use variable store results double--computation returning . call local? function scope (environment variables functions) (mostly) independent scope global script. Unfortunately, environment scopes R different complicated programming languages, Python, C/C++, Java, always pay attention careful future.global scope/environment environment code outside functions. global variables functions, ones define code outside functions (typing console, running scripts, running chunks code notebooks, etc.), live . can see global scope time looking Environment tab (note Global Environment tag).\ncase, one table (mpg, tables go Data), three vectors (angle, some_variable, x, vectors go Values), example function exercise #2 created (double_it, functions go Functions, makes sense). However, access parameters functions variables define inside function. run function, scope/environment includes parameters function (e.g., the_number double_it function value assigned call), local variables create inside function (e.g., result <- ...computation perform... creates local variable), copy(!) global variables33. code , take look comments specify accessibility variables global script functions (ignore glue moment, glues variable’s value text, use make printouts easier trace)34Do exercise 4 build understanding scopes.","code":"\n# this is a GLOBAL variable\nglobal_variable <- 1\n\ni_am_test_function <- function(parameter){\n  # parameter live is a local function scope\n  # its values are set when you call a function\n  print(glue(\"  parameter inside the function: {parameter}\"))\n  \n  # local variable created inside the function\n  # it is invisible from outside\n  local_variable <- 2\n  print(glue(\"  local_variable inside the function: {local_variable}\"))\n  \n  # here, a_global_variable is a LOCAL COPY of the the global variable\n  # of the same name. You can use this COPY\n  print(glue(\"  COPY of global_variable inside the function: {global_variable}\"))\n  \n  # you can modify the LOCAL COPY but that won't affect the original!\n  global_variable <- 3\n  print(glue(\"  CHANGED COPY of global_variable inside the function: {global_variable}\"))\n}\n\nprint(glue(\"global_variable before the function call: {global_variable}\"))## global_variable before the function call: 1\ni_am_test_function(5)##   parameter inside the function: 5\n##   local_variable inside the function: 2\n##   COPY of global_variable inside the function: 1\n##   CHANGED COPY of global_variable inside the function: 3\n# the variable outside is unchanged because we modify its COPY, not the variable itself\nprint(glue(\"UNCHANGED global_variable after the function call: {global_variable}\"))## UNCHANGED global_variable after the function call: 1"},{"path":"functions.html","id":"function-with-two-parameters","chapter":"4 Functions! Functions everywhere!","heading":"4.4 Function with two parameters","text":"Let us write function takes two parameters –— x y –— computes radius (distance (0,0) (x, y))35. formula \n\\[R = \\sqrt{x^2 + y^2}\\]\nsimilar exercises 2 3, number parameters difference. , suggest first writing debugging code separate chunk putting inside formal function definition (testing !)exercise 5.","code":""},{"path":"functions.html","id":"table-as-a-parameter","chapter":"4 Functions! Functions everywhere!","heading":"4.5 Table as a parameter","text":"far, passed vectors (.k.. values) functions can pass object including tables36. Let us use mpg table ggplot2 package. Write function takes table parameter. mean function assume table name exists global environment. use mpg parameter name (makes confusing hard understand table global local actually mean), call something else37. function compute return average miles-per-gallon efficiency city cty highway hwy test cycles car (, need compute 234 values). two ways. First, compute return vector based table passed parameter. Second, computation add result table function received parameter (call new column avg_mpg) return entire table (remember, modifying table enough, working copy).exercise 6.Let us write another function computes mean efficiency particular cycle, either city highway, cars (single value output). , function take two parameters: 1) table 2) string (text variable) name column. , use column name access via simplifying [[]] subsetting. summarize, function takes 1) table 2) string column name returns single number (mean specified column). E.g.exercise 7.","code":"\naverage_efficiency(mpg, \"cty\") # should return 16.85897"},{"path":"functions.html","id":"named-versus-positional-arguments","chapter":"4 Functions! Functions everywhere!","heading":"4.6 Named versus positional arguments","text":"Remember talked assignment statement, noted always use <- operator assignments warned encounter alternative = operator use functions. also may notice using , example, seq(0, 2*pi, length.= 100). use name third parameter two? use length.= ? R can pass arguments position (first two arguments example) name (length.=).pass arguments position, values put corresponding arguments order supplied . .e., first value goes first parameter, second second, etc. specify arguments name, explicitly state argument gets value arg = value. , exact order matter can put arguments order need (.e., order makes understanding code easier). can also mix two ways , R R, can really mix interleaving positional named arguments way like. case, never use named arguments positional ones. time use named argument, position taken changes position index remaining positional arguments. almost certain way put value wrong parameter , time, create error really hard find. Thus, despite flexibility R gives confuse others, use -positional-followed---named-arguments order, e.g., seq(0, 2*pi, length.= 100).use ? depends solely way clearer possible. single parameter widely used mathematical functions, like mean sin little point using names (particularly, argument named x). time, always use named parameters atan2 simply never 100% sure order (.e., x followed y occurs far often). time, formulas statistical models R specific look typically first argument, probably redundant specify formula = y ~ x.Returning seq() function, length.actually fourth argument (alternative way define many values get sequence) third. parameters define, essentially, thing, impossible specify length.using positional arguments . Finally, function couple parameters, probably good idea use named arguments. short, use better judgement whatever makes understanding code easier.exercise 8.","code":""},{"path":"functions.html","id":"default-values","chapter":"4 Functions! Functions everywhere!","heading":"4.7 Default values","text":"write function, can combine simplicity use flexibility via default values. .e., parameters can sensible commonly used values , desired, user can specify values modify functions behavior. example, function mean three parameters. always need specify first one (x, vector values computing mean ). can also specify trimming via trim whether ignore NA via na.rm38. default, trim (trim = 0) ignore NA (na.rm = FALSE). defaults sensible produce typically expected behavior. time existence means can fine-tune mean computation way require.writing function, specify default values define argument. E.g., , second parameter r, radius, default value 1, can specify direction vector compute (x, y) coordinates (default) unit vector.exercise 9.","code":"\npolar_to_cartesian <- function(angle, r=1) {\n  # ...\n}"},{"path":"functions.html","id":"nested-calls","chapter":"4 Functions! Functions everywhere!","heading":"4.8 Nested calls","text":"need call several functions single chain compute result? Think function exercise #3 converts degree radians. likely usage scenario convert degrees radians use compute sine (trigonometric function). different ways can . example, can store angle radians temporary variable (e.g., angle_in_radians) pass sine function next call.Alternatively, can use value returned deg2rad() directly parameter function sin()case, computation proceeds inside-order: innermost function gets computed first, function uses return value next, etc. Kind like assembling Russian doll: start innermost, put inside slightly bigger one, now take one put inside next, etc. Nesting means need pollute memory temporary variables39 make code expressive nesting explicitly informs reader intermediate results value saved later use.exercise 10.","code":"\nangle_in_radians <- deg2rad(90) # function returns 1.570796, this value is stored in angle_in_radians\nsin(angle_in_radians) # returns 1\nsin(deg2rad(90)) # returns 1"},{"path":"functions.html","id":"pipe","chapter":"4 Functions! Functions everywhere!","heading":"4.9 Piping","text":"Although nesting better tons intermediate variables, lots nesting can mightily confusing. Tidyverse alternative way chain , Tidyverse-speak, pipe computation series function calls. magic operator |> (’s pipe). another pipe operator likely encounter: %>%. “original” pipe introduced magrittr library widely used, particularly Tidyverse, R version 4.1.0 provided built-|> solution. far concerned, two operators synonyms can used interchangeably (goes read someone else’s code). However, need import either tidyverse one libraries enable %>% |> official future, strongly recommend using |> code40 pipe transforms nested callDo exercise 11.functions used exercise one parameter single-output |> single-input piping straightforward. one functions takes one parameter? default, |> puts piped value first parameter. Thus 4 |> radius(3) equivalent radius(4, 3). Although default useful (entire Tidyverse build around idea piping value first parameter streamline experience), sometimes need value parameter (e.g., pre-processing data piping statistical test, latter typically takes formula first parameter data second). , can use special underscore variable _ |> dot variable . %>%. latter flexible, can used named positional arguments many times, whereas former restrictive can used named parameters. variables hidden 41 temporary variable holds value piping via either |> %>%42. Thus, standard pipe |>magritt pipe %>% can use anyway want even multiple times.can see %>% flexible , honest, realized never really bumped limitations standard pipe |> started compile examples. Thus, advice stick |> use %>% actually need (R liberal least restrictions good.)exercise 12.","code":"\nsin(deg2rad(90)) # returns 1\n\ndeg2rad(90) |> sin() # also returns 1\n\n90 |> deg2rad() |> sin() # also returns 1\nz <- 4\nw <- 3\nz |> radius(w) # equivalent to radius(z, w)## [1] 5\n# Note the underscore and that we use all named parameters!\nz |> radius(x = _, y = w) # equivalent to radius(z, w)## [1] 5\nz |> radius(x = w, y = _) # equivalent to radius(w, z)## [1] 5# Using it without name of the parameter generates a mistake\nz |> radius(_, w)\n\n# Using it twice is also not allowed\nz |> radius(x = _, y = _)## Error in radius(\"_\", w): pipe placeholder can only be used as a named argument (<input>:2:6)\nz %>% radius(w) # equivalent to radius(z, w)## [1] 5\n# Note the underscore and that we use all named parameters!\nz %>% radius(x = ., y = w) # equivalent to radius(z, w)## [1] 5\nz %>% radius(x = w, y = .) # equivalent to radius(w, z)## [1] 5\n# Now positional parameters\nz %>% radius(., w) # equivalent to radius(z, w)## [1] 5\nz %>% radius(w, .) # equivalent to radius(w, z)## [1] 5\n# Now same value twice\nz %>% radius(., .) # equivalent to radius(z, z)## [1] 5.656854"},{"path":"functions.html","id":"function-is-just-a-code-stored-in-a-variable","chapter":"4 Functions! Functions everywhere!","heading":"4.10 Function is just a code stored in a variable","text":"spot assignment <- operator function definitions wondered ? Yes, means literally storing function code variable. call function name, asking run code stored inside variable. can appreciate fact typing name function wrote without round brackets, e.g. double_the_value radius, , voila, see code. means function name really name (programming languages), rather name variable function’s code stored . can use variable function code inside just way treat variable. example, can copy (radius2 <- radius radius2 work exactly way). pass argument another function (effectively, copy code parameter), handy learn bootstrapping (needs data function) applying/mapping functions data.","code":""},{"path":"functions.html","id":"functions-functions-everywhere","chapter":"4 Functions! Functions everywhere!","heading":"4.11 Functions! Functions everywhere!","text":"Every computation perform R implemented function, even look like function call. example, + addition operator function. Typically, write 2 + 3, round brackets, comma-separated list parameters, looks different. just special implementation function call known function operator makes code readable humans. can actually call + function way call normal function using backticks around name.Even assignments operator <- , ’ve guessed , functionThis mean start using operators functions (although, helps make particular code clearer, , ?), merely stress one way program computation R —– function —– regardless may appear code. Later , learn apply functions vectors tables (, Tidyverse version , use functions map inputs outputs), helps know can apply function, even one look like function.","code":"\n2 + 3## [1] 5\n# note the `backticks` around +\n`+`(2, 3)## [1] 5\n`<-`(some_variable, 1)\nsome_variable## [1] 1"},{"path":"functions.html","id":"using-or-not-using-explicit-return-statement","chapter":"4 Functions! Functions everywhere!","heading":"4.12 Using (or not using) explicit return statement","text":"code always used return function. However, explicit return(some_value) can omitted last line function, just write value (variable) :lack return function final line actually officially recommended style Tidyverse , personally, fence approach “explicit better implicit”. omission may reasonable comes end long pipe , general, recommend using explicit return().","code":"\nsome_fun <- function(){\n  x <- 1\n  return(x)\n}\n\nsome_other_fun <- function(){\n  x <- 2\n  x\n}\n\nyet_another_fun <- function(){\n  3\n}\n\nsome_fun()## [1] 1\nsome_other_fun()## [1] 2\nyet_another_fun()## [1] 3"},{"path":"ggplot2.html","id":"ggplot2","chapter":"5 ggplot2: Grammar of Graphics","heading":"5 ggplot2: Grammar of Graphics","text":"previous chapters, learned tables main way representing data psychological research R. following ones, learn manipulate data tables: change , aggregate transform individual groups data, use statistical analysis. need understand store data table optimal (, least, recommended) way. First, introduce idea tidy data, concept gave Tidyverse name. Next, see tidy data helps visualize relationships variables. Don’t forget download notebook.","code":""},{"path":"ggplot2.html","id":"tidydata","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.1 Tidy data","text":"tidy data follows three rules:variables columns,observations rows,values cells.probably sound straightforward point wonder “Can table tidy?” matter fact lot typical results psychological experiments tidy. Imagine experiment participants rated face symmetry, attractiveness, trustworthiness. Typically (least experience), data stored follows:typical table optimized humans. single row contains responses single face, easy visually compare responses individual observers. Often, table even wider single row holds responses single observer (experience, lot online surveys produce data format)., wrong ? Don’t variables columns, observations rows, values cells? really. can already see comparing two tables . face identity variable, however, second table hidden column names. columns face M1, columns M2, etc. , interested analyzing symmetry judgments across faces participants, need select columns end .Symmetry figure way extract face identity columns’ names. Thus, face variable column second table., first table, Face column, tidy? short answer: really depends goals well! experiment, collected responses (numbers cells) different judgments. latter variable hidden column names. Thus, tidy table data beThis table () tidy makes easy group data every different combination variables (e.g., face judgment, participant judgment), perform statistical analysis, etc. However, may always best way represent data. example, like model Trustworthiness using Symmetry Attractiveness predictors, first table suitable. end, table structure must fit needs, way around. Still, probably want tidy table begin best suited things want data makes easy transform data match specific needs (e.g., going third table first one via pivoting).data get experiments tidy. spent quite time learning tidy first let us see already tidy data makes easy visualize relationships .","code":""},{"path":"ggplot2.html","id":"ggplot2-1","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.2 ggplot2","text":"ggplot2 package main tool data visualization R. ggplot2 tends make really good looking production-ready plots (given, default-looking Matlab plot , used used Matlab, pretty ugly). Hadley Wickham influenced works Edward Tufte developing ggplot2. Although aesthetic aspect goes beyond seminar, need visualize data future, strongly recommend reading Tufte’s books. fact, informative aesthetically pleasing experience recommend reading case.importantly, ggplot2 uses grammar-based approach describing plot makes conceptually different software Matlab, Matplotlib Python, etc. need get used , probably never want go back.plot ggplot2 described three parts:Aesthetics: Relationship data visual properties define working space plot (variables map individual axes, color, size, fill, etc.).Geometrical primitives visualize data (points, lines, error bars, etc.) added plot.properties plot (scaling axes, labels, annotations, etc.) added plot.always need first one. need specify two, even though plot without geometry looks empty. Let us start simple artificial example table . simulate response \n\\[Response = Normal(\\mu=1, \\sigma=0.2) - \\\\\nNormal(\\mu=2*ConditionIndex, \\sigma=0.4) + \\\\ Normal(\\mu=Intensity, \\sigma=0.4)\\]plot data 1) defining aesthetics (mapping Intensity x-axis, Responseon y-axis, Condition color) 2) adding lines plot (note plus43 + geom_line()).already wrote, technically, thing need define aesthetics, let us add anything plot dropping + geom_line().Told look empty yet can already see ggplot2 action. Notice axes labeled limits set. see legend (plotted without corresponding geometry) also ready behind scenes. initial call specified important part: individual variables map various properties even tell ggplot2 visuals use plot data. specified x-axis represent Intensity, ggplot2 figured range values, knows going plot whatever decide plot, axis’s label. Points, lines, bars, error bars span range. goes properties color. wanted color represent condition. , may know exactly plotting (points, lines?) even many different visuals adding plot (just lines? points + lines? points + lines + linear fit?) know whatever visual add, can color, color must represent condition data point. beauty ggplot2 analyses data figures many colors need ready apply consistently visuals add later. ensure points, bars, lines, etc. consistent coordinates scaling, color, size, fill mapping across entire plot. may sound trivial typically (e.g., Matlab, Matplotlib), job make sure properties match represent value across visual elements. pretty tedious job, particularly decide change mappings redo individual components hand. ggplot2, dissociation mapping visuals means can tinker one time. E.g., keep visuals change grouping see effect condition easier see via line type, size shape point? can keep mapping see whether adding another visual make plot easier understand. Note mappings also group data, use group-based visual information (e.g., linear regression line) know data belongs together perform computation per group.Let us see can keep relationship mapping add visuals. Let us add lines points.plot , kept relationship variables properties said “Oh, , please, throw points well”. ggplot2 knows add points appear proper location proper color. want !Now added linear regression line helps us better see relationship Intensity Response. , simply wished another visual added (method=\"lm\" means wanted average data via linear regression formula = y ~ x meaning regress y-axis x-axis covariates, se=FALSE means standard error stripe, linetype=\"dashed\" just makes easier distinguish linear fit solid data line)., can keep visuals see whether changing mapping make informative (need specify group=Intensity continuous data grouped automatically)., can check whether splitting several plots helps., note three plots live scale x- y-axis, making easy compare (fully appreciate magic ever struggled ensuring optimal consistent scaling hand Matlab). went many examples stress ggplot allows think aesthetics variable mapping independently actual visual representation (vice versa).Now lets us explore ggplot2 exercises. recommend using ggplot2 reference page cheatsheet exercises.","code":"\nggplot(data=simple_tidy_data, aes(x = Intensity, y = Response, color=Condition)) + \n  geom_line()\nggplot(data=simple_tidy_data, aes(x = Intensity, y = Response, color=Condition))\nggplot(data=simple_tidy_data, aes(x = Intensity, y = Response, color=Condition)) + \n  geom_line() +\n  geom_point() # this is new!\nggplot(data=simple_tidy_data, aes(x = Intensity, y = Response, color=Condition)) + \n  geom_line() +\n  geom_point() +\n  # a linear regression over all dots in the group\n  geom_smooth(method=\"lm\", formula = y ~ x, se=FALSE, linetype=\"dashed\") \nggplot(data=simple_tidy_data, aes(x = Condition, y = Response, color=Intensity, group=Intensity)) + \n  geom_line() +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE,  formula = y ~ x, linetype=\"dashed\")\nggplot(data=simple_tidy_data, aes(x = Intensity, y = Response, color=Condition)) + \n  geom_line() +\n  geom_point() +\n  geom_smooth(method=\"lm\", formula = y ~ x, se=FALSE, linetype=\"dashed\") +\n  facet_grid(. ~ Condition)  + # makes a separate subplot for each group\n  theme(legend.position = \"none\") # we don't need the legend as conditions are split between facets "},{"path":"ggplot2.html","id":"auto-efficiency-continuous-x-axis","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.3 Auto efficiency: continuous x-axis","text":"start visualizing car efficiency, measured miles-per-gallon, affected various factors production year, size engine, type transmission, etc. data table mpg, part ggplot2 package. Thus, need first import library load table via data() function. Take look table description familiarize variables.First, let us look relationship car efficiency city cycle (cty), engine displacement (displ), drive train type (drv) using color points. Reminder, call look asThink aesthetics, .e., variables mapped axes best depicted color.exercise 1.see clear dependence? Let us try making evident adding geom_smooth geometric primitive.exercise 2.engine size (displacement) drive train clear effect car efficiency. Let us visualize number cylinders (cyl) well. Including mapping size geometry.exercise 3.Currently, mixing together cars produced different times. Let us visually separate turning year subplot via facet_wrap function.exercise 4.dependence plotted look linear instead saturating certain low level efficiency. sort dependencies easier see logarithmic scale. See functions different scales use logarithmic scaling y-axis.exercise 5.Note now managed include five variables plots. can continue including transmission fuel type pushing , many variables can make plot confusing cryptic. Instead, let us make prettier using meaningful axes labels (xlab(), ylab() functions) adding plot title (labs).exercise 6.","code":"ggplot(data_table_name, aes(x = var1, y = var2, color = var3, shape = var4, ...)) + \n  geom_primitive1() + \n  geom_primitive2() +\n  ..."},{"path":"ggplot2.html","id":"auto-efficiency-discrete-x-axis","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.4 Auto efficiency: discrete x-axis","text":"previous section use continuous engine displacement variable x-axis (least assumption mapped variables). Frequently, need plot data discrete groups: experimental groups, conditions, treatments, etc. Let us practice mpg data set visualize relationship drive train (drv) highway cycle efficiency (hwy). Start using point visuals.exercise 7.One problem plot points plotted x-axis location. means two points share location, overlap appear just one dot. makes hard understand density: one point can mean one point, two, hundred. better way plot data using box violin plots44. Experiment using instead points.exercise 8., let’s ante split plots via number cylinders year manufacturing. Use facet_grid function generate grid plots.exercise 9.Let us improve presentation using better axes labels figure title.exercise 10.","code":""},{"path":"ggplot2.html","id":"mammals-sleep-single-variable","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.5 Mammals sleep: single variable","text":"Now lets us work plotting distribution single variable using mammals sleep dataset. , need map sleep_total variable x-axis plot histogram. Explore available options, particular bins determines bins number , therefore, size. Note “correct” number bins use. ggplot2 defaults 30 small sample probably better served fewer bins , vice versa, large data set can afford hundreds bins.exercise 11.Using histogram gives exact counts per bin. However, appearance may change quite dramatically use fewer bins. alternative way represent information via smoothed density estimates. use sliding window compute estimate point also include points around weight according kernel (e.g., Gaussian one default). makes plot look smoother mask sudden jumps density (counts) , effectively, average many bins. Whether approach better visualizing data depends sample message trying get across. always worth checking (just like worth checking different number bins histogram) see way best specific case.exercise 12.Let us return using histograms plot distribution per vore variable (carnivore, omnivore, herbivore, NA). can map fill color histogram, vore kind binned separately.exercise 13.plot may look confusing default ggplot2 colors values group differently stacks together produce total histogram counts45. One way disentangle individual histograms via facet_grid function. Use plot vore distribution separate rows.exercise 14.trick alternative way plot individual distributions plot setting position argument geom_histogram \"identity\" (\"stack\" default).exercise 15.Hmm, shouldn’t carnivores, going ? Opacity answer. bar “front” occludes bars “behind” . Go back exercise fix specifying alpha argument controls transparency. 1 (completely opaque) default can go 0 (fully transparent “invisible”), see intermediate value works best.","code":""},{"path":"ggplot2.html","id":"mapping-for-all-visuals-versus-just-one-visual","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.6 Mapping for all visuals versus just one visual","text":"previous exercise, assigned constant value alpha (transparency) argument. two places, inside either ggplot() geom_histogram() call. former case, set alpha level geometric primitives plot, whereas latter histogram. better see difference, reuse code plotting city cycle efficiency versus engine size (exercise #6) set alpha either visuals (ggplot() call) visuals (e.g. points) see difference.exercise 16.","code":""},{"path":"ggplot2.html","id":"mapping-on-variables-versus-constants","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.7 Mapping on variables versus constants","text":"previous exercise, assigned constant value alpha (transparency) argument. However, transparency just property just like x, color, size. Thus, two ways can use :inside aes(x=column), column column table supplied via data=outside aes stating x=value, value constant value variable table.Test setting size previous plot constant outside aesthetics variable inside .exercise 17.","code":""},{"path":"ggplot2.html","id":"themes","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.8 Themes","text":"Finally, fan way plots look, can quickly modify using theme. can define (lots options can specify theme) can use one ready-mades. Explore latter option, find one like best.exercise 18.","code":""},{"path":"ggplot2.html","id":"you-aint-seen-nothing-yet","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.9 You ain’t seen nothing yet","text":"explored just tip iceberg. many geometric primitive, annotations, scales, themes, etc. take entire separate seminar ggplot2 justice. However, basics get started can always consult reference, books (see ), need .","code":""},{"path":"ggplot2.html","id":"further-reading","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.10 Further reading","text":"plotting data part daily routine, recommend reading ggplot2 book. gives -depth view package goes many possibilities offers. may need find useful know exists (knows, might need one day). Another book worth reading Data Visualization: Practical Introduction. Kieran Healy.","code":""},{"path":"ggplot2.html","id":"extending-ggplot2","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.11 Extending ggplot2","text":"128 (05.10.2023) extensions find ggplot2 website. add ways plot data, themes, animated plots, etc. feel ggplot2 geometric primitives need, take look gallery , likely, find something fits bill.One package use particularly often patchwork. created “make ridiculously simple combine separate ggplots graphic”. bold promise authors make good . probably easiest way combine multiple plots can also consider cowplot gridExtra packages.","code":""},{"path":"ggplot2.html","id":"ggplot2-cannot-do-everything","chapter":"5 ggplot2: Grammar of Graphics","heading":"5.12 ggplot2 cannot do everything","text":"many different plotting routines packages R recommend use ggplot2 main tool. However, mean must tool, , CRAN brimming packages. particular, ggplot2 built plotting data single tidy table, meaning less optimal plotting data cases. E.g., can use combine information several tables one plot things become less automatic consistent. Similarly, can plot data stored non-tidy tables even individual vectors makes less intuitive convoluted. package can everything ggplot2 exception.","code":""},{"path":"dplyr.html","id":"dplyr","chapter":"6 Tidyverse: dplyr","heading":"6 Tidyverse: dplyr","text":"Now understand vectors, tables, functions pipes, know end goal (tidy table), can start data wrangling Tidyverse way . functions discussed part dplyr46 “grammar data manipulation” package. Grab exercise notebook!","code":""},{"path":"dplyr.html","id":"tidyverse-philosophy","chapter":"6 Tidyverse: dplyr","heading":"6.1 Tidyverse philosophy","text":"Data analysis different “normal” programming mostly involves series sequential operations table. might load table, transform variables, filter data, select smaller subset columns, aggregate summarizing across different groups variables plotting formally analyzing via statistical tests. Tidyverse built around serial nature data analysis piping table chain functions. Accordingly, Tidyverse functions take table (data.frame tibble) first parameter, makes piping simpler, return modified table output. table-→ table-consistency makes easy pipe operations one another. , helps think Tidyverse functions verbs: Actions perform table step.quick teaser sequential piping works. , examine verb/function separately also show operations can carried using base R. Note put verb/function separate line. don’t need , makes easier understand many different operations perform (number lines), complex (long individuals lines code ), makes easy read line--line. Note even though many lines, fairly easy follow entire code sequence.","code":"\n# miles-per-gallon to kilometers-per-liter conversion factor\nmpg2kpl <- 2.82481061\n\nmpg |>\n  # we filter the table by rows, \n  # only keeping rows for which year is 2008\n  filter(year == 2008) |>\n  \n  # we change cty and hwy columns by turning\n  # miles/gallon into liters/kilometer\n  mutate(cty_kpl = cty / mpg2kpl,\n         hwy_kpl = hwy / mpg2kpl) |>\n  \n  # we create a new column by computing an \n  # average efficiency as mean between city and highway cycles\n  mutate(avg_kpl = (cty_kpl + hwy_kpl) / 2) |>\n  \n  # we convert kilometers-per-liter to liters for 100 KM\n  mutate(avg_for_100 = 100 / avg_kpl) |>\n  \n  # we reduce the table to only two columns\n  # class (of car) and avg_mpg\n  select(class, avg_for_100)  |>\n\n  # we group by each class of car\n  # and compute average efficiency for each group (class of car)\n  group_by(class) |>\n  summarise(class_avg = mean(avg_for_100), .groups = \"keep\") |>\n  \n  # we round the value to just one digit after the decimal point\n  mutate(class_avg = round(class_avg, 1)) |>\n  \n  # we sort table rows to go from best to worst on efficiency\n  arrange(class_avg) |>\n  \n  # we rename the class_avg_lpk to have a more meaningul name\n  rename(\"Average liters per 100 KM\" = class_avg) |>\n\n  # we kable (Knit the tABLE) to make it look nicer in the document\n  knitr::kable()"},{"path":"dplyr.html","id":"select","chapter":"6 Tidyverse: dplyr","heading":"6.2 select() columns by name","text":"Select verb allows select/pick columns table using names. similar using columns names indexes tables learned seminar 3.First, let us make shorter version mpg table keeping first five rows. Note can also pick first N rows via head() function slide_head() function dplyr .can select model cty columns via preserving [] subsettingAnd done via select().idea Tidyverse functions adopt , can use quotes pass vector strings column names. calls produce effect, pick style prefer (mine, code ) stick it47.surely remember, can use negation select indexes within vector (c(4, 5, 6)[-2] gives [4, 6]). single brackets [] mechanism work column names (indexes). However, select covered, can select everything cty modelIn current version dplyr, can negation via ! (logical operator, meet later), moreover, now recommended way writing selection48. - ! synonyms difference subtle important, see .However, stick putting column names vector, direct selection , can use negation names strings, can negate vector names, etc. , mostly matter taste consistency important specific choice make.Unlike vector indexing forbids mixing positive negative indexing, select allow . However, use it49 results can fairly counter-intuitive , top , - ! work somewhat differently. Note difference ! -: former case !model part appears effect, whereas case - cty works.make things even, worse select(-model, cty) work way select(cty, !model) (sigh…), bottom line, mix positive negative indexing select! showing signal potential danger.exercise 1.Simple names negation sufficient projects. However, recommend taking look official manual just see select offers lot flexibility (selecting range columns, column type, partial name matching, etc), something might useful work.","code":"\nshort_mpg <- mpg[1:5, ]\n\n# same \"first five rows\" but via head() function\nshort_mpg <- head(mpg, 5)\n\n# same \"first five rows\" but via dplyr::slice_head() function\nshort_mpg <- slice_head(mpg, n = 5)\n\n\nknitr::kable(short_mpg)\nshort_mpg[, c(\"model\", \"cty\")] \nshort_mpg |>\n  select(model, cty)\nshort_mpg |>\n  select(c(\"model\", \"cty\"))\n  \nshort_mpg |>\n  select(\"model\", \"cty\")\n  \nshort_mpg |>\n  select(c(model, cty))\nshort_mpg |>\n  select(-cty, -model)\n# This will NOT produce the same result as above\n# Note that the model column is still in the table\nshort_mpg |>\n  select(!cty, !model)\n# will produce the same result as for \"-\"\nshort_mpg |>\n  select(!c(\"cty\", \"model\"))\n\nshort_mpg |>\n  select(!\"cty\", !\"model\")\n  \nshort_mpg |>\n  select(!c(cty, model))\nshort_mpg |>\n  select(cty, !model)\nshort_mpg |>\n  select(cty, -model)\nshort_mpg |>\n  select(-model, cty)"},{"path":"dplyr.html","id":"conditions","chapter":"6 Tidyverse: dplyr","heading":"6.3 Conditions","text":"can work next verb, need understand conditions. Conditions statements values either TRUE FALSE. simplest case, can check whether two values (one variable one hard-coded) equal via == operatorFor numeric values, can use usual comparison operators including equal !=, less <, greater >, less equal <= (note order symbols!), greater equal >= (, note order symbols).exercise 2.can negate statement via ! symbol !TRUE FALSE vice versa. However, note round brackets examples ! critical express order computation. Anything inside brackets evaluated first. brackets inside brackets, similar nested functions, innermost expression get evaluated first. example , x==5 evaluated first logical inversion happens . particular example, may need suggest using ensure clarity.exercise 3.can also combine several conditions using & | operators50. , note round brackets explicitly define evaluated first.exercise 4.examples used scalars remember everything vector, including values used (just vectors length one). Accordingly, logic works vectors arbitrary length comparisons working element-wise, get vector length TRUE FALSE values pairwise comparison.exercise 5.","code":"\nx <- 5\nprint(x == 5)## [1] TRUE\nprint(x == 3)## [1] FALSE\nx <- 5\nprint(!(x == 5))## [1] FALSE\nprint(!(x == 3))## [1] TRUE\nx <- 5\ny <- 2\n\n# x is not equal to 5 OR y is equal to 1\nprint((x != 5) | (y == 1))## [1] FALSE\n# x less than 10 AND y is greater than or equal to 1\nprint((x < 10) & (y >= 1))## [1] TRUE"},{"path":"dplyr.html","id":"logical-indexing","chapter":"6 Tidyverse: dplyr","heading":"6.4 Logical indexing","text":"second seminar, learned vector indexing access elements vector specifying index. alternative way, called logical indexing. , supply vector equal length logical values get elements original vector whenever logical value TRUEThis particularly useful, interested elements satisfy certain condition. example, want negative values can use condition x<5 produce vector logical values , turn, can used indexYou can conditions complexity combining via & | operators. example, want number -1 3 (careful space < -, otherwise interpreted assignment <-).exercise 6.Sometimes may want know actual index elements condition TRUE. Function () exactly .","code":"\nx <- 1:5\nx[c(TRUE, TRUE, FALSE, TRUE, FALSE)]## [1] 1 2 4\nx <- c(-2, 5, 3, -5, -1)\nx[x < 0]## [1] -2 -5 -1\nx <- c(-2, 5, 3, -5, -1)\nx[(x < -1) | (x > 3)]## [1] -2  5 -5\nx <- c(-2, 5, 3, -5, -1)\nwhich( (x< -1) | (x>3) )## [1] 1 2 4"},{"path":"dplyr.html","id":"filter-rows-by-values","chapter":"6 Tidyverse: dplyr","heading":"6.5 filter() rows by values","text":"Now understand conditions logical indexing, using filter() straightforward: simply put condition describes rows want retain inside filter() call. example, can look efficiency two-seater cars.can use information row, can look midsize cars four-wheel drive.exercise 7.Note can emulate filter() straightforward way using single-brackets base R, main difference need prefix every column table name, mpg[[\"class\"]] instead just class51.use filter() ? isolation, single line computation, options equally compact clear (apart extra table[[\"...\"]] base R). pipe-oriented nature filter() makes suitable chains computations, main advantage Tidyverse.","code":"\nmpg |>\n  filter(class == \"2seater\")\nmpg |>\n  filter(class == \"midsize\" & drv == \"4\")\nmpg[mpg[[\"class\"]] == \"midsize\" & mpg[[\"drv\"]] == \"4\", ]"},{"path":"dplyr.html","id":"arrange-rows-in-a-particular-order","chapter":"6 Tidyverse: dplyr","heading":"6.6 arrange() rows in a particular order","text":"Sometimes might need sort table rows go particular order52. Tidyverse, arrange rows based values specific variables. verb straightforward, simply list variables, must used sorting, order sorting must carried . .e., first table sorted based values first variable. , equal values variable, rows sorted based second variable, etc. default, rows arranged ascending order can reverse putting variable inside desc() function. short_mpg table arranged city cycle highway efficiency (ascending order) engine displacement (descending order, note order last two rows).exercise 8.can arrange table using base R via order() function gives index ordered elements can used inside preserving subsetting via single brackets [] notation. can control ascending/descending specific variable using rev() function applied ordering, rev(order(...)).exercise 9.","code":"\nshort_mpg |>\n  arrange(cty, desc(displ)) \nshort_mpg[order(short_mpg[[\"cty\"]], rev(short_mpg[[\"displ\"]])), ]"},{"path":"dplyr.html","id":"mutate","chapter":"6 Tidyverse: dplyr","heading":"6.7 mutate() columns","text":"Tidyverse, mutate function allows add new columns/variables table change existing ones. essence, equivalent simple column assignment statement base R.Note two critical differences. First, mutate() takes table input returns table output. start table, pipe mutate, assign results back original variable. verbs/lines, output last computation assigned variable left-hand side assignment53. Look listing indexes line executed.Second, performing computation inside call mutate() function, avg_mpg = (short_mpg$cty + short_mpg$hwy) / 2 parameter pass (yes, look like one). use = rather normal assignment arrow <-. Unfortunately, can use <- inside mutate computation work intended , internal-processing reasons, entire statement, rather just left-hand side, used column name. Thus, use <- outside = inside Tydiverse verbs.shown example , can perform several computations within single mutate call executed one another, just using base R.exercise 10.Finally, mutate two cousin-verbs called transmute add_column. former — transmute — works way discards original columns modified. probably won’t use verb often want able recognize , name function similar mutate two easy confuse.latter — add_column — similar mutate need add new column rather modify new one. advantage produce error, try overwrite existing column. disadvantage appear respect data grouping (see ), can confusing. short, stick mutate unless need either two functions specifically.","code":"\n# base R\nshort_mpg[[\"avg_mpg\"]] <- (short_mpg[[\"cty\"]] + short_mpg[[\"hwy\"]]) / 2\n\n# Tidyverse equivalent\nshort_mpg <- \n  short_mpg |>\n  mutate(avg_mpg = (cty + hwy) / 2)\nsome_table <-    # 3. We assign the result to the original table, only once all the code below has been executed.\n  some_table |>  # 1. We start here, with the original table and pipe to the next computation\n  mutate(...)    # 2. We add/change columns inside of the table. The output is a table which we use for assignment all the way at the top.\nshort_mpg |>\n  select(cty, hwy) |>\n  mutate(avg_mpg =  (cty + hwy) / 2,      # column name will be avp_mpg\n         avg_mpg <- (cty + hwy) / 2) |>  # column name will be `avg_mpg <- (short_mpg$cty + short_mpg$hwy) / 2`\n  knitr::kable()\nshort_mpg |>\n  transmute(avg_mpg = (cty + hwy) / 2) |>\n  knitr::kable(align = \"c\")"},{"path":"dplyr.html","id":"summarize","chapter":"6 Tidyverse: dplyr","heading":"6.8 summarize() table","text":"verb used aggregate across rows, reducing single value. examples aggregating functions probably already familiar mean, median, standard deviation, min/max. However, can “aggregate” taking first last value even putting constant. Important assign single value column using summarize.use summarize ungrouped table (tables ’ve working far), keeps computed columns, makes wonder “’s point?”","code":"\nmpg |>\n  summarise(avg_cty = mean(cty),\n            avg_hwy = mean(hwy))## # A tibble: 1 × 2\n##   avg_cty avg_hwy\n##     <dbl>   <dbl>\n## 1    16.9    23.4"},{"path":"dplyr.html","id":"group_by","chapter":"6 Tidyverse: dplyr","heading":"6.9 Work on individual groups of rows","text":"real power summarize mutate becomes evident applied data grouped certain criteria. group_by() verb groups rows table based values variables specified. Behind scenes, turns single table set tables, Tidyverse verbs applied table separately. ability parse table different groups rows (rows belong particular participant, participant condition, rows per block, per trial), change grouping fly, return back original full table, etc. makes analysis breeze. can compute average efficiency across cars (code ) car class separately.Note compute single value per table seven tables, get seven rows resultant table. group_by makes easy group data way want. interested manufacturers instead car classes? Easy!efficiency per class year? Still easy!.groups parameter summarize function determines whether grouping use dropped (.groups = \"drop\", table become single ungrouped table) kept (.groups = \"keep\"). get warning, specify .groups parameter, good idea explicitly. general, use .groups = \"drop\" better later regroup table work groupped table without realizing (leads weird looking output tricky chase problem). can also explicitly drop grouping via ungroup() verb.Finally, cousin verb rowwise() groups row. .e., every row table becomes group, useful need apply computation per row usual mutate() approach work (however, something advanced topic, recognizing using ).exercise 11.can replicate functionality group_by + summarize base R via aggregate() group_by + mutate via functions. However, somewhat less straightforward use rely functional programming (haven’t learned yet) require grouping summary function within single call. Hence, skip .","code":"\nmpg |>\n  # there are seven different classes of cars, so group_by(cars)  will\n  # create seven hidden independent tables and all verbs below will be \n  # applied to each table separately\n  group_by(class)  |>\n  \n  # same mean computation but per table and we've got seven of them\n  summarise(avg_cty = mean(cty),\n            avg_hwy = mean(hwy),\n            .groups = \"drop\") |>\n  knitr::kable()\nmpg |>\n  group_by(manufacturer)  |>\n  # same mean computation but per table and we've got seven of them\n  summarise(avg_cty = mean(cty),\n            avg_hwy = mean(hwy),\n            .groups = \"drop\") |>\n  knitr::kable()\nmpg |>\n  group_by(class, year)  |>\n  # same mean computation but per table and we've got seven of them\n  summarise(avg_cty = mean(cty),\n            avg_hwy = mean(hwy),\n            .groups = \"drop\") |>\n  knitr::kable()"},{"path":"dplyr.html","id":"putting-it-all-together","chapter":"6 Tidyverse: dplyr","heading":"6.10 Putting it all together","text":"Now enough tools disposal start programming continuous analysis pipeline!exercise 12.","code":""},{"path":"dplyr.html","id":"should-i-use-tidyverse","chapter":"6 Tidyverse: dplyr","heading":"6.11 Should I use Tidyverse?","text":"saw , whatever Tidyverse can , base R can well. use non-standard family packages? using function isolation, probably much sense . Base R can equally well individual function also compact simple. However, need chain computation, almost always case, Tidyverse’s ability pipe entire sequence functions simple consistent , therefore, easy understand way game-changer. long run, pick style. Either go “” Tidyverse (approach), stick base R, find alternative package family (e.g., data.table). However, far book concerned, almost exclusively Tydiverse now .","code":""},{"path":"working-with-factors.html","id":"working-with-factors","chapter":"7 Working with Factors","heading":"7 Working with Factors","text":"Let us start “warm ” exercise require combining various things already learned. Download persistence.csv file (can use URL directly, cause R download every time call function, let’s overstress server) put data subfolder seminar project folder. data Master thesis project student Kristina Burkel, published article Attention, Perception, & Psychophysics. work investigated change object’s shape affected perceptual stability brief interruptions (50 ms blank intervals). research question whether results match one two history effects, work longer time scales. match indicate history effects likely produced shared neuronal representations 3D rotation. Grab exercise notebook start.","code":""},{"path":"working-with-factors.html","id":"how-to-write-code","chapter":"7 Working with Factors","heading":"7.1 How to write code","text":"now , need implement progressively longer analysis sequences. Unfortunately, longer complex analysis , easier make mistake ruin everything stage. make mistakes, simply one perfect everyone makes . make time. Professional programmers make . skill programming writing perfect code first attempt, writing code iterative manner, mistake make (, , make !) spotted fixed immediately, continue adding code. like walking blind uncertain terrain: One step time, running, jumping, idea awaits .mean practical terms? typical analysis (exercise ), need many things: read data, select columns, filter , compute new variables, group data summarize , plot , etc. might tempted program whole thing one go terrible idea. step #2 think , later stages work wrong data tracing back step #2 may trivial (almost never ). Instead, implement one step time check results look . E.g., exercise , read table. Check, look good, even data? sure reading bit works, proceed columns selection. Run two-step code check works table looks way . (relevant columns)? Good, proceed next step.Never skip checks! Always look results additional step, just hope . might, might . latter case, lucky, see long debugging session. may even notice computation subtly broken use results draw erroneous conclusions. may feel overly slow keep checking continuously faster way program long term. Moreover, step time, actually know, hope, works.’ve spent three paragraphs (now adding even forth one!), , opinion, approach main difference novice experienced programmers (, one go even say good bad programmers). see mistake writing everything one go repeated irrespective tool people use (can make really fine mess using SPSS!). makes mess every time deviate approach! , pace let’s start programming earnest!","code":""},{"path":"working-with-factors.html","id":"implementing-a-typical-analysis","chapter":"7 Working with Factors","heading":"7.2 Implementing a typical analysis","text":"first exercise, want implement actual analysis performed paper. Good news now know enough program ! Note steps 1-4 (everything plotting) implemented single pipeline. Start first action keep adding verbs end single chained computation. actually important might think: Putting read--preprocessing single pipe means lurking temporary / intermediate variables may changed mean time. single pipe ensures guaranteed get table run . Splitting smaller chunks means can (means ) run --order, run one computation extra time, forget modified data need reload , etc. caught way often like admit, single definitive computation always good idea.Load data table. Name variable . Typically, use names like data, reports, results, etc. Don’t forget specify columns’ type.Exclude filename column (duplicates Participant Session columns).Compute new variable SameResponse TRUE Response1 Response2 match (experiment, means object rotating direction intervention).every combination Participant, Prime Probe compute proportion responses. can two ways. Recall .integer(TRUE) 1 .integer(FALSE) 0. Thus, can either compute proportion mean compute sum responses divide total number trials. Use function n() latter, returns total number rows table group. Try ways.Plot results Probe variable x-axis, proportion responses y-axis, use Prime facet plots. Use box plots (violin plots) visualize data. Try adding color, labels, etc. make plots look nice.exercise 1.examine plot, can see sort non-monotonic dependence dip \"stripes-2\" \"stripes-4\" objects. reality, dependence monotonic, merely order values x-axis wrong. correct order, based area object covered dots, \"heavy poles sphere\", \"stripes-8\", \"stripes-4\", \"stripes-2\". Prime Probe ordinal variables called factors R. Thus, fix order make object names bit better looking, must figure work factors R.","code":""},{"path":"working-with-factors.html","id":"factors","chapter":"7 Working with Factors","heading":"7.3 Factors","text":"Factors categorical variables, thus variables finite fixed known set possible values. can either nominal (ordered) ordinal (specific order ). example former drive train (drv) variable mpg table. finite set possible values (\"f\" front-wheel drive, \"r\" rear wheel drive, \"4\" four-wheel drive) ordering makes sense. example ordinal variable Likert scale finite set possible responses (example, \"disagree\", \"neither agree, disagree\", \"agree\") specific fixed order (participant’s support statement progressively stronger \"disagree\" < \"neither agree, disagree\" < \"agree\").can convert variable factor using factor() .factor() functions. latter limited version former, makes little sense ever use . , use factor().convert variable (vector) factor, R:figures unique values vectorsorts ascending orderassigns value integer index, .k.. “level”uses actual value “label”.example sequence: four levels sorted alphabetically (note R prints vector also levels).can extracts levels factor variable using function nameYou can specify order levels either factor() call later using forcats library (later). example, want levels reverse order specify via levels parameter. Note opposite order levels.can also specify labels individual labels instead using values . Note labels must match levels number order.can see indexes assigned level converting letter_as_factor numeric vector. case, R throws away labels returns indexes.However, careful level labels numbers. example , might think .numeric(tens) give [20, 40, 30]54 labels, levels go \\(1\\) \\(3\\)! need convert labels numbers, two steps .numeric(.character(tens)): .character() turns factors strings (using labels) .numeric() converts labels numbers (conversion can work).next exercise, copy-paste code exercise #1 alter labels \"sphere\" (\"heavy poles sphere\"), \"quad band\" (\"stripes-8\"), \"dual band\" (\"stripes-4\"), \"single band\" (\"stripes-2\") levels order. plot look something like .exercise 2.","code":"\nletters <- c(\"C\", \"A\", \"D\", \"B\", \"A\", \"B\")\nletters_as_factor <- factor(letters)\nletters_as_factor## [1] C A D B A B\n## Levels: A B C D\nlevels(letters_as_factor)## [1] \"A\" \"B\" \"C\" \"D\"\nletters <- c(\"C\", \"A\", \"D\", \"B\", \"A\", \"B\")\nletters_as_factor <- factor(letters, levels = c(\"D\", \"C\", \"B\", \"A\"))\nletters_as_factor## [1] C A D B A B\n## Levels: D C B A\nresponses <- c(1, 3, 2, 2, 1, 3)\nresponses_as_factor <- factor(responses, levels = c(1, 2, 3), labels = c(\"negative\", \"neutral\", \"positive\"))\nresponses_as_factor## [1] negative positive neutral  neutral  negative positive\n## Levels: negative neutral positive\nas.numeric(letters_as_factor)## [1] 2 4 1 3 4 3\ntens <- factor(c(20, 40, 30))\nprint(tens)## [1] 20 40 30\n## Levels: 20 30 40\nprint(as.numeric(tens))## [1] 1 3 2\nprint(as.numeric(as.character(tens)))## [1] 20 40 30"},{"path":"working-with-factors.html","id":"forcats","chapter":"7 Working with Factors","heading":"7.4 Forcats","text":"Tidyverse package forcats55 makes working factors easier. example, allows reorder levels either hand automatically based order appearance, frequency, value variable, etc. also gives flexible tools changes labels either hand, lumping levels together, anonymising , dropping unused levels, etc. work, mostly use reordering (fct_relevel()) renaming (fct_recode()) factors hand. need use two functions exercise #3. However, find working factors, good idea check forcats functions see whether can make life easier.reorder factor hand, simply state desired order factors, similar way specify via levels= parameters factor() function. However, fct_relevel() can move factors others “pushed back”.can also put level back, second level, etc. fct_relevel() flexible, check reference whenever use .rename individual levels use fct_recode() providing new = old pairs values.Note allows merge levels hand.exercise #3, redo exercise #2 using fct_relevel() fct_recode(). still need use factor() function convert Prime Probe factor specify levels labels. Use fct_relevel() fct_recode() inside mutate() verbs reorder relabel factor values (, first relabel reorder, whatever intuitive ). end product (plot) .exercise 3.","code":"\nletters <- c(\"C\", \"A\", \"D\", \"B\", \"A\", \"B\")\nletters_as_factor <- factor(letters, levels = c(\"B\", \"C\", \"D\", \"A\"))\nprint(letters_as_factor)## [1] C A D B A B\n## Levels: B C D A\n# specifying order for ALL levels\nletters_as_factor <- fct_relevel(letters_as_factor, \"D\", \"C\", \"B\", \"A\")\nprint(letters_as_factor)## [1] C A D B A B\n## Levels: D C B A\n# specifying order for just ONE level, the rest are \"pushed back\"\n# \"A\" should now be the first level and the rest are pushed back in their original order\nletters_as_factor <- fct_relevel(letters_as_factor, \"A\")\nprint(letters_as_factor)## [1] C A D B A B\n## Levels: A D C B\nletters_as_factor <- factor(c(\"C\", \"A\", \"D\", \"B\", \"A\", \"B\"))\nletters_as_factor <- fct_recode(letters_as_factor, \"_A_\" = \"A\", \"_C_\" = \"C\")\nprint(letters_as_factor)## [1] _C_ _A_ D   B   _A_ B  \n## Levels: _A_ B _C_ D\nletters_as_factor <- factor(c(\"C\", \"A\", \"D\", \"B\", \"A\", \"B\"))\nletters_as_factor <- fct_recode(letters_as_factor, \"_AC_\" = \"A\", \"_AC_\" = \"C\")\nprint(letters_as_factor)## [1] _AC_ _AC_ D    B    _AC_ B   \n## Levels: _AC_ B D"},{"path":"working-with-factors.html","id":"plotting-group-averages","chapter":"7 Working with Factors","heading":"7.5 Plotting group averages","text":"Let us keep practicing extend analysis compute plots averages condition (Prime×Probe) participants. Use preprocessing code exercise #3 , compute proportion per Participant×Prime×Probe, need group data Prime×Probe compute average performance across observers. Advice, reuse name column, e.g., used Psame proportion per Participant×Prime×Probe, use name Prime×Probe (e.g. Pavg). Otherwise, may turn confusing (least, mistake make routinely). Take look code , Range values ?routinely assume 1 \"\" (2-1) 2 \"B\" (6-4). Nope, 0 time Range = max(Response) - min(Response) executed, original values Response overwritten Response = mean(Response), just one value, mean. min() max() single value value, difference 0. obvious carefully consider code obvious (least ) straightaway. short, careful reusing column names. Better still, reuse , creative, come new ones!Getting back exercise, compute average performance per Prime×Probe. Store result computation new variable (’ve called persistence_avg) check results makes sense, e.g. just three columns Prime, Probe, Pavg (however decided name column). look like :exercise 4., plot results. Use geom_point() plus geom_line() plot mean response plot like like (hint, drop color mapping map Prime group property).\nexercise 5.exercise 6.","code":"\ntibble(ID = c(\"A\", \"A\", \"B\", \"B\"),\n       Response = c(1, 2, 4, 6)) |>\n  \n  group_by(ID) |>\n  summarise(Response = mean(Response),\n            Range = max(Response) - min(Response))"},{"path":"working-with-factors.html","id":"plotting-our-confidence-in-group-averages-via-quantiles","chapter":"7 Working with Factors","heading":"7.6 Plotting our confidence in group averages via quantiles","text":"plots , get sense identities probe prime (objects interruption) matter. Single band appears poorest prime (line lowest) probe (dots lower rest). Conversely, sphere excellent prime (line top) probe (dots high). However, averages plotted just point estimate likely effect strength alone tell us whether differences objects’ shape matter. , need perform statistical analysis get least feeling confident can differences, need plot measure variability associated statistics. .e., [1, 5, 9] [4, 5, 6] identical mean 5 standard deviation 4.5 times different (4.51 vs. 1). second case, true mean likely somewhere near 5, whereas much less confidence former one.One way characterize mean computing standard error. However, best used actual data distributed normally , least, symmetrically around mean, .e., distance observation mean irrespective whether larger smaller. luxury can expect variables live ±∞ range (support) practically observed range values far either floor ceiling. Adult height example latter: height 0 average adult height far enough limit distribution normal symmetric enough. Unfortunately, lot data collect psychology social science research fit description: Binomial data yes/correct/incorrect responses lives 0..1 range, response times long right tail negative even particularly short (200 ms realistic floor key presses, ~120 ms eye saccadic response specific experimental conditions.) End mention Likert scale data ordered categorical type data, use raw data compute even mean, let alone error, show visualize later.case outcome variable proportion limited 0 1 range. practical point view means measure variability unlikely symmetric relative mean (unique exception mean exactly 0.5). .e., think group average \\(P_{avg} = 0.8\\), points average can away mean (0.8) points (0.2 away ). compression called either ceiling (get squashed upper range) flooring (go certain value) effect. Thus, need measure take asymmetry account. Later learn using bootstrapping start simpler approach just using quantiles distribution understand variability.compute quantiles-based interval, need compute lower upper limits separately via quantiles. quantile 0.1 (10%) returns value, 10% values vector , quantile 0.9 (90%) means 10% values (90% ). , 80% confidence intervals includes values 10% 90% , alternatively, 0.1 0.9 quantiles.\ncompute , R function quantile().Modify code exercise #5 compute two additional variables/columns lower upper limits 89%56 interval (think limits 89% interval). , use geom_errorbar() plot 89% interval (need map two variable computed ymin ymax properties). plot look like (hint, drop color mapping map Prime group property).exercise 7.","code":"\nx <- 0:50\nquantile(x, 0.1)## 10% \n##   5"},{"path":"working-with-factors.html","id":"likert-scale-01","chapter":"7 Working with Factors","heading":"7.7 Dealing with Likert scales","text":"Likert scales one popular ways measure responses psychology. , least opinion literature, tend misanalyzed misreported (IMHO). quite common report average standard error response across participants, numbers actually computed (ordered categorical values, numbers) even trick computer (pretending levels actual numbers) numbers ill-suited characterize response, see . proper analysis Likert scale data requires use “Item Response Theory” although straightforward, outside scope book. Instead, look can visualize responses meaninful way, although complete story wait learn bootstrapping., use data collected using Intrinsic Motivation Inventory (IMI), participants needed respond 7-point Likert scale indicating indicate true statement (“enjoyed activity much” “pretty skilled activity.”) :trueHardly trueSlightly trueSomewhat trueMostly trueAlmost completely trueVery trueBefore visualize data, need read preprocess . Perform following using single pipe:Read file likert-scale.csv specifying column typesConvert Condition column factor 1 “game” 2 “experiment”Convert Response column factor using levels described .first five rows table look :exercise 8.Hopefully, using actual labels instead original number makes clear computing mean standard error impossible (average “Slightly true” “Hardly true”?). Instead, need count number response per response level, convert proportion responses, plot see distribution. know almost everything need perform . logic:group data per response levelsummarize data computing number entries per group, don’t forget ungroup table. can also combine two steps count responses per level.compute proportion per response dividing number responses level total number responses.computed aggregates, plot using geom_point() geom_line(). catch , geom_line() work discrete x-axis, specifying aesthetics x-axis, need use .integer(Response) instead just Response need use scale_x_continuous() specify breaks (1 7) labels (levels Likert scale). end results look follows:study question used experimental conditions, informative see whether produce difference responses. logic computation (almost) : need group response level experimental condition counting responses condition alone computing proportion (?). visualizing, need add color aesthetics distringuish groups. end results follows:exercise 10.Note missing information uncertainty proportion response (.e., error bars), show compute get bootstrapping. Also note even convert ordinal levels real numbers, using mean standard error still bad idea. Note squashed distribution lower side (particularly, averaging across groups). Mean nice descriptor symmetric distributions coincides mode (probable, highest point) median (point splits distribution half/half). skewed distributions, three statistics — mode, mean, median — different locations either report think better ways present data. goes standard error nice descriptor normally distributed data, less symmetric data (’ll missing information exact shape probably thinking normal one), good skewed data. , data lie equally sides, symmetric standard error overestimate variance left side (much space vary left), simultaneously underestimating right side. short, always look data compacting various statistic, even practice common (make good practice).","code":""},{"path":"tyding.html","id":"tyding","chapter":"8 Tidyng your data: joins and pivots","heading":"8 Tidyng your data: joins and pivots","text":"fun work tidy complete data. Unfortunately, often need preprocess tidy can analyze . addition dplyr, Tidyverse tidyr package helps problems. Grab exercise notebook let’s get started.","code":""},{"path":"tyding.html","id":"joins","chapter":"8 Tidyng your data: joins and pivots","heading":"8.1 Joining tables","text":"Sometimes, results study stored separate tables. example, demographic information can stored separately responses, former trials conditions, makes little sense duplicate . actual analysis, may need add information, merging , SQL/Tidyverse-speak, joining two tables (’ll use term “join” hereon).Examples make joining operations much easier understand, let us create two tables.\nTable 8.1: demographics (left) report (right) tables\njoin two tables, require “key” columns: columns contain values used identify matching rows two tables. can use multiple differently named columns purpose start simplest case: single column name \"ID\". join function (inner_join() , see details different joins) takes first row demographics table \"\" ID column look rows table reports ID == \"\". , remaining rows demographics table, one row time. takes three parameters: table x (first table), table y (second table), - vector key column names argument57. call merge() function similar produces identical results. Note column order changed merge(), used different order tables, content .\nTable 8.2: Joining/merging tables fully matching keys via join (left) merge (right). Note different order tables results different order columns.\nThings easy every key first table counterpart second one vice versa. Things get tricky, case. four different ways join tables (note produce identical results fully matching set keys, example ):inner join: uses key values present tables.full join: uses key values tables, matching keys missing one tables, rows filled NA.left join: uses key values present left (first) table, matching keys missing right table, , rows filled NA.right join: mirror twin left join, uses key values present right (second) table, matching keys missing left table, , rows filled NA.see join action, let us slightly modify reports table include ID “E” instead “D”. Now, “D” missing second table “E” missing demographics:\nTable 8.3: Now demographics (left) reports (right) table unique keys (“D” demographics, “E” reports) without match second table.\nInner join conservative excludes non-matching keys, note rows ID == \"D\" ID == \"E\" missing. default behavior merge function.Table 8.4: Inner join. rows matching keys mergedIn contrast, full join liberal includes rows tables, filling missing values NA (e.g., see Report ID == \"D\" Age ID == \"E\"). base R merge() function, turn inner join full one using = TRUE.Table 8.5: Full join. rows merged, NA used missing values rows complementary tableLeft join uses rows left (first) table, dropping extra rows second one. Note NA Report column ID == \"D\" rows ID == \"E\". left join via merge(), need specify .x = TRUE.Table 8.6: Left join. rows demographics tables used missing matching rows filled NARight join mirror twin left join, now rows ID == \"D\" missing missing values ID == \"E\". include .y = TRUE right join via merge().Table 8.7: Right join. rows reports tables used missing matching rows filled NAAs noted , can also use one key.\nTable 8.8: Two identically named key columns: ID Gender.\nTable 8.9: Joining/merging two tables ID Gender.Finally, key columns can named differently two tables. case, need “match” explicitly. dplyr joins, use named vector match pairs individual columns. merge(), supply two vectors: one columns first table (parameter .x) one columns second one (parameter .y). , need careful check columns order matches parameters.\nTable 8.10: Differently named key columns. VPCode demographics table corresponds ID reports; sex demographics corresponds Gender reports\nTable 8.11: Joining tables matching VPCode ID Sex Gender.saw examples , dplyr joins merge() produce identical results. However, recommend use former, simply function names make explicit kind join perform (something can figure checking additional parameters merge()).Download files IM.csv GP.csv need exercise #1. participants responses two questionnaires participant identified ID (Participant IM.csv Respondent GP.csv), Condition (experimental group belong ), Gender. Read tables join missing values table (participants missing GP.csv, three joins can , one pick?). , turn Condition Gender factors, Condition levels \"control\" (2) \"game\" (1) Gender levels \"female\" (1) \"male\" (2). final table look follows (’ve dropped columns IM.csv, fit page):exercise 1.Repeat exercise use merge().exercise 2.Now let us practice joining simulating data well. Create two tables need joined single key column. first table, Use rnorm() function generate normally distributed data mean 175 standard deviation 7.6 column Height (range expect cover 95% data?). second table, use normal distribution mean 80 standard deviation 10 column Weight. fill key column tables, inner right join give final table left full give longer one (test explicitly!). joining two tables, plot Height Weight superimpose linear regression fit. two columns correlated? given generated data?exercise 3.","code":"\ndemographics <- tibble(ID = c(\"A\", \"B\", \"C\", \"D\"),\n                       Age = c(20, 19, 30, 22))\nreports <- \n  tibble(ID = c(\"A\", \"B\", \"C\", \"D\", \"A\", \"B\", \"C\", \"D\")) |>\n  mutate(Report = round(runif(length(ID), 1, 7)), 2) |>\n  arrange(ID)\nvia_join <- inner_join(demographics, reports, by = \"ID\")\nvia_merge <- merge(reports, demographics, by = \"ID\")\nreports <- \n  tibble(ID = c(\"A\", \"B\", \"C\", \"E\", \"A\", \"B\", \"C\", \"E\")) |>\n  mutate(Report = runif(length(ID), 1, 7)) |>\n  arrange(ID)\ninner_tidy <- inner_join(reports, demographics, by = \"ID\") \ninner_base <- merge(reports, demographics, by = \"ID\")\nfull_tidy <- full_join(demographics, reports, by = \"ID\") \nfull_base <- merge(demographics, reports, by = \"ID\", all = TRUE)\nleft_tidy <- left_join(demographics, reports, by = \"ID\")\nleft_base <- merge(demographics, reports, by = \"ID\", all.x = TRUE)\nright_tidy <- right_join(demographics, reports, by = \"ID\") \nright_base <- merge(demographics, reports, by = \"ID\", all.y = TRUE)\ndemographics <- tibble(ID = c(\"A\", \"B\", \"A\", \"B\"),\n                       Gender = c(\"M\", \"F\", \"F\", \"M\"),\n                       Age = c(20, 19, 30, 22))\nreports <- tibble(ID = c(\"A\", \"B\", \"A\", \"B\"),\n                  Gender = c(\"M\", \"F\", \"F\", \"M\"),\n                  Report = runif(length(ID), 1, 7))\ninner_multi_tidy <- inner_join(demographics, reports, by = c(\"ID\", \"Gender\"))\ninner_multi_base <- merge(demographics, reports, by = c(\"ID\", \"Gender\"))\ndemographics <- tibble(VPCode = c(\"A\", \"B\", \"A\", \"B\"),\n                       Sex = c(\"M\", \"F\", \"F\", \"M\"),\n                       Age = c(20, 19, 30, 22))\n\nreports <- tibble(ID = c(\"A\", \"B\", \"A\", \"B\"),\n                  Gender = c(\"M\", \"F\", \"F\", \"M\"),\n                  Report = runif(length(ID), 1, 7))\ninner_diff_tidy <- inner_join(demographics, reports, by=c(\"VPCode\"=\"ID\", \"Sex\"=\"Gender\"))\ninner_diff_base <- merge(demographics, reports, by.x=c(\"VPCode\", \"Sex\"), by.y=c(\"ID\", \"Gender\"))"},{"path":"tyding.html","id":"pivoting","chapter":"8 Tidyng your data: joins and pivots","heading":"8.2 Pivoting","text":"Recall idea tidy data:variables columns,observations rows,values cells., also recall, quite often data stored wide format easier humans read.Table 8.12: Wide non-tidy table., Symmetry, Attractiveness, Trustworthiness different face properties participants responded , whereas values Response gave. can work table like often convenient column Scale code face property participants respond column Response hold values. , can easily split group data property performing analysis .can pivoting using base R reshape() function. pivoting fairly confusing business, Tidyverse alone offers three different solutions starting reshape2 package58, continuing original gather() spread()\nfunctions tidyr modern pivot_longer() pivot_wider() functions package.","code":""},{"path":"tyding.html","id":"pivot-longer","chapter":"8 Tidyng your data: joins and pivots","heading":"8.3 Pivot longer","text":"pivot_longer() takes table, can pipe function Tidyverse-style, vector column names need transformed. column names go one new column values go another new column. Defaults names two columns , respectively, \"name\" \"value\" can specify something suitable via, respectively, names_to values_to parameters.many bells--whistles (name value transformations, removing prefix via regular expressions, etc.), recommend looking manual vignette. However, cases four parameters need, let us see pivot_longer action.assume table presented widish_df table defined . columns want transform Symmetry, Attractiveness, Trustworthiness. Thus, simplest call defaults isWhen compare two tables, see original three columns × four rows now stretched twelve rows name-value pairs consistent across two tables59. noted , can improve specifying proper names new columns.want stick base R, can pivot longer via reshape() function can pivot longer wider tables. flexible , therefore, much confusing (least ). parameters need specify order emulate pivot_longer() call :direction. Either \"long\" (), \"wide\".idvar: names variables identify multiple records long format. contrast, pivot_longer(), assumes columns transform identity columns.varying: names columns turned single variable contains values new long table. Corresponds cols argument pivot_longer()v.names: name column values. Corresponds values_to parameter pivot_longer().timevar : sort corresponds names_to parameter pivot_longer(), name column indexes labels transformed columns go.times : default, reshape() put column names timevar column uses relative indexes instead. E.g., Symmetry column get index 1, Attractiveness get 2, Trustworthiness 3. can replaces indexes labels. , used labels column names used three values .Table 8.13: table pivoted via reshape.using reshape(), strongly suggest experimenting parameters get feeling (!) used. can see, involved uses fewer defaults pivot_longer(), need make sure understand .","code":"\nlong_tidy <- tidyr::pivot_longer(widish_df, \n                                cols=c(\"Symmetry\", \"Attractiveness\", \"Trustworthiness\"))\nlong_tidy <- tidyr::pivot_longer(widish_df, \n                               cols=c(\"Symmetry\", \"Attractiveness\", \"Trustworthiness\"),\n                               names_to = \"Scale\",\n                               values_to = \"Response\")\nlong_base <- reshape(widish_df, \n                     direction = \"long\",\n                     idvar = c(\"Participant\",  \"Face\"),\n                     varying = c(\"Symmetry\", \"Attractiveness\", \"Trustworthiness\"),\n                     v.names=\"Response\",\n                     times = c(\"Symmetry\", \"Attractiveness\", \"Trustworthiness\"),\n                     timevar = \"Scale\")"},{"path":"tyding.html","id":"likert-scale-02","chapter":"8 Tidyng your data: joins and pivots","heading":"8.4 Practice using Likert scale data","text":"Let us put new knowledge practice, using GP.csv file. questionnaire gaming habits, conducted prior experiment check whether two groups participants assigned Game Experiment conditions similar gaming habits. like visually inspect responses individual items questionnaire appear different conditions, tell us whether expect difference. , reuse approach summarizing plotting ordinal responses last seminar. Split computations two pipelines. One loads pre-processes data (steps 1-4). Another one produces summary stores different table (steps 5-6, see previous seminar forgot ). Advice, implement one step time, checking table making sure get expected results piping adding next operation.Read file, make sure specify column types.Convert Condition column factor “game” (1) “control” (2).Pivot GP.. columns. get table five columns: Respondent, Condition, Gender, Item (column original column names go), Response (column original go). Hint, can use slicing : specify range columns starts_with() function specify common prefix. Try approaches.Convert Response column factor assuming seven-point scale, levels areNot allSlightlySomewhatModeratelyQuite bitConsiderablyVery muchCount responses item condition.Convert counts proportion responses item condition.first table, long format, look like (show first four rows)second table, aggregated results, show like thisDo exercise 4.Repeat exercise now using base R reshape() function.\n::: {.practice}\nexercise 5.\n:::Now table proportion response condition item, lets plot compare visually. Use Response x-axis, Proportion y-axis, Condition color, think facet plot.\nexercise 6.Note computation plot problem: whenever response level used participant, missing table plot. However, entry every response level 0 count proportion, 0 counts response level missing absent response level. way solve problem using function complete() adds missing combinations data based subset columns specified fills values remaining columns either NA values specified. example table contains columns Class Count. Note defined Class factor five levels (“” till “E”) latter missing table. Complete adds row fill remaining column ‘NA’.Although cases, filling NA may reasonable, , want fill count 0 (fact particular class missing means never counted ). Note fill parameter takes named list named vector (? data types different columns still sure , reread difference atomic vectors lists).exercise 7.","code":"\nclass_df <-\n  tibble(Class = c(\"A\", \"B\", \"C\", \"D\"),\n         Count = c(20, 19, 30, 22)) |>\n  mutate(Class = factor(Class, levels = c(\"A\", \"B\", \"C\", \"D\", \"E\")))\n\nclass_df |>\n  complete(Class)## # A tibble: 5 × 2\n##   Class Count\n##   <fct> <dbl>\n## 1 A        20\n## 2 B        19\n## 3 C        30\n## 4 D        22\n## 5 E        NA\nclass_df |>\n  complete(Class, fill = list(Count = 0))## # A tibble: 5 × 2\n##   Class Count\n##   <fct> <dbl>\n## 1 A        20\n## 2 B        19\n## 3 C        30\n## 4 D        22\n## 5 E         0"},{"path":"tyding.html","id":"pivot-wider","chapter":"8 Tidyng your data: joins and pivots","heading":"8.5 Pivot wider","text":"can also always go long wide representation via pivot_wider() reshape functions. logic reverse, need specify columns identify different rows belong together, columns contain column names, contain values. example table face ratings, names columns column Scale values Response. columns identify rows belong together? case, Participant Face, rows long table combination Participant Face values merged together single row. explicitly specify id_cols, default, pivot_wider() use remaining columns identify rows belong together. irrelevant toy example, Participant Face left anyhow show things can get confusing overcome .Let us undo previous wide--long transformation get original wide table back!60Or, using explicit id_colsTable 8.14: table wide !can pivot wider using reshape() well. However, note , 07.10.2023, works correctly data frames, tibble (), need convert data frame via data.frame() .data.frame() (drop .data.frame() example see mean). Otherwise, need specifydirection = \"wide\"idvar : different rows belong together. id_cols pivot_wider() defaults .timevar : names_from pivot_wider(), column values used column names.v.names : values_from.sep: new column names constructed v.names + sep + timevar. default sep=\".\".main difference, compared pivot_wider(), column names constructed. reshape() function, v.names + sep + timevar rule means end column names Response.Symmetry instead just Symmetry.Table 8.15: table wide via reshape()!Let us take look importance id_cols. Imagine another column, say, response times. , long table look like thisFor pivot_wider, specify columns identify rows belong together, RT used well. , different every response, row original table unique end weird looking table wit lots NAs.remedy , need specify id columns explicitly, pivot_wider() can ignore drop rest:practice, let us take adaptation data turn onto wide format easier humans read. original form, table long format row pair prime probe stimuli.Let us turn wider table, single row corresponds single prime four new columns contain proportion responses individual probes. table look like (use round() function reduce number digits):overall procedure fairly straightforward (single pipe!):Read file (don’t forget specify column types!)Computer Psame proportion responses given number total responses Probe.Pivot table wider, think id columns. Also try without specifying see get.Compute average stability across probes put new Average column.Pipe output, using knitr::kable().Use pivot_wider() exercise 8.exercise 8.Repeat analysis now using reshape() function.exercise 9.Let us practice create group average summary square 5×4 table single row per Prime four columns Probe plus column says prime row corresponds . value cell, want code median value. table look like :know almost everything need, think implement single pipeline. Hints: match table definitely convert Prime Probe factors ensure consistent ordering (otherwise, sorted alphabetically), need group individual combinations prime probe computing summary statistics. , course, need pivot table wider (use preferred method).exercise 10.","code":"\nwide_again_tidy <-\n  long_tidy |>\n  pivot_wider(names_from = \"Scale\", values_from=\"Response\")\nwide_again_tidy <-\n  long_tidy |>\n  pivot_wider(id_cols = c(\"Participant\", \"Face\"), names_from = \"Scale\", values_from=\"Response\")\nwide_again_base <- reshape(as.data.frame(long_tidy),\n                           direction = \"wide\",\n                           idvar = c(\"Participant\", \"Face\"),\n                           timevar = \"Scale\",\n                           v.names = \"Response\",\n                           sep = \"_\")\nlong_tidy_rt <-\n  long_tidy |>\n  ungroup() |>\n  mutate(RT = round(rgamma(n(), 4, 3), 2))\nwide_odd_rt <-\n  pivot_wider(long_tidy_rt, names_from = \"Scale\", values_from=\"Response\")\nwide_rt <-\n  pivot_wider(long_tidy_rt,\n              id_cols = c(\"Participant\", \"Face\"),\n              names_from = \"Scale\",\n              values_from=\"Response\") "},{"path":"control-flow-chapter.html","id":"control-flow-chapter","chapter":"9 Controling computation flow","heading":"9 Controling computation flow","text":"Grab exercise notebook start.One powerful features R vector-based. Remember, everything vector (list). previous seminars saw can apply function, filter, perform computation values vector rows table single call. However, sometimes, need go one value row time explicitly. example, working time-series, might easier use explicit loop compute current value based previous state system. However, instances fairly rare, general rule R “probably need loop”. , go various tools render explicit loops redundant, cover loops well conditional control statements (-else) functions (ifelse, case_when).Note material cover () {<-code} loop mostly rarely used data analysis61 come examples artificial looking. time, need , simple structure, understanding using challenge.","code":""},{"path":"control-flow-chapter.html","id":"rep","chapter":"9 Controling computation flow","heading":"9.1 rep()","text":"basic repetition mechanism R rep() function. takes vector repeats specified number times.Alternatively, can repeat element specified number times repeating next one via parameter. difference options lies order elements new vector. can see vectors length individual value repeated four times.can specify length output vector via length.. combined times can useful producing truncated vectors. E.g., repeat three element vector want get ten values. Using times , can get either nine (times = 3) twelve (times = 4), ten. length.= 10 makes happen.However, can also use subsetting new vector achieve end.careful combining length., value repeated times , length.longer, sequence repeated . confusing might get unbalanced repeated sequence.exercise 1.","code":"\nrep(c(1, 2, 3), times = 4)##  [1] 1 2 3 1 2 3 1 2 3 1 2 3\nrep(c(1, 2, 3), each = 4)##  [1] 1 1 1 1 2 2 2 2 3 3 3 3\nrep(c(1, 2, 3), times=4, length.out = 10)##  [1] 1 2 3 1 2 3 1 2 3 1\nrep(c(1, 2, 3), times = 4)[1:10]##  [1] 1 2 3 1 2 3 1 2 3 1\nrep(c(1, 2, 3), each = 8, length.out = 10)##  [1] 1 1 1 1 1 1 1 1 2 2"},{"path":"control-flow-chapter.html","id":"repeating-combinations","chapter":"9 Controling computation flow","heading":"9.2 Repeating combinations","text":"create table combinations values, can use either base R expand.grid() tidyr’s implementation expand_grid(). latter bit robust can expand even tables matrices (see documentation subtle differences implementation output).usage straightforward, provide column names values get combinations values.expand_grid() works order values within columns.exercise 2.","code":"\nknitr::kable(grid_base)\nexpand_grid(gender=c(\"female\", \"male\"), \n            handidness=c(\"right\", \"left\"),\n            colorblindness=c(TRUE, FALSE))\nknitr::kable(grid_tidyr)"},{"path":"control-flow-chapter.html","id":"forloop","chapter":"9 Controling computation flow","heading":"9.3 For loop","text":"can loop (iterate) elements vector list via loop, similar -loops programming languages. However, use loop R fairly rare, vectors fundamental building block R , therefore, inherently vectorized (can thing values, one value time). sense, loop un-R, find using , consider whether simpler expressive way . time, loop simplest, clearest, robust way write code, means, use !general format isNote curly brackets. used put code inside function. , use put code inside loop. loop (code inside curly brackets) repeated many times number elements vector list loop variable62 getting assigned vector/list value iteration. Thus, print value vector can doHere, three elements vector, therefore code inside curly brackets repeated three times variable a_number taking value turn. .e., a_number equal 1 first iteration, 5 second, 200 third. Note code equivalent just assigning one value time a_number calling print() function three times.can see, really matter assign value variable repeat code. However, loop approach much flexible, easier read maintain. .e., let us assume decided alter print() call cat() instead. loop version just one line take care . copy-paste version, three lines need alter. imagine need repeat code hundred thousand times, copy-paste clearly viable solution.Also note might interested repeat code inside loop given number times interested loop variable values takes. example, might want repeat print(\"Ho!\") three times (Christmas time “Ho! Ho! Ho!” Santa Clause says). case, still need vector three elements can loop care three elements .Note using variable temp inside loop effect code inside curly brackets. can use three values, total number matters, values .orIn short, number elements vector determines many times code inside loop repeat. Vector elements stored loop variable iteration can either use (a_number example ) ignore (“Ho! Ho! Ho!”) example.exercises 3 4.One typical scenarios loop variable used index access element vector list. can build vector indexes via start:stop sequence tool used slicing. can compute length object via length() function. data.frame tibble, can figure number rows columns via, respectively, nrow() ncol() functions.exercise 5.can also nest loopsDo exercise 6.One scenario loops particularly advantages current value depends previous one (many previous values). case, new value depends previous one , turn, determines following ones computed known. means vectorized approach applies computation values parallel work need perform computation sequantial manner.next exercise, use loop create random walk. start zero (, initial value variable zero). next step, draw random value normal distribution using previous value mean (decide standard deviation ). function looking rnorm()). Generate ten-step random walk, might look like (can plot replicate exactly, set.seed() 1977):\nexercise 7.","code":"for(loop_variable in vector_or_list){\n  ...some operations using loop_variable that\n  changes its value on each iteration using\n  values from the vector or list... \n}\nfor(a_number in c(1, 5, 200)){\n  print(a_number)\n}## [1] 1\n## [1] 5\n## [1] 200\na_number <- 1 \nprint(a_number)## [1] 1\na_number <- 5\nprint(a_number)## [1] 5\na_number <- 200\nprint(a_number)## [1] 200\nfor(temp in 1:3){\n  print(\"Ho!\")\n}## [1] \"Ho!\"\n## [1] \"Ho!\"\n## [1] \"Ho!\"\nfor(temp in c(\"A\", \"B\", \"C\")){\n  print(\"Ho!\")\n}## [1] \"Ho!\"\n## [1] \"Ho!\"\n## [1] \"Ho!\"\nfor(temp in seq(100, 300, length.out=3)){\n  print(\"Ho!\")\n}## [1] \"Ho!\"\n## [1] \"Ho!\"\n## [1] \"Ho!\"\nvector_of_some_numbers <- c(1, 5, 200)\nfor(index in 1:length(vector_of_some_numbers)){\n  print(vector_of_some_numbers[index])\n}## [1] 1\n## [1] 5\n## [1] 200\nfor(letter in c(\"A\", \"B\", \"C\")){\n  for(number in 1:2){\n    cat(letter, number, \"\\n\")\n  }\n}## A 1 \n## A 2 \n## B 1 \n## B 2 \n## C 1 \n## C 2"},{"path":"control-flow-chapter.html","id":"if-else","chapter":"9 Controling computation flow","heading":"9.4 Conditional statement","text":"programming languages, can control flow execution using -else statements. general usage follows.rules logical indexing, can define condition using mathematical comparisons.However, aware uses single logical value. one value, stops error63.logical indexing, can combine several conditions using logical operators && ||.example, can check whether x smaller zero larger -10.However, aware R &/| &&/|| versions logical operators (single versus double symbols). perform logical operations vectors double-symbol works single value. E.g., single-symbol returns TRUE/FALSE value x&& || generate error vectors longer64.Let us combine loop -else operator. Generate vector ten normally distributed values (, rnorm() function). Loop loop print() cat() \"Positive\" value larger zero \"positive\" . results look like this65Do exercise 8.","code":"\nif (some_condition) {\n  # code runs if some_condition is TRUE\n} else {\n  # code runs if some_condition is FALSE\n}\nx <- -3\nif (x > 0) {\n  cat(\"X is definitely greater than  zero\")\n} else {\n  cat(\"X is not greater than zero.\")\n}## X is not greater than zero.\nx <- c(10,  -3)\nif (x < 0) {\n  cat(\"Bingo!\")\n} else {\n  cat(\"I don't like this x.\")\n}## Error in if (x < 0) {: the condition has length > 1\nx <- -3\nif ((x < 0) && (x > -10)) {\n  cat(\"Bingo!\")\n} else {\n  cat(\"I don't like this x.\")\n}## Bingo!\nx <- c(10, -3, -11)\n(x < 0) & (x > -10)## [1] FALSE  TRUE FALSE\n# This works\nx <- -3\n(x < 0) && (x > -10)## [1] TRUE\n# This generates and error\nx <- c(10, -3, -11)\n(x < 0) && (x > -10)## Error in (x < 0) && (x > -10): 'length = 3' in coercion to 'logical(1)'##  [1]  0.03003683  1.22390943  1.71573769 -0.89994016  0.55507190  0.42319195\n##  [7]  0.82993426 -1.28614375  1.21511589 -0.05815403## [1] \"Positive\"\n## [1] \"Positive\"\n## [1] \"Positive\"\n## [1] \"Not positive\"\n## [1] \"Positive\"\n## [1] \"Positive\"\n## [1] \"Positive\"\n## [1] \"Not positive\"\n## [1] \"Positive\"\n## [1] \"Not positive\""},{"path":"control-flow-chapter.html","id":"ifelse","chapter":"9 Controling computation flow","heading":"9.5 ifelse()","text":"wrote , strength R lies built-vectorization typically renders loops unnecessary. Although used loop apply comparison one element time, R function ifelse() performs task vectors. format ifelse(logical-vector, values--true, values--false) logical vector TRUE FALSE values can produced via comparison. example, can classify letters \"Vowel\" \"Consonant\" using %% check whether letter vowels list (note %, distinguished %% matching loop).Note vectors values--true values--false must equal length. example, one can use ifelse() replace vowels original vector keep consonant letter using letters vector values--falseHowever vectors values--true values--false ideally either match length logical comparison vector single value. Otherwise, recycled might lead confusing results. E.g., , can figure \"Consonant1\" \"Consonant2\" used?Use ifelse() replicate exercise 8 without loop.exercise 9.","code":"\nvowels <- c(\"a\", \"e\", \"i\", \"o\", \"u\")\nletters <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\nifelse(letters %in% vowels, \"Vowel\", \"Consonant\")## [1] \"Vowel\"     \"Consonant\" \"Consonant\" \"Consonant\" \"Vowel\"     \"Consonant\"\nifelse(letters %in% vowels, \"Vowel\", letters)## [1] \"Vowel\" \"b\"     \"c\"     \"d\"     \"Vowel\" \"f\"\nifelse(letters %in% vowels, \"Vowel\", c(\"Consonant1\", \"Consonant2\"))## [1] \"Vowel\"      \"Consonant2\" \"Consonant1\" \"Consonant2\" \"Vowel\"     \n## [6] \"Consonant2\""},{"path":"control-flow-chapter.html","id":"case_when","chapter":"9 Controling computation flow","heading":"9.6 case_when","text":"Function ifelse() useful just two outcomes. can use nested ifelse calls, just like can use nested -else statements becomes awkward hard read fast. Instead, can use function case_when() dplyr library. , can use many conditions want. general format isFor example, internet tells letter y complicated, can either consonant vowel depending word. Thus, can label “Complicated” via case_when() (note vectorized nature , applies comparison one element vector time)Note last condition always TRUE, looks odd servers “default” branch: got far, conditions must FALSE, must TRUE (options left).exercise 10.","code":"case_when(Condition1 ~ Value1,\n          Condition2 ~ Value2,\n          ...\n          ConditionN ~ ValueN)\nvowels <- c(\"a\", \"e\", \"i\", \"o\", \"u\")\nletters <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"y\")\ncase_when(letters == \"y\" ~ \"Complicated\",\n          letters %in% vowels ~ \"Vowel\",\n          TRUE ~ \"Consonant\")## [1] \"Vowel\"       \"Consonant\"   \"Consonant\"   \"Consonant\"   \"Vowel\"      \n## [6] \"Consonant\"   \"Complicated\""},{"path":"control-flow-chapter.html","id":"breaking-out-of-the-loop","chapter":"9 Controling computation flow","heading":"9.7 Breaking out of the loop","text":"code inside loop typically repeated every value vector supplied. However, way break using break statement. stops execution code inside loop immediately continues code immediately loop.Note case, code executed (incompletely!) . Typically, break used combination -else statement break loop, certain condition met. Let us practice. , generate ten normally distributed numbers, loop print one. However, break fifth value. , need loop indexes values (go 1 length() vector). Thus, loop variable contain index element x (called ix) need use get value position within vector. index equal 5, break loop.\nten values ’ve generated , output following (“Done today” printed loop, also first printed value exited loop, analysis logic question, technical one).exercise 11.","code":"\nfor(y in c(1, 2, 3)){\n  print(\"This will be exected\")\n  break\n  print(\"But this won't be\")\n}## [1] \"This will be exected\"\nprint(\"Now to the code AFTER the loop\")## [1] \"Now to the code AFTER the loop\"\nset.seed(164)\nx <- rnorm(10)\nfor(ix in 1:length(x)){\n  if (ix == 5)  break\n  print(x[ix])\n}## [1] 0.03003683\n## [1] 1.223909\n## [1] 1.715738\n## [1] -0.8999402\nprint(\"Done for today\")## [1] \"Done for today\""},{"path":"control-flow-chapter.html","id":"using-for-loop-to-load-and-join-multiple-data-files","chapter":"9 Controling computation flow","heading":"9.8 Using for loop to load and join multiple data files","text":"analysis starts loading data. Quite often, data individual participants stored different files identical structure, need code figures files need load, loads one time binds one final table. Using loop elegant way implement job gives another example loops can useful. walk details implement code loads merges individual files persistence study. Download persistence.zip unzip Persistence subfolder (want create mess main folder!).First, need character vector relevant file names. Package looking fs (File System). everything need work file system, including working file names (dropping adding path, extension, etc.), creating/moving/deleting files, checking whether file exists, . One function use dir_ls() list files specified folder. two parameters need path folder (can use relative path) , optionally, glob filter string. latter globbing wildcard pattern, * stands “sequence characters” ? stand “one arbitrary character. csv file, pattern \"*.csv\". Test single function call using appropriate path glob parameters make sure get files Persistence folder.Next, need create full table variable (, typically, call results reports) initialize empty data.frame() (empty tibble). loop file names, read one file time (don’t forget specify column types), use bind_rows() combine full table new table loaded. Note bind_rows() returns new table, need assign back original full table variable. done, table 5232 rows twelve columns.exercise 12.","code":""},{"path":"control-flow-chapter.html","id":"apply","chapter":"9 Controling computation flow","heading":"9.9 Apply","text":"noted , loops job might elegant way things. R, can apply function row column matrix. addition, case-specific versions , lapply.function called apply apply values vector. sense, applying functions whole time calling . example, might compute sinus sequence numbers asOr, can apply sinus function number sequence (note pass name function alone sin call , round brackets!)might ask, point use apply? much simple vector cases like , useful two dimensional data, can apply function along horizontal (rows) vertical (columns) margin. example, imagine need compute average (median, quantile) row column matrix (something might fairly often posterior samples Bayesian statistics).Let us create simple 3 4 matrix normally distributed random numbers.expect median value row column 0 data points, close exactly zero. Computing median row (get three numbers)Similarly column (, four numbers)go details functions, concentrating similar functionality purrr package. However, find working matrices needing apply function rows data frame, apply might simpler solution. Keep option mind, feel either looping purrring looks inadequate.","code":"\nsin(seq(0, pi, length.out = 5))## [1] 0.000000e+00 7.071068e-01 1.000000e+00 7.071068e-01 1.224606e-16\nsapply(seq(0, pi, length.out = 5), sin)## [1] 0.000000e+00 7.071068e-01 1.000000e+00 7.071068e-01 1.224606e-16\na_matrix <- matrix(rnorm(12), nrow = 3)\na_matrix##               [,1]       [,2]       [,3]       [,4]\n## [1,] -1.306076e-05 -0.9815949  0.3981706 -0.1809577\n## [2,]  1.880631e-02 -0.4210554 -1.2544423 -0.2319481\n## [3,]  2.429102e+00 -1.1964317  0.2265607  0.5361628\napply(a_matrix, MARGIN = 1, FUN = median)## [1] -0.0904854 -0.3265017  0.3813618\napply(a_matrix, MARGIN = 2, FUN = median)## [1]  0.01880631 -0.98159487  0.22656072 -0.18095773"},{"path":"control-flow-chapter.html","id":"purrr","chapter":"9 Controling computation flow","heading":"9.10 Purrr","text":"Package purrr part tidyverse. provides functional programming approach similar apply easier use (IMHO) explicit consistent way describe combine output. Language-wise, apply function, use map inputs outputs66The basic map() function always returns list can explicitly state expect function return number (map_dbl()) outputs combined numeric vector. , unlike apply, map_dbl() generate error outputs converted numeric.basic call similar apply easier use can explicitly address current value via . variable (parts Tidyverse) can write “normal” function call, prefixing ~. example computing sinus . First, apply via function name onlyNow, magic tilde ~. Note explicit call sin() function . argument., using map_dbl() case looks complete overkill. let us something relevant. Let us implement loading merging persistence study files. already know get vector names relevant files. Now can use map() function vector generate list tables combine single table piping list_rbind() binds tables rows (twin list_cbind() function binds table columns). using ~ call notation, remember . correspond single value vector file names (, single filename). , get single table twelve columns 5232 rows. need single pipeline entire operation.exercise 13.just mapped inputs outputs using read_csv() functional programming particularly useful, program functions. Let us program function takes filename, loads file returns total number trials/rows (forgot compute number rows table, see ). function, use map_dbl vector persistence filenames. get vector ten values. Now can easily see something wrong one files must pay attention amount data .exercise 14.","code":"\nmap_dbl(seq(0, pi, length.out = 5), sin)## [1] 0.000000e+00 7.071068e-01 1.000000e+00 7.071068e-01 1.224606e-16\nmap_dbl(seq(0, pi, length.out = 5), ~sin(.))## [1] 0.000000e+00 7.071068e-01 1.000000e+00 7.071068e-01 1.224606e-16##  [1] 528 528 480 528 528 528 528 528 528 528"},{"path":"missing-data.html","id":"missing-data","chapter":"10 Missing data","heading":"10 Missing data","text":"Grab exercise notebook start!Sometimes data missing. can missing explicitly NA standing Available / Missing data. , can missing implicitly entry particular condition. latter case, strategy make missing values explicit first (discussed ).() missing values, represented NA R, must decide deal : can use information directly missing data can diagnostic , can impute values using either sophisticated statistical methods via simple average/default value strategy, can exclude analysis. Every option pros cons, think carefully use option whose effects fully understand compromise rest analysis.","code":""},{"path":"missing-data.html","id":"complete","chapter":"10 Missing data","heading":"10.1 Making missing data explicit (completing data)","text":"make implicit missing data explicit, tidyr provides function complete() already met. figures combinations values columns specified, finds missing combinations, adds using NA (specified value) columns. Imagine toy incomplete table (data Participant 2 Face M-2).Table 10.1: Table data Face M2 Participant 2.can complete table specifying columns define required combinations.Table 10.2: Completed table explicit NAsFor non-factor variables (Participant numeric Face character/string), complete finds unique values column finds combinations elements. However, variable factor, complete uses levels, even levels present data. E.g., can use Face factor three levels: “M-1”, “M-2”, “F-1”. case, information missing participants (neither responses face “F-1”) filled NAs. approach useful know combinations present data need ensure completeness.Table 10.3: Completed missing data including F-1 face.exercise 1.can also supply default values via fill parameter takes named list, e.g., list(column_name = default_value). However, ’d like remind impute values “make sense” given rest analysis. Zeros illustration , real-life scenario, ruin inferences either artificially lowering symmetry attractiveness second face (lucky) break stop analysis expects values within 1-7 range (rmANOVA won’t bothered first scenario),Table 10.4: Completed missing data non-NA values.exercise 2.complete() easy use convenience function can easily replicate . , need create new table lists combinations variables interested (can use either expand.grid() expand_grid() ) left joining original table (left join? use another join purpose?). results complete() .exercise 3.","code":"\ncomplete_df <- complete(incomplete_df, Participant, Face)\nextended_df <-\n  incomplete_df |>\n  # converting Face to factor with THREE levels (only TWO are present in the data)\n  mutate(Face = factor(Face, levels = c(\"M-1\", \"M-2\", \"F-1\"))) |>\n  # completing the table\n  complete(Participant, Face)\nfilled_df <- \n  incomplete_df |>\n  complete(Participant, Face, fill=list(Attractiveness=0, Symmetry=0))"},{"path":"missing-data.html","id":"na.omit","chapter":"10 Missing data","heading":"10.2 Dropping / omitting NAs","text":"two approaches excluding missing values. can exclude incomplete rows missing values variable via na.omit() (base R function) drop_na() (tidyr package function). can exclude rows NA specific columns specifying names.table see belowTable 10.5: Table missing values.First, can ensure complete cases via na.omit()Table 10.6: Complete cases via na.omit()via drop_na()Table 10.7: Complete cases via drop_na()Second, drop rows Attractiveness data missing.Table 10.8: Complete Attractiveness via drop_na()Practice time. Create table missing values exclude missing values using na.omit() drop_na().exercise 4.drop_na() convenient function can replicate functionality using .na() combination filter dplyr function logical indexing. Implement code excludes rows contain NA specific column using two approaches.exercises 5 6.Recall can write functions R can use create convenience wrappers like drop_na(). Implement code uses logical indexing function takes table (data.frame) first argument name single column second, filters rows NA column returns table back.exercise 7.noted , can also impute values. simplest strategy use either fixed average (mean, median, etc.) value. tidyr function performs simple substitution replace_na()67 , second parameter, takes named list values list(column_name = value_for_NA). toy table, can replace missing Attractiveness Symmetry values default value, e.g. 0 -1 (arbitrary, just demonstrate works, things like real analysis unless know !)Table 10.9: Missing values filled 0 -1Do exercise 8.Unfortunately, replace_na() works constant values handle grouped tables well68 replace NA mean value grouped data, need combine old knowledge ifelse(conditon, value_if_true, value_if_false) function learned . Recall function vectorized cousin -else takes 1) vector logical values (condition), 2) vector values returned condition true, 3) vector values returned condition false. Note usual rules vector length-matching apply, three vectors different length, automatically (silently) adjusted match length condition vector. computations, can use original values . replace negative values keep positive ones:, essentially, tell function, “condition false, use original value”. Now, turn! Using vector ifelse() function, replace negative values mean value positive values vector.exercise 9.Now know use ifelse(), replacing NA mean (relatively) easy. Use adaptation_with_na table replace missing information using participant-specific values.Table 10.10: adaptation_with_na.csv missing valuesWe missing data different columns, use different case. one way approach problem. know number trials specific Prime × Probe combination, can replace missing values Ntotal participant-specific median value (“typical” integer number trials forget na.rm option, see manual details). Nsame trickier. , compute proportion response condition Psame = Nsame / Ntotal. produce missing values whenever Nsame missing. Now, replace missing Psame values (.na()) mean Psame per participant (, watch na.rm!) using ifelse() (can use inside mutate()). Finally, compute missing values Nsame Psame Ntotal (forget round , end integer number trials). entire computation implemented single pipeline. end following table.Table 10.11: adaptation_with_na.csv imputed valuesDo exercise 10.","code":"\nna.omit(widish_df_with_NA)\nwidish_df_with_NA |>\n  drop_na()\nwidish_df_with_NA |>\n  drop_na(Attractiveness)\nwidish_df_with_NA |>\n  replace_na(list(Attractiveness = 0, Symmetry = -1)) \nv <- c(-1, 3, 5, -2, 5)\nifelse(v < 0, 0, v)## [1] 0 3 5 0 5"},{"path":"strings.html","id":"strings","chapter":"11 Working with strings","heading":"11 Working with strings","text":"working strings, strongly suggest consulting manual vignettes stringr package. many functions cover needs. Grab exercise notebook start.","code":""},{"path":"strings.html","id":"warming-up","chapter":"11 Working with strings","heading":"11.1 Warming up","text":"start working strings, let us warm preprocessing band-adaptation.csv working .Read (try specifying URL instead local filename). forget specify column types!compute proportion “” responses using Nsame (number “” responses) Ntotal (total number trials).Convert Prime Probe column factors order “Sphere”, “Quadro”, “Dual”, “Single”.Compute median median absolute deviation median Psame combinations Prime Probe.table look follows:Table 11.1: bands_dfDo exercise 1.","code":""},{"path":"strings.html","id":"glue","chapter":"11 Working with strings","heading":"11.2 Formatting strings via glue()","text":"table gives us information median probability seeing rotation absolute deviation median. However, convenient reader combine two pieces information single entry form “ ± ”. Plus, easier see pattern square table one Prime per row one Probe per column. table mind look like :Table 11.2: Probability persistence, median ± MADYou already know perform second step (pivoting table wider turn Probe factor levels columns). first step, need combine two values string. different ways construct string via sprintf(), paste(), via glue package. start Tidyverse’s glue() explore base R functions later.glue package part Tidyverse, already installed. However, part core tidyverse, get imported automatically via library(tidyverse) need import separately use glue:: prefix. Function glue() allows “glue” values code directly string. simply surround R code wiggly brackets inside string result code execution glued . use just variable, value glued-. can put code inside, although, code put, harder read understand .Use table prepared exercise 1 compute new column “ ± ” (want use round() function restrict values just 2 digit decimal point). Think want perform computation make easier (pivoting?) column(s?) need pivot wider.exercise 2.","code":"\nanswer <- 42\nbad_answer <- 41\nglue::glue(\"The answer is {answer}, not {abs(bad_answer / -4)}\")## The answer is 42, not 10.25"},{"path":"strings.html","id":"paste","chapter":"11 Working with strings","heading":"11.3 Formatting strings via paste()","text":"Base R functions paste() paste0() concatenate vector strings single string. recall, vector values can one (flexible) type. Therefore, vector intersperses strings values, first converted strings anyhow. difference paste() paste0() former puts separator string -value (defaults ' ' can define via sep argument), whereas paste0() uses separator. can replicate glue() example.Redo exercise 2 using one paste functions instead glue().exercise 3.","code":"\nanswer <- 42\nbad_answer <- 41\npaste(\"The answer is \", answer, \", not \", abs(bad_answer / -4), sep = \"\")## [1] \"The answer is 42, not 10.25\"\npaste0(\"The answer is \", answer, \", not \", abs(bad_answer / -4))## [1] \"The answer is 42, not 10.25\""},{"path":"strings.html","id":"sprintf","chapter":"11 Working with strings","heading":"11.4 Formatting strings via sprintf()","text":"detailed string formatting, base R sprintf() function provides C-style string formatting (Python’s original string formatting common way format string many programming languages). general function call sprintf(\"string formatting\", value1, value2, value), values inserted string. \"string formatting\", specify want put value via % symbol followed optional formatting info required symbol defines type value. type symbols ares stringd integerf float value using “fixed point” decimal notatione float value using scientific notation (e.g., 1e2).g “optimally” printed float value, scientific notation used large small values (e.g., 1e+5 instead 100000 1-e5 0.00001).example formatting string using integer:limited single value can put string. can specify locations via % must make sure pass matching number values. fewer parameters specified string, receive error. many, warning69. running , can figure call actually work (output) produce error warning?case real values two options: %f %g. latter uses scientific notation (e.g. 1e10 10000000000) make representation compact. formatting floating numbers, can specify number decimal points displayed.Note functions R, sprintf() vectorized pass vector values generate vector strings one formatted string value.means can use sprintf() work column base R inside mutate() Tidyverse verb.Redo exercise #2 use sprintf() instead glue().exercise 4.","code":"\nsprintf(\"I had %d pancakes for breakfast\", 10)## [1] \"I had 10 pancakes for breakfast\"\nsprintf(\"I had %d pancakes and either %d  or %d stakes for dinner\", 2)\nsprintf(\"I had %d pancakes and %d stakes for dinner\", 7, 10)\nsprintf(\"I had %d pancake and %d stakes for dinner\", 1, 7, 10)\ne <- 2.71828182845904523536028747135266249775724709369995\nsprintf(\"Euler's number is roughly %.4f\", e)## [1] \"Euler's number is roughly 2.7183\"\nsprintf(\"The number is %d\", c(2, 3))## [1] \"The number is 2\" \"The number is 3\"\ntibble(Number = 1:3) |>\n  mutate(Message = sprintf(\"The number is %d\", Number)) |>\n  knitr::kable()"},{"path":"strings.html","id":"extracting-information-from-a-string","chapter":"11 Working with strings","heading":"11.5 Extracting information from a string","text":"Previous exercises dealt combining various bits information single string. Often, also need opposite: extract bits information single string. example, toy table face perception, working , Face column code gender face \"M\" (table short can easily assume faces genders used) second index (1 2). worked persistence, Participant column encoded year birth gender, whereas Session contained detailed information year, month, day, hour, minutes, seconds merged together. several ways extract information, either extracting one piece time via substr() string processing library stringr. Alternatively, can split string column several columns via separate() use extract() function.","code":""},{"path":"strings.html","id":"splitting-strings-via-separate","chapter":"11 Working with strings","heading":"11.6 Splitting strings via separate()","text":"Function separate() part tidyr use straightforward: pass 1) name column want split, 2) names columns needs split , 3) separator symbol indexes splitting positions. Examples using face table make clear. Reminder, original wide table want separate Face FaceGender FaceIndex.convenient “dash” two, can use separator symbol:Note original Face column gone. can keep via remove=FALSE optionWe also need extract information. example, can extract face gender face index. get gender, specify one column add extra=\"drop\" parameter, telling separate() drop extra piece obtained:Alternatively, can explicitly ignore pieces using NA column name:keeping second piece FaceIndex column? ignore first one via NALet’s practice. Use separate() preprocess persistence data create two new columns hour minutes Session column. single pipeline, starting reading files (use tidyverse read_csv() specify column types!) renaming Shape1 (Prime) Shape2 (Probe) columns. results look like , think columns drop keep (first four rows, think can limit output way via head() slice_head() functions):exercise 5.noted , position individual pieces fixed, can specify explicitly. Let us make toy table bit explicitFor toy faces table, first piece gender last one index. Thus, tell separate() starting position pieces, starting second one:, ’ve create Dash column separator , course, omitted via NA column name.Practice time! Using persistence data extract birth year gender participants Participant code (however, keep code column). Put nice extra touch converting year number (separate() splits string strings well) gender factor type better labels. look like:exercise 6.","code":"\nwidish_df <- \n  tibble(Participant = c(1, 1, 2, 2),\n         Face = rep(c(\"M-1\", \"M-2\"), 2), \n         Symmetry = c(6, 4, 5, 3),\n         Attractiveness = c(4, 7, 2, 7),\n         Trustworthiness = c(3, 6, 1, 2))\n\nknitr::kable(widish_df)\nwidish_df |>\n  separate(Face, into=c(\"FaceGender\", \"FaceIndex\"), sep=\"-\")\nwidish_df |>\n  separate(Face, into=c(\"FaceGender\", \"FaceIndex\"), sep=\"-\", remove=FALSE)\nwidish_df |>\n  separate(Face, into=c(\"Gender\"), sep=\"-\", remove=FALSE, extra=\"drop\")\nwidish_df |>\n  separate(Face, into=c(\"Gender\", NA), sep=\"-\", remove=FALSE)\nwidish_df |>\n  separate(Face, into=c(\"Gender\", NA), sep=\"-\", remove=FALSE) |>\n  knitr::kable()\nwidish_df |>\n  separate(Face, into=c(NA, \"Index\"), sep=\"-\", remove=FALSE)\nwidish_df |>\n  separate(Face, into=c(NA, \"Index\"), sep=\"-\", remove=FALSE) |>\n  knitr::kable(align = \"c\")\nwidish_df |>\n  separate(Face, into=c(\"FaceGender\", \"Dash\", \"FaceIndex\"), sep=c(1, 2))\nwidish_df |>\n  separate(Face, \n           into = c(\"FaceGender\", \"Dash\", \"FaceIndex\"), \n           sep = c(1, 2), \n           remove = FALSE) |>\n  knitr::kable()\nwidish_df |>\n  separate(Face, into=c(\"FaceGender\", NA, \"FaceIndex\"), sep=c(1, 2))\nwidish_df |>\n  separate(Face,\n           into = c(\"FaceGender\", NA, \"FaceIndex\"), \n           sep = c(1, 2)) |>\n  knitr::kable()"},{"path":"strings.html","id":"extracting-a-substring-when-you-know-its-location","chapter":"11 Working with strings","heading":"11.7 Extracting a substring when you know its location","text":"Base R provides function extract substring (many substrings) via substr() function (can also alias substring()). takes string (vector strings) vectors start stop indexes substring.Repeat exercise 6 use substr() extract column (BirthYear Gender) participant code.exercise 7.Tidyverse stringr library working strings. uses consistent naming scheme str_<action> function covers virtually tasks related working strings. stringr equivalent substr() str_sub() behaves similarly.Repeat exercise 7 using str_sub() function.exercise 8.","code":"\nface_img <- c(\"M01\", \"M02\", \"F01\", \"F02\")\nsubstr(face_img, 2, 3)## [1] \"01\" \"02\" \"01\" \"02\"\nface_img <- c(\"M01\", \"M02\", \"F01\", \"F02\")\nstr_sub(face_img, 2, 3)## [1] \"01\" \"02\" \"01\" \"02\""},{"path":"strings.html","id":"detecting-a-substring-using-regular-expressions","chapter":"11 Working with strings","heading":"11.8 Detecting a substring using regular expressions","text":"people, confronted problem, think “know, ’ll use regular expressions.” Now two problems.Jamie ZawinskOne powerful ways work strings via regular expressions allow code flexible pattern matched substring within string. example, can detect whether string contains number without knowing located. pattern \"\\\\d{3}\" means looking 3 (hence {3}) digits (hence \\\\d). base R functions grepl()70 grep() , correspondingly, return vector logical values whether pattern match index vector elements matched.Stringr library version obvious name str_detect() acts similar grepl(), .e., returns vector logical values whether pattern matched. Note, however, reverse order arguments, str_ function always take (vector ) strings first parameterYou can also look 1 digits (+)specific wordOr specific word beginning (^) stringWhen comes regular expressions, shown far even tip iceberg, tip tip iceberg best. flexible, allowing code complicated patterns also hard read , therefore, hard debug71. example, regular expression check validity email address72Still, need work text indispensable, remember . facing actual task grab cheatsheet use online expression tester debug pattern.next exercise, use regular expression filter() Primes Probes end single digit. know end single digit, digit , can make simple expression job. want practice working cheatsheet, must specify one digit allowed must last symbol. pattern works, end table Primes Probes \"heavy poles sphere\".exercise 9.","code":"\nQandA <- c(\"What was the answer, 42, right?\", \"No idea! What could it be, 423?\")\n# returns logical vector for each element\ngrepl(\"\\\\d{3}\", QandA)## [1] FALSE  TRUE\n# returns index of elements for which pattern was matched\ngrep(\"\\\\d{3}\", QandA)## [1] 2\nstr_detect(QandA, \"\\\\d{3}\")## [1] FALSE  TRUE\nstr_detect(QandA, \"\\\\d+\")## [1] TRUE TRUE\nstr_detect(QandA, \"What\")## [1] TRUE TRUE\nstr_detect(QandA, \"^What\")## [1]  TRUE FALSE(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])"},{"path":"strings.html","id":"extracting-substring-defined-by-a-regular-expression","chapter":"11 Working with strings","heading":"11.9 Extracting substring defined by a regular expression","text":"can just detect substring defined regular expression also extract . advantage may know many symbols substring starts, regular expression give maximal flexibility. function str_extract() works similar str_detect() returns actual detected substring instead just TRUE FALSE. Use extract participants unique code, first three letters Participant column. , can simply use substr() want write pattern matches 1) one 2) upper case letters 3) beginning string.exercise 10.","code":""},{"path":"strings.html","id":"replacing-substring-defined-by-a-regular-expression","chapter":"11 Working with strings","heading":"11.10 Replacing substring defined by a regular expression","text":"Another manipulation replace arbitrary substring fixed one. base R provides functions sub() replaces first occurence matched pattern gsub() replaces matched substring. Stringr equivalents str_replace() str_replace_all(). main difference, grepl() versus str_detect() order parameters: str_detect() input string first parameter, followed pattern replacement string, whereas grepl() pattern, replacement, input string order.exercise, use sub() str_replace() anonymize birth year participants. need replace four digits represent birth year single \"-\". table look follows:exercise 11.Now, repeat exercise replace single digit Participant code ‘-’. functions use produce results exercise 11?exercise 12.","code":""},{"path":"sampling-and-simulations.html","id":"sampling-and-simulations","chapter":"12 Sampling and simulations","heading":"12 Sampling and simulations","text":"important trick toolbox skill resample simulate data. latter, sampling predefined distributions, allows develop analysis routine ensure can correctly recover anticipated effects even collected data perform power analysis. Resampling data paves way non-parametric bootstrapping permutation testing helps whenever assumptions parametric tests violated require estimate easy derive analytically.Grab exercise notebook reading .","code":""},{"path":"sampling-and-simulations.html","id":"estimating-mean-of-a-normal-distribution-via-resampling","chapter":"12 Sampling and simulations","heading":"12.1 Estimating mean of a normal distribution via resampling","text":"Let us start simple. task generate samples normal distribution use resampling approach estimate original mean. Step one simple, decide mean standard deviation normal distribution generate 20 samples using rnorm() function (r<distribution functions generate random number based distribution parameters). Check results visually plotting histogram adding red vertical line indicate true mean distribution. also need see difference true mean sample mean, include blue vertical line indicate sample mean. Finally, always nice visual textual information plots, add information true mean, number samples, sample mean plot’s title. Run code several times appreciate variability data , therefore, sample mean.\nplot look something like (number set.seed 1745).\nexercise 1.real life, know true mean need collect data begin . also know sample mean different true mean73 like know much can trust value. words, like know much sample mean vary draw samples distribution. Theoretically, want draw samples “true” distribution directly. Practically, access , apart replicating experiment study many times. Instead, can make educated guess shape parameters distribution. parametric approach used compute estimators analytically, e.g., Student t Distribution. way done t.test().approach assume sample , therefore, data collected representative, frequency individual values sample proportional probability, .e., often see particular value, likely . case, sampling data just like sampling true distribution. obviously strong assumption, particularly small samples, however, approach can work data, regardless distribution, can used estimate statistic easy derive analytically. Thus, use brute force approach relies sheer computer power compute confidence interval one analytically computed t-test resampling data generated.need three functions . First, function samples data: sample(). takes original data (first parameter x) randomly samples size items either without replacement (controlled replace parameter defaults FALSE, replacement). case want get sample size original data want sample replacement. replacement means value drawn sample, recorded put back , can drawn . replacement procedure means probability drawing particular value always , whereas without replacement probabilities change every draw simply fewer fewer values left sample .purposes, want resample data compute mean. Write code just . Run chunk several times see computed mean value changes due resampling. exercise, set replace = FALSE , running chunk, think value expect whether change run chunk .exercise 2.resampling--computing-mean code simple brief. However, generally good idea pack function meaningful name. just : turn code exercise 2 function (think function parameters return) call check everything works (pass sample , return resampled mean ).exercise 3.second step repeat first step many (say, 1000) times. base R function helps replicate(). takes number repetitions (first parameter n) arbitrary R code returns value (step one). run , get vector 1000 means resampled data. Plot histogram, overlaying true average samples’ means (mean means samples, just mean samples!) reference. plot look like \nfinal step use quantile() function compute 95% confidence interval. quantile() function takes vector computes value greater probs fraction values vector. E.g., probs=c(0.25, 0.75), return two values, 25% values smaller first one 75% smaller second. , put differently, 50% values probs=c(0.25, 0.75). case, want compute 97%74 confidence interval, .e., 97% values lower upper confidence interval values. run code, see 97% confidence interval resampling similar t-test reported (want get values due random sampling also close t-test’s analytic estimate). Add information caption (often, information put directly text, find simpler quantitative information together easy find)\n\n::: {.practice}\nexercise 4.\n:::","code":"\nt.test(samples, mu = 10)## \n##  One Sample t-test\n## \n## data:  samples\n## t = -1.1076, df = 19, p-value = 0.2819\n## alternative hypothesis: true mean is not equal to 10\n## 95 percent confidence interval:\n##   8.911319 10.335209\n## sample estimates:\n## mean of x \n##  9.623264"},{"path":"sampling-and-simulations.html","id":"repeating-computation-via-for-loop","chapter":"12 Sampling and simulations","heading":"12.2 Repeating computation via for loop","text":"discussed chapter loops repetitions, whereas many ways repeat computation R. Let’s replicate sampling part code, using loop. best way perform current task, safer approach computation heavy takes long time, easier perform bookkeeping, retain data restart computation something goes wrong (e.g., run memory file space), compared functional programing via replicate purrr, might need start scratch.Think define number iterations, whether need use loop variable, concatenate new sample mean vector, etc.exercise 5.","code":""},{"path":"sampling-and-simulations.html","id":"repeating-computation-via-purrr","chapter":"12 Sampling and simulations","heading":"12.3 Repeating computation via purrr","text":"Practice makes perfect, let us replicate code repeating via purrr library. Think map function best job, whether need use special . variable, etc.exercise 6.","code":""},{"path":"sampling-and-simulations.html","id":"bootstrapping-via-boot-library","chapter":"12 Sampling and simulations","heading":"12.4 Bootstrapping via boot library","text":"approach used called “bootstrapping” R giving options, boot library simplify automate bootstrapping confidence interval computation. need install (boot comes base R) need import via library(boot).key function boot(). plenty parameters allow fine tune performance three key compulsory parameters aredata: original data want use bootstrapping.statistic: function(s) compute desired statistic, mean case.R: number bootstrap replicates (used 1000 hand).non-parametric bootstrapping, like one used , need write statistic function even want compute statistic functions already exist, like mean standard deviation. statistic function must take least two arguments: 1) data passed 2) resampled. default, second parameter contain indexes elements data. Note bootstrap resamples replacement, index can appear , .e., element drawn (just ).statistic function like following, course better name actual code inside.function, can bootstrap samples viaNext, use function boot.ci() compute confidence interval, takes bootstrapped samples first parameter. can also specify confidence interval interested (conf, defaults 0.95 want 97!) type confidence interval. one computed called percentile (type=\"perc\"), type specify75. run code output similar .can see, similar results (variation due sampling). Thus, either approach work , cases, boot flexible solution (read bootstrapping using advanced options).exercise 7.","code":"\nyour_statistic_function <- function(data, indexes){\n  # here you compute desired statistic subsetting data using indexes\n}\nbooted_samples <- boot(samples, statistic = your_statistic_function, R = 1000)## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n## Based on 1000 bootstrap replicates\n## \n## CALL : \n## boot.ci(boot.out = booted_samples, type = \"perc\")\n## \n## Intervals : \n## Level     Percentile     \n## 95%   ( 8.939, 10.242 )  \n## Calculations and Intervals on Original Scale"},{"path":"sampling-and-simulations.html","id":"likert-confidence","chapter":"12 Sampling and simulations","heading":"12.5 Confidence about proportion of responses for Likert scale","text":"looked one present information Likert scale responses couple times already. Let us return see can compute just proportion response level report, also percentile confidence interval , see reliable numbers. use smaller file just one scale — likert-scale.csv — first task reuse old chapter factors making sure counts complete chapter tidyr. Package code counts computes proportions per condition response level function. take data frame original data first input second parameter resample FALSE default (use later). function return summary table can reuse\ncan plot using code chapter factors. Also, can reuse original code reading preprocessing data, used compute counts proportions. plot look almost exactly zeros uncounted response levels plotted previously.exercise 8.Modify function resample parameter TRUE, samples entire table replacement. suitable function sample_frac() dplyr, can easily specify size data 1 (many row original table). can use sample_n() must specify desired number rows explicitly. Don’t forget replacement! Test updated function calling resample = TRUE checking get different averages every time.exercise 9.resampling mean, now need repeat many (1000, 2000) times . , prior approaches won’t , expect single number (statistic) whereas function returns table . solution use map() function purrr library store tables list can turn single table via list_rbind(). suggestion first program list_of_tables <- map(...) part alone just couple iterations. can check table inside list (remember use simplifying subsetting?) look good different, can combine via list_rbind(). sure computation works correctly, can run iterations get samples. Advice, read .progress parameter map() makes waiting code finish bit less stressful.exercise 10.final stretch! table bootstrapped counts proportions contains many samples combination condition response level (e.g., 1000). Use quantile() function compute lower upper limits 97% confidence interval. Think dplyr verbs need . table confidence intervals look like (variability due sampling).exercise 11.thing left use combination geom_ribbon() geom_errorbar() add 97% CIs original plot. Now two tables single plot two ways handle . First, can join single table. , can pass table CIs geom . latter case, need use explicit named parameter data = likert_CI (whatever name table ) geoms ggplot2 expect aesthetics first parameter. Also, CIs table Probability column use aesthetics y ggplot2 complain. solution set y one limits, used geom make actual difference76.exercise 12.procedure plot preferred way reporting Likert scale data. can use approaches always keep mind ordinal data treated , even label individual levels numbers.","code":""},{"path":"generalized-linear-regression-and-resampling.html","id":"generalized-linear-regression-and-resampling","chapter":"13 (Generalized) Linear regression and Resampling","heading":"13 (Generalized) Linear regression and Resampling","text":"warned beginning, seminar teach statistics. latter big topic , experience, know statistics know specific tool need, figuring use R fairly trivial (just find package implements analysis read docs). Conversely, knowledge statistics approximate, knowing call functions little good. catch statistical models easy run (even implement hand scratch) easy misuse hard interpret77.make things worse, computers algorithms care. absolute majority cases, statistical models happily accept input provide, even completely unsuitable, spit numbers. Unfortunately, , computer, know whether results even make sense. solution problem: spare effort learn statistics. solid understanding basic regression analysis help figuring statistical tools applicable , even importantly, definitely misguide . give general overview examples simulations explain use particular tool interpret outputs. Want know ? Attend Bayesian Statistics seminar read excellent Statistical Rethinking Richard McElreath seminar based .Grab exercise notebook reading .","code":""},{"path":"generalized-linear-regression-and-resampling.html","id":"linear-regression-simulating-data","chapter":"13 (Generalized) Linear regression and Resampling","heading":"13.1 Linear regression: simulating data","text":"first statistical model linear regression. experiment analysis, easier notice something goes wrong already know answer. let us simulate data linear relationship : overall height versus foot length. conference paper found online suggests foot length distribution 246.726±10.3434 mm (mean standard deviation, assume normal distribution) formula estimate height \\(Height = 710 + 4 \\cdot Foot + \\epsilon\\)78 \\(\\epsilon\\) (residual error) normally distributed around zero standard deviation 10. Generate data (used 100 points) putting table two columns (called FootLength Height) plot match figure . can set seed 826 replicate exactly.exercise 1.","code":""},{"path":"generalized-linear-regression-and-resampling.html","id":"linear-regression-statistical-model","chapter":"13 (Generalized) Linear regression and Resampling","heading":"13.2 Linear regression: statistical model","text":"Let us use linear regression model fit data see accurately can estimate intercept slope terms. R built-function linear regression model — lm() — uses common formula design specify relationship. formula approach widely used R due power simplicity. syntax full factorial linear model two predictors y ~ x1 + x2 + x1:x2 individual effects added via + : mean “interaction” variables. many additional bells whistles, specifying main effects interaction variables via asterisk (formula can compressed y ~ x1 * x2) removing intercept term (setting explicitly zero” y ~ 0 + x1 * x2). can see detail online documentation statistical packages frequently expand accommodate additional features random effects. However, pay extra attention new syntax different packages may use different symbols encode certain feature , vice versa, use symbol different features. E.g., | typically means random effect mightily confused package used denote variance term instead.Read documentation lm() summary() functions. Fit model print full summary . output look like thisAs can see, estimate intercept term fairly close 725±24 mm versus 710 mm used formula. goes foot length slope: 3.95±0.1 versus 4.exercise 2.think nice present information model fit alongside plot, let us prepare summary intercept slope terms format estimate [lower-97%-CI-limit..upper-97%-CI-limit]. can extract estimates via coef() function confidence intervals via confint(). cases,\nnames terms specified either names vector rownames matrix. Think handle . approach convert matrix confidence intervals data frame (converting directly tibble removes row names need later), turn row names column via rownames_to_column(), convert tibble can rename ugly converted column names, add estimates new column table, relocate columns consistent look. , can combine new variable via string formatting (prefer glue). need one(!) pipeline .\nsummary table looks like thisDo exercise 3.Statistical model good predictions, whenever fit statistical model data, compare predictions data visually. ggplot2 provides easy solution via geom_smooth() met earlier. didactic purposes, let us use slightly longer way generating predictions plotting alongside data. R provides simple interface generating prediction fitted model via predict() function: pass fitted model generate prediction every original data point. However, can also generate data data points present data (-called “counterfactuals”) passing table newdata parameter. use latter option. Generate new table single column FootLength (, however called original data) sequence number going lowest highest range values (220 270 case) regular steps (picked step 1 think whether choosing different one make difference). Pass new data newdata predict, store predictions Height column (now structure table predictions matches real data) use geom_line(). ’s plot looks like.\nexercise 4.can see trend, working statistical models, important understand uncertainty predictions. , need plot just prediction foot length also confidence interval prediction. First, easy way predict() function options well via interval parameter (want \"confidence\"). Use generate 97% confidence interval foot length generated plot geom_ribbon().exercise 5.","code":"## \n## Call:\n## lm(formula = Height ~ FootLength, data = height_df)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -24.0149  -7.0855   0.7067   6.0991  26.6808 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 721.26822   24.12584   29.90   <2e-16 ***\n## FootLength    3.95535    0.09776   40.46   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 10.13 on 98 degrees of freedom\n## Multiple R-squared:  0.9435, Adjusted R-squared:  0.9429 \n## F-statistic:  1637 on 1 and 98 DF,  p-value: < 2.2e-16"},{"path":"generalized-linear-regression-and-resampling.html","id":"linear-regression-bootstrapping-predictions","chapter":"13 (Generalized) Linear regression and Resampling","heading":"13.3 Linear regression: bootstrapping predictions","text":"Let us replicate results use bootstrap approach, work even don’t convenience function. One iteration consists :Randomly sample original data table replacement.Fit linear model data.Generate predictions interval, way , end table FootLength (going 220 270) (predicted) Height.Write code put function (think parameters need). Test running function times. Values column stay change?exercise 6.function, things easy. need follow algorithm computing visualizing confidence intervals Likert scale:Call function multiple times recording predictions (think map() list_rbind())Compute 97% confidence interval foot length via quantilesPlot geom_ribbon() .exercise 7.","code":""},{"path":"generalized-linear-regression-and-resampling.html","id":"logistic-regression-simulating-data","chapter":"13 (Generalized) Linear regression and Resampling","heading":"13.4 Logistic regression: simulating data","text":"Let us practice time using binomial data. Let us assume measure success playing video games people drank tea versus coffee (idea effect , can use liquids liking example). Let us assume measure thirty participants group average probability success 0.4 tea 0.7 coffee79 groups. much know drill now, use rbinom() (want twenty 0/1 values, figure size n need) generate data condition, put single table 60 rows (bind_rows() might useful) two columns (Condition Success). table look similar (seed 12987)exercise 8.Now, let us visualize data. need compute average 97% confidence interval condition. average easy (divide total number successes condition total number participants) confidence interval trickier. Luckily us, package binom us covered. implements multiple methods computing . used binom.exact() (use ?binom.exact console read manual, loaded library). plot look like (similar, set seed). Note mean probability Tea condition higher designed (sampling variation!) confidence intervals asymmetric. latter easier see coffee condition (redo probability success 0.9 make apparent) common data limited interval.exercise 9.","code":""},{"path":"generalized-linear-regression-and-resampling.html","id":"logistic-regression-fitting-data","chapter":"13 (Generalized) Linear regression and Resampling","heading":"13.5 Logistic regression: fitting data","text":"Let us fit data using generalized linear models — glm — \"binomial\" family. latter bit name function new bits, formula works way lm(). Print summary look like .exercise 10.Note logistic regression, estimates need harder interpret. Slope term condition Tea units log-odds see negative, meaning model predict fewer successes tea coffee group. Intercept trickier need inverted logit function (example, implemented inv.logit() boot package) convert probabilities. , 0 corresponds probability 0.5, 1 somewhere . Personally, find units confusing, make sense need use estimates (coef() functio work well) compute scores condition tranlate probabilities via inv.logit(). Coffee “baseline” group (simple alphabetically), \\(logit(Coffee) = Intercept\\) \\(logit(Tea) = Intercept + ConditionTea\\). statistics, link function applied left side, apply inverse right side. just wanted show notation, recognize next time see .code generate table two columns (Condition P Probability). look like thisDo exercise 11.","code":"## \n## Call:\n## glm(formula = Success ~ Condition, family = \"binomial\", data = game_df)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)  \n## (Intercept)    1.0116     0.4129   2.450   0.0143 *\n## ConditionTea  -0.8781     0.5517  -1.592   0.1115  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 78.859  on 59  degrees of freedom\n## Residual deviance: 76.250  on 58  degrees of freedom\n## AIC: 80.25\n## \n## Number of Fisher Scoring iterations: 4"},{"path":"generalized-linear-regression-and-resampling.html","id":"logistic-regression-bootstrapping-uncertainty","chapter":"13 (Generalized) Linear regression and Resampling","heading":"13.6 Logistic regression: bootstrapping uncertainty","text":"Let us bootstrap predicted probability success following template used already twice slight twist. Write function (first write inside code, make sure works, turn function) samples data, fits model, generates returns model prediction. twist sample condition separately. sample_frac() need group data condition . Also, pass iteration index (one purrring ) parameter function store separate column Iteration. allows us identify individual samples, making easier compute difference conditions later .Repeat computation 1000 times end table two columns (Condition P) 2000 rows (1000 per condition). Instead computing aggregate information, visualize distributions using geom_violin(). ’s plot look like.exercise 12.final step, let us compute average 97% confidence interval difference conditions. samples long format, need make table wide, end three columns: Iteration, Coffee, andTea`. Compute new column difference tea coffee, compute nicely format statistics putting figure caption. Hint: can pull difference column table make things easier.exercise 13.","code":""}]
